[2022-06-11 09:30:25] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: ''
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 160
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 09:30:25] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 09:30:29] __main__ INFO: MACs   : 255.27M
[2022-06-11 09:30:29] __main__ INFO: #params: 1.73M
[2022-06-11 09:31:23] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: ''
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 160
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 09:31:23] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 09:31:27] __main__ INFO: MACs   : 255.27M
[2022-06-11 09:31:27] __main__ INFO: #params: 1.73M
[2022-06-11 09:31:49] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: ''
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 160
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 09:31:49] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 09:31:52] __main__ INFO: MACs   : 112.57M
[2022-06-11 09:31:52] __main__ INFO: #params: 758.55K
[2022-06-11 09:40:40] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: ''
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 160
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 09:40:40] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 09:40:43] __main__ INFO: MACs   : 112.57M
[2022-06-11 09:40:43] __main__ INFO: #params: 758.55K
[2022-06-11 09:41:50] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: ''
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 160
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 09:41:50] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 09:41:53] __main__ INFO: MACs   : 112.57M
[2022-06-11 09:41:53] __main__ INFO: #params: 758.55K
[2022-06-11 09:41:54] __main__ INFO: Val 0
[2022-06-11 09:46:24] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 160
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 09:46:24] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 09:46:28] __main__ INFO: MACs   : 112.57M
[2022-06-11 09:46:28] __main__ INFO: #params: 758.55K
[2022-06-11 09:48:39] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 160
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 09:48:39] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 09:48:43] __main__ INFO: MACs   : 112.57M
[2022-06-11 09:48:43] __main__ INFO: #params: 758.55K
[2022-06-11 09:49:27] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 160
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 09:49:27] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 09:49:31] __main__ INFO: MACs   : 112.57M
[2022-06-11 09:49:31] __main__ INFO: #params: 758.55K
[2022-06-11 09:49:56] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 160
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 09:49:56] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 09:50:00] __main__ INFO: MACs   : 112.57M
[2022-06-11 09:50:00] __main__ INFO: #params: 758.55K
[2022-06-11 09:50:00] __main__ INFO: Val 0
[2022-06-11 09:50:01] __main__ INFO: Epoch 0 loss 16761.8818 acc@1 0.1000 acc@5 0.5000
[2022-06-11 09:50:01] __main__ INFO: Elapsed 0.98
[2022-06-11 09:50:01] __main__ INFO: Train 1 0
[2022-06-11 09:51:35] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 160
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 09:51:35] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 09:51:39] __main__ INFO: MACs   : 112.57M
[2022-06-11 09:51:39] __main__ INFO: #params: 758.55K
[2022-06-11 09:51:39] __main__ INFO: Val 0
[2022-06-11 09:52:14] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 160
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 09:52:14] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 09:52:22] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 160
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 09:52:22] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 09:52:26] __main__ INFO: MACs   : 112.57M
[2022-06-11 09:52:26] __main__ INFO: #params: 758.55K
[2022-06-11 09:53:29] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 160
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 09:53:29] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 09:53:33] __main__ INFO: MACs   : 112.57M
[2022-06-11 09:53:33] __main__ INFO: #params: 758.55K
[2022-06-11 09:53:33] __main__ INFO: Val 0
[2022-06-11 09:53:34] __main__ INFO: Epoch 0 loss 2.9075 acc@1 0.1334 acc@5 0.5883
[2022-06-11 09:53:34] __main__ INFO: Elapsed 1.07
[2022-06-11 09:53:34] __main__ INFO: Train 1 0
[2022-06-11 09:53:38] __main__ INFO: Epoch 1 Step 100/390 lr 0.100000 loss 0.6737 (1.1222) acc@1 0.8047 (0.6043) acc@5 0.9922 (0.9480)
[2022-06-11 09:53:42] __main__ INFO: Epoch 1 Step 200/390 lr 0.100000 loss 0.6866 (0.9273) acc@1 0.7734 (0.6747) acc@5 0.9922 (0.9646)
[2022-06-11 09:53:46] __main__ INFO: Epoch 1 Step 300/390 lr 0.100000 loss 0.6710 (0.8345) acc@1 0.7500 (0.7083) acc@5 1.0000 (0.9722)
[2022-06-11 09:53:50] __main__ INFO: Epoch 1 Step 390/390 lr 0.100000 loss 0.5345 (0.7789) acc@1 0.8516 (0.7280) acc@5 0.9922 (0.9758)
[2022-06-11 09:53:50] __main__ INFO: Elapsed 16.10
[2022-06-11 09:53:50] __main__ INFO: Val 1
[2022-06-11 09:53:51] __main__ INFO: Epoch 1 loss 0.7673 acc@1 0.7416 acc@5 0.9840
[2022-06-11 09:53:51] __main__ INFO: Elapsed 0.95
[2022-06-11 09:53:51] __main__ INFO: Train 2 390
[2022-06-11 09:53:55] __main__ INFO: Epoch 2 Step 100/390 lr 0.100000 loss 0.6133 (0.5385) acc@1 0.7891 (0.8152) acc@5 0.9922 (0.9912)
[2022-06-11 09:53:59] __main__ INFO: Epoch 2 Step 200/390 lr 0.100000 loss 0.4462 (0.5285) acc@1 0.8594 (0.8192) acc@5 1.0000 (0.9910)
[2022-06-11 09:54:03] __main__ INFO: Epoch 2 Step 300/390 lr 0.100000 loss 0.3723 (0.5239) acc@1 0.8672 (0.8203) acc@5 0.9922 (0.9910)
[2022-06-11 09:54:07] __main__ INFO: Epoch 2 Step 390/390 lr 0.100000 loss 0.4450 (0.5197) acc@1 0.8047 (0.8212) acc@5 1.0000 (0.9911)
[2022-06-11 09:54:07] __main__ INFO: Elapsed 15.64
[2022-06-11 09:54:07] __main__ INFO: Val 2
[2022-06-11 09:54:08] __main__ INFO: Epoch 2 loss 0.6593 acc@1 0.7904 acc@5 0.9852
[2022-06-11 09:54:08] __main__ INFO: Elapsed 0.97
[2022-06-11 09:54:08] __main__ INFO: Train 3 780
[2022-06-11 09:54:12] __main__ INFO: Epoch 3 Step 100/390 lr 0.100000 loss 0.3870 (0.4540) acc@1 0.8359 (0.8427) acc@5 0.9766 (0.9943)
[2022-06-11 09:54:16] __main__ INFO: Epoch 3 Step 200/390 lr 0.100000 loss 0.4824 (0.4590) acc@1 0.8359 (0.8407) acc@5 0.9922 (0.9936)
[2022-06-11 09:54:20] __main__ INFO: Epoch 3 Step 300/390 lr 0.100000 loss 0.6466 (0.4524) acc@1 0.7578 (0.8429) acc@5 0.9844 (0.9936)
[2022-06-11 09:54:24] __main__ INFO: Epoch 3 Step 390/390 lr 0.100000 loss 0.5034 (0.4538) acc@1 0.8281 (0.8420) acc@5 1.0000 (0.9934)
[2022-06-11 09:54:24] __main__ INFO: Elapsed 15.94
[2022-06-11 09:54:24] __main__ INFO: Val 3
[2022-06-11 09:54:25] __main__ INFO: Epoch 3 loss 0.5483 acc@1 0.8188 acc@5 0.9891
[2022-06-11 09:54:25] __main__ INFO: Elapsed 0.95
[2022-06-11 09:54:25] __main__ INFO: Train 4 1170
[2022-06-11 09:54:29] __main__ INFO: Epoch 4 Step 100/390 lr 0.100000 loss 0.3638 (0.4060) acc@1 0.8828 (0.8602) acc@5 1.0000 (0.9956)
[2022-06-11 09:54:33] __main__ INFO: Epoch 4 Step 200/390 lr 0.100000 loss 0.4588 (0.4115) acc@1 0.8438 (0.8584) acc@5 1.0000 (0.9953)
[2022-06-11 09:54:36] __main__ INFO: Epoch 4 Step 300/390 lr 0.100000 loss 0.5021 (0.4133) acc@1 0.8203 (0.8588) acc@5 1.0000 (0.9948)
[2022-06-11 09:54:40] __main__ INFO: Epoch 4 Step 390/390 lr 0.100000 loss 0.3403 (0.4129) acc@1 0.8750 (0.8584) acc@5 1.0000 (0.9943)
[2022-06-11 09:54:40] __main__ INFO: Elapsed 15.50
[2022-06-11 09:54:40] __main__ INFO: Val 4
[2022-06-11 09:54:41] __main__ INFO: Epoch 4 loss 0.5714 acc@1 0.8152 acc@5 0.9914
[2022-06-11 09:54:41] __main__ INFO: Elapsed 0.91
[2022-06-11 09:54:41] __main__ INFO: Train 5 1560
[2022-06-11 09:54:45] __main__ INFO: Epoch 5 Step 100/390 lr 0.100000 loss 0.3706 (0.3736) acc@1 0.8750 (0.8724) acc@5 0.9922 (0.9960)
[2022-06-11 09:54:49] __main__ INFO: Epoch 5 Step 200/390 lr 0.100000 loss 0.2914 (0.3719) acc@1 0.8984 (0.8723) acc@5 0.9922 (0.9961)
[2022-06-11 09:54:53] __main__ INFO: Epoch 5 Step 300/390 lr 0.100000 loss 0.2328 (0.3769) acc@1 0.9453 (0.8708) acc@5 0.9922 (0.9955)
[2022-06-11 09:54:56] __main__ INFO: Epoch 5 Step 390/390 lr 0.100000 loss 0.4291 (0.3776) acc@1 0.8594 (0.8705) acc@5 1.0000 (0.9954)
[2022-06-11 09:54:57] __main__ INFO: Elapsed 15.62
[2022-06-11 09:54:57] __main__ INFO: Val 5
[2022-06-11 09:54:57] __main__ INFO: Epoch 5 loss 0.5110 acc@1 0.8320 acc@5 0.9931
[2022-06-11 09:54:57] __main__ INFO: Elapsed 0.92
[2022-06-11 09:54:57] __main__ INFO: Train 6 1950
[2022-06-11 09:55:02] __main__ INFO: Epoch 6 Step 100/390 lr 0.100000 loss 0.4245 (0.3538) acc@1 0.8203 (0.8777) acc@5 1.0000 (0.9963)
[2022-06-11 09:55:06] __main__ INFO: Epoch 6 Step 200/390 lr 0.100000 loss 0.2674 (0.3568) acc@1 0.8984 (0.8763) acc@5 1.0000 (0.9962)
[2022-06-11 09:55:10] __main__ INFO: Epoch 6 Step 300/390 lr 0.100000 loss 0.2773 (0.3548) acc@1 0.8828 (0.8771) acc@5 1.0000 (0.9961)
[2022-06-11 09:55:13] __main__ INFO: Epoch 6 Step 390/390 lr 0.100000 loss 0.3538 (0.3564) acc@1 0.8906 (0.8764) acc@5 1.0000 (0.9960)
[2022-06-11 09:55:13] __main__ INFO: Elapsed 15.72
[2022-06-11 09:55:13] __main__ INFO: Val 6
[2022-06-11 09:55:14] __main__ INFO: Epoch 6 loss 0.5229 acc@1 0.8319 acc@5 0.9906
[2022-06-11 09:55:14] __main__ INFO: Elapsed 0.95
[2022-06-11 09:55:14] __main__ INFO: Train 7 2340
[2022-06-11 09:55:18] __main__ INFO: Epoch 7 Step 100/390 lr 0.100000 loss 0.4397 (0.3269) acc@1 0.8516 (0.8870) acc@5 0.9922 (0.9964)
[2022-06-11 09:55:22] __main__ INFO: Epoch 7 Step 200/390 lr 0.100000 loss 0.2563 (0.3368) acc@1 0.9297 (0.8845) acc@5 0.9922 (0.9961)
[2022-06-11 09:55:26] __main__ INFO: Epoch 7 Step 300/390 lr 0.100000 loss 0.3956 (0.3374) acc@1 0.8750 (0.8837) acc@5 0.9922 (0.9963)
[2022-06-11 09:55:30] __main__ INFO: Epoch 7 Step 390/390 lr 0.100000 loss 0.2600 (0.3372) acc@1 0.9141 (0.8831) acc@5 0.9922 (0.9964)
[2022-06-11 09:55:30] __main__ INFO: Elapsed 15.64
[2022-06-11 09:55:30] __main__ INFO: Val 7
[2022-06-11 09:55:31] __main__ INFO: Epoch 7 loss 0.6200 acc@1 0.8104 acc@5 0.9874
[2022-06-11 09:55:31] __main__ INFO: Elapsed 0.91
[2022-06-11 09:55:31] __main__ INFO: Train 8 2730
[2022-06-11 09:55:35] __main__ INFO: Epoch 8 Step 100/390 lr 0.100000 loss 0.4133 (0.3188) acc@1 0.8359 (0.8893) acc@5 0.9844 (0.9966)
[2022-06-11 09:55:39] __main__ INFO: Epoch 8 Step 200/390 lr 0.100000 loss 0.3322 (0.3282) acc@1 0.8750 (0.8873) acc@5 1.0000 (0.9964)
[2022-06-11 09:55:43] __main__ INFO: Epoch 8 Step 300/390 lr 0.100000 loss 0.3693 (0.3287) acc@1 0.8828 (0.8868) acc@5 0.9922 (0.9963)
[2022-06-11 09:55:46] __main__ INFO: Epoch 8 Step 390/390 lr 0.100000 loss 0.3040 (0.3263) acc@1 0.9375 (0.8875) acc@5 0.9922 (0.9965)
[2022-06-11 09:55:46] __main__ INFO: Elapsed 15.73
[2022-06-11 09:55:46] __main__ INFO: Val 8
[2022-06-11 09:55:47] __main__ INFO: Epoch 8 loss 0.4959 acc@1 0.8433 acc@5 0.9915
[2022-06-11 09:55:47] __main__ INFO: Elapsed 0.95
[2022-06-11 09:55:47] __main__ INFO: Train 9 3120
[2022-06-11 09:55:51] __main__ INFO: Epoch 9 Step 100/390 lr 0.100000 loss 0.3766 (0.2999) acc@1 0.8828 (0.8962) acc@5 0.9844 (0.9974)
[2022-06-11 09:55:56] __main__ INFO: Epoch 9 Step 200/390 lr 0.100000 loss 0.2971 (0.3073) acc@1 0.9219 (0.8925) acc@5 1.0000 (0.9973)
[2022-06-11 09:55:59] __main__ INFO: Epoch 9 Step 300/390 lr 0.100000 loss 0.4753 (0.3109) acc@1 0.8438 (0.8920) acc@5 1.0000 (0.9970)
[2022-06-11 09:56:03] __main__ INFO: Epoch 9 Step 390/390 lr 0.100000 loss 0.2710 (0.3105) acc@1 0.8984 (0.8926) acc@5 1.0000 (0.9968)
[2022-06-11 09:56:03] __main__ INFO: Elapsed 15.58
[2022-06-11 09:56:03] __main__ INFO: Val 9
[2022-06-11 09:56:04] __main__ INFO: Epoch 9 loss 0.4733 acc@1 0.8490 acc@5 0.9934
[2022-06-11 09:56:04] __main__ INFO: Elapsed 0.91
[2022-06-11 09:56:04] __main__ INFO: Train 10 3510
[2022-06-11 09:56:08] __main__ INFO: Epoch 10 Step 100/390 lr 0.100000 loss 0.3147 (0.2934) acc@1 0.8984 (0.8985) acc@5 0.9844 (0.9973)
[2022-06-11 09:56:12] __main__ INFO: Epoch 10 Step 200/390 lr 0.100000 loss 0.3431 (0.2985) acc@1 0.9062 (0.8978) acc@5 0.9922 (0.9970)
[2022-06-11 09:56:16] __main__ INFO: Epoch 10 Step 300/390 lr 0.100000 loss 0.2463 (0.2971) acc@1 0.9219 (0.8973) acc@5 1.0000 (0.9971)
[2022-06-11 09:56:19] __main__ INFO: Epoch 10 Step 390/390 lr 0.100000 loss 0.2263 (0.3026) acc@1 0.9219 (0.8959) acc@5 1.0000 (0.9969)
[2022-06-11 09:56:20] __main__ INFO: Elapsed 15.65
[2022-06-11 09:56:20] __main__ INFO: Val 10
[2022-06-11 09:56:20] __main__ INFO: Epoch 10 loss 0.5039 acc@1 0.8431 acc@5 0.9935
[2022-06-11 09:56:20] __main__ INFO: Elapsed 0.94
[2022-06-11 09:56:20] __main__ INFO: Train 11 3900
[2022-06-11 09:56:25] __main__ INFO: Epoch 11 Step 100/390 lr 0.100000 loss 0.3778 (0.2642) acc@1 0.8359 (0.9076) acc@5 1.0000 (0.9984)
[2022-06-11 09:56:29] __main__ INFO: Epoch 11 Step 200/390 lr 0.100000 loss 0.3571 (0.2753) acc@1 0.8984 (0.9041) acc@5 0.9922 (0.9979)
[2022-06-11 09:56:33] __main__ INFO: Epoch 11 Step 300/390 lr 0.100000 loss 0.2886 (0.2799) acc@1 0.8984 (0.9030) acc@5 1.0000 (0.9976)
[2022-06-11 09:56:36] __main__ INFO: Epoch 11 Step 390/390 lr 0.100000 loss 0.4592 (0.2862) acc@1 0.8281 (0.9007) acc@5 1.0000 (0.9974)
[2022-06-11 09:56:36] __main__ INFO: Elapsed 15.81
[2022-06-11 09:56:36] __main__ INFO: Val 11
[2022-06-11 09:56:37] __main__ INFO: Epoch 11 loss 0.4441 acc@1 0.8593 acc@5 0.9932
[2022-06-11 09:56:37] __main__ INFO: Elapsed 0.94
[2022-06-11 09:56:37] __main__ INFO: Train 12 4290
[2022-06-11 09:56:41] __main__ INFO: Epoch 12 Step 100/390 lr 0.100000 loss 0.2452 (0.2715) acc@1 0.9219 (0.9035) acc@5 1.0000 (0.9972)
[2022-06-11 09:56:45] __main__ INFO: Epoch 12 Step 200/390 lr 0.100000 loss 0.3847 (0.2758) acc@1 0.8750 (0.9046) acc@5 1.0000 (0.9969)
[2022-06-11 09:56:49] __main__ INFO: Epoch 12 Step 300/390 lr 0.100000 loss 0.2670 (0.2776) acc@1 0.8984 (0.9045) acc@5 0.9922 (0.9969)
[2022-06-11 09:56:53] __main__ INFO: Epoch 12 Step 390/390 lr 0.100000 loss 0.2926 (0.2829) acc@1 0.9141 (0.9030) acc@5 1.0000 (0.9968)
[2022-06-11 09:56:53] __main__ INFO: Elapsed 15.76
[2022-06-11 09:56:53] __main__ INFO: Val 12
[2022-06-11 09:56:54] __main__ INFO: Epoch 12 loss 0.4671 acc@1 0.8482 acc@5 0.9934
[2022-06-11 09:56:54] __main__ INFO: Elapsed 0.92
[2022-06-11 09:56:54] __main__ INFO: Train 13 4680
[2022-06-11 09:56:58] __main__ INFO: Epoch 13 Step 100/390 lr 0.100000 loss 0.2534 (0.2630) acc@1 0.9062 (0.9101) acc@5 1.0000 (0.9977)
[2022-06-11 09:57:02] __main__ INFO: Epoch 13 Step 200/390 lr 0.100000 loss 0.3493 (0.2658) acc@1 0.8906 (0.9078) acc@5 0.9922 (0.9978)
[2022-06-11 09:57:06] __main__ INFO: Epoch 13 Step 300/390 lr 0.100000 loss 0.2027 (0.2701) acc@1 0.9297 (0.9066) acc@5 1.0000 (0.9977)
[2022-06-11 09:57:10] __main__ INFO: Epoch 13 Step 390/390 lr 0.100000 loss 0.3353 (0.2731) acc@1 0.8672 (0.9056) acc@5 1.0000 (0.9977)
[2022-06-11 09:57:10] __main__ INFO: Elapsed 15.75
[2022-06-11 09:57:10] __main__ INFO: Val 13
[2022-06-11 09:57:11] __main__ INFO: Epoch 13 loss 0.4527 acc@1 0.8561 acc@5 0.9943
[2022-06-11 09:57:11] __main__ INFO: Elapsed 0.95
[2022-06-11 09:57:11] __main__ INFO: Train 14 5070
[2022-06-11 09:57:15] __main__ INFO: Epoch 14 Step 100/390 lr 0.100000 loss 0.1080 (0.2505) acc@1 0.9688 (0.9129) acc@5 1.0000 (0.9976)
[2022-06-11 09:57:19] __main__ INFO: Epoch 14 Step 200/390 lr 0.100000 loss 0.2212 (0.2575) acc@1 0.9297 (0.9107) acc@5 0.9922 (0.9977)
[2022-06-11 09:57:23] __main__ INFO: Epoch 14 Step 300/390 lr 0.100000 loss 0.1798 (0.2613) acc@1 0.9453 (0.9097) acc@5 1.0000 (0.9977)
[2022-06-11 09:57:26] __main__ INFO: Epoch 14 Step 390/390 lr 0.100000 loss 0.3119 (0.2643) acc@1 0.8906 (0.9083) acc@5 0.9922 (0.9975)
[2022-06-11 09:57:26] __main__ INFO: Elapsed 15.73
[2022-06-11 09:57:26] __main__ INFO: Val 14
[2022-06-11 09:57:27] __main__ INFO: Epoch 14 loss 0.4985 acc@1 0.8424 acc@5 0.9919
[2022-06-11 09:57:27] __main__ INFO: Elapsed 0.97
[2022-06-11 09:57:27] __main__ INFO: Train 15 5460
[2022-06-11 09:57:31] __main__ INFO: Epoch 15 Step 100/390 lr 0.100000 loss 0.2765 (0.2426) acc@1 0.9297 (0.9161) acc@5 1.0000 (0.9985)
[2022-06-11 09:57:35] __main__ INFO: Epoch 15 Step 200/390 lr 0.100000 loss 0.2194 (0.2493) acc@1 0.9219 (0.9147) acc@5 1.0000 (0.9981)
[2022-06-11 09:57:39] __main__ INFO: Epoch 15 Step 300/390 lr 0.100000 loss 0.2947 (0.2537) acc@1 0.9141 (0.9124) acc@5 0.9922 (0.9979)
[2022-06-11 09:57:43] __main__ INFO: Epoch 15 Step 390/390 lr 0.100000 loss 0.2358 (0.2571) acc@1 0.9062 (0.9114) acc@5 1.0000 (0.9980)
[2022-06-11 09:57:43] __main__ INFO: Elapsed 15.67
[2022-06-11 09:57:43] __main__ INFO: Val 15
[2022-06-11 09:57:44] __main__ INFO: Epoch 15 loss 0.4242 acc@1 0.8656 acc@5 0.9938
[2022-06-11 09:57:44] __main__ INFO: Elapsed 0.95
[2022-06-11 09:57:44] __main__ INFO: Train 16 5850
[2022-06-11 09:57:48] __main__ INFO: Epoch 16 Step 100/390 lr 0.100000 loss 0.2836 (0.2382) acc@1 0.9062 (0.9158) acc@5 0.9922 (0.9978)
[2022-06-11 09:57:52] __main__ INFO: Epoch 16 Step 200/390 lr 0.100000 loss 0.2538 (0.2501) acc@1 0.8984 (0.9114) acc@5 0.9922 (0.9976)
[2022-06-11 09:57:56] __main__ INFO: Epoch 16 Step 300/390 lr 0.100000 loss 0.1840 (0.2512) acc@1 0.9297 (0.9117) acc@5 1.0000 (0.9978)
[2022-06-11 09:57:59] __main__ INFO: Epoch 16 Step 390/390 lr 0.100000 loss 0.2013 (0.2529) acc@1 0.9297 (0.9113) acc@5 1.0000 (0.9978)
[2022-06-11 09:58:00] __main__ INFO: Elapsed 15.56
[2022-06-11 09:58:00] __main__ INFO: Val 16
[2022-06-11 09:58:01] __main__ INFO: Epoch 16 loss 0.5293 acc@1 0.8414 acc@5 0.9895
[2022-06-11 09:58:01] __main__ INFO: Elapsed 0.97
[2022-06-11 09:58:01] __main__ INFO: Train 17 6240
[2022-06-11 09:58:05] __main__ INFO: Epoch 17 Step 100/390 lr 0.100000 loss 0.3087 (0.2463) acc@1 0.9141 (0.9148) acc@5 1.0000 (0.9982)
[2022-06-11 09:58:09] __main__ INFO: Epoch 17 Step 200/390 lr 0.100000 loss 0.3197 (0.2479) acc@1 0.8984 (0.9148) acc@5 0.9922 (0.9980)
[2022-06-11 09:58:13] __main__ INFO: Epoch 17 Step 300/390 lr 0.100000 loss 0.5097 (0.2531) acc@1 0.8125 (0.9122) acc@5 1.0000 (0.9981)
[2022-06-11 09:58:16] __main__ INFO: Epoch 17 Step 390/390 lr 0.100000 loss 0.1541 (0.2525) acc@1 0.9609 (0.9124) acc@5 0.9922 (0.9981)
[2022-06-11 09:58:16] __main__ INFO: Elapsed 15.57
[2022-06-11 09:58:16] __main__ INFO: Val 17
[2022-06-11 09:58:17] __main__ INFO: Epoch 17 loss 0.4396 acc@1 0.8584 acc@5 0.9936
[2022-06-11 09:58:17] __main__ INFO: Elapsed 0.95
[2022-06-11 09:58:17] __main__ INFO: Train 18 6630
[2022-06-11 09:58:21] __main__ INFO: Epoch 18 Step 100/390 lr 0.100000 loss 0.3825 (0.2298) acc@1 0.8828 (0.9193) acc@5 1.0000 (0.9986)
[2022-06-11 09:58:25] __main__ INFO: Epoch 18 Step 200/390 lr 0.100000 loss 0.3288 (0.2344) acc@1 0.8984 (0.9167) acc@5 1.0000 (0.9986)
[2022-06-11 09:58:29] __main__ INFO: Epoch 18 Step 300/390 lr 0.100000 loss 0.2586 (0.2370) acc@1 0.8906 (0.9166) acc@5 1.0000 (0.9983)
[2022-06-11 09:58:33] __main__ INFO: Epoch 18 Step 390/390 lr 0.100000 loss 0.2421 (0.2443) acc@1 0.9453 (0.9141) acc@5 1.0000 (0.9982)
[2022-06-11 09:58:33] __main__ INFO: Elapsed 15.63
[2022-06-11 09:58:33] __main__ INFO: Val 18
[2022-06-11 09:58:34] __main__ INFO: Epoch 18 loss 0.4365 acc@1 0.8644 acc@5 0.9945
[2022-06-11 09:58:34] __main__ INFO: Elapsed 0.93
[2022-06-11 09:58:34] __main__ INFO: Train 19 7020
[2022-06-11 09:58:38] __main__ INFO: Epoch 19 Step 100/390 lr 0.100000 loss 0.1901 (0.2104) acc@1 0.9141 (0.9274) acc@5 1.0000 (0.9987)
[2022-06-11 09:58:42] __main__ INFO: Epoch 19 Step 200/390 lr 0.100000 loss 0.3619 (0.2215) acc@1 0.8750 (0.9235) acc@5 0.9922 (0.9985)
[2022-06-11 09:58:46] __main__ INFO: Epoch 19 Step 300/390 lr 0.100000 loss 0.2559 (0.2276) acc@1 0.9219 (0.9206) acc@5 1.0000 (0.9984)
[2022-06-11 09:58:49] __main__ INFO: Epoch 19 Step 390/390 lr 0.100000 loss 0.3109 (0.2328) acc@1 0.8750 (0.9185) acc@5 0.9922 (0.9982)
[2022-06-11 09:58:49] __main__ INFO: Elapsed 15.78
[2022-06-11 09:58:49] __main__ INFO: Val 19
[2022-06-11 09:58:50] __main__ INFO: Epoch 19 loss 0.3774 acc@1 0.8784 acc@5 0.9946
[2022-06-11 09:58:50] __main__ INFO: Elapsed 0.95
[2022-06-11 09:58:50] __main__ INFO: Train 20 7410
[2022-06-11 09:58:55] __main__ INFO: Epoch 20 Step 100/390 lr 0.100000 loss 0.2223 (0.2269) acc@1 0.9297 (0.9200) acc@5 1.0000 (0.9983)
[2022-06-11 09:58:59] __main__ INFO: Epoch 20 Step 200/390 lr 0.100000 loss 0.2705 (0.2316) acc@1 0.9062 (0.9187) acc@5 1.0000 (0.9982)
[2022-06-11 09:59:03] __main__ INFO: Epoch 20 Step 300/390 lr 0.100000 loss 0.1986 (0.2356) acc@1 0.9375 (0.9174) acc@5 0.9922 (0.9982)
[2022-06-11 09:59:07] __main__ INFO: Epoch 20 Step 390/390 lr 0.100000 loss 0.1408 (0.2381) acc@1 0.9531 (0.9170) acc@5 1.0000 (0.9982)
[2022-06-11 09:59:07] __main__ INFO: Elapsed 16.32
[2022-06-11 09:59:07] __main__ INFO: Val 20
[2022-06-11 09:59:08] __main__ INFO: Epoch 20 loss 0.4564 acc@1 0.8569 acc@5 0.9947
[2022-06-11 09:59:08] __main__ INFO: Elapsed 0.89
[2022-06-11 09:59:08] __main__ INFO: Train 21 7800
[2022-06-11 09:59:12] __main__ INFO: Epoch 21 Step 100/390 lr 0.100000 loss 0.1707 (0.2163) acc@1 0.9531 (0.9220) acc@5 1.0000 (0.9983)
[2022-06-11 09:59:16] __main__ INFO: Epoch 21 Step 200/390 lr 0.100000 loss 0.1988 (0.2241) acc@1 0.9219 (0.9199) acc@5 1.0000 (0.9985)
[2022-06-11 09:59:20] __main__ INFO: Epoch 21 Step 300/390 lr 0.100000 loss 0.3306 (0.2283) acc@1 0.9062 (0.9183) acc@5 0.9844 (0.9985)
[2022-06-11 09:59:23] __main__ INFO: Epoch 21 Step 390/390 lr 0.100000 loss 0.3509 (0.2326) acc@1 0.8672 (0.9174) acc@5 1.0000 (0.9984)
[2022-06-11 09:59:23] __main__ INFO: Elapsed 15.63
[2022-06-11 09:59:23] __main__ INFO: Val 21
[2022-06-11 09:59:24] __main__ INFO: Epoch 21 loss 0.4912 acc@1 0.8528 acc@5 0.9932
[2022-06-11 09:59:24] __main__ INFO: Elapsed 0.96
[2022-06-11 09:59:24] __main__ INFO: Train 22 8190
[2022-06-11 09:59:28] __main__ INFO: Epoch 22 Step 100/390 lr 0.100000 loss 0.2923 (0.2092) acc@1 0.9062 (0.9288) acc@5 1.0000 (0.9988)
[2022-06-11 09:59:32] __main__ INFO: Epoch 22 Step 200/390 lr 0.100000 loss 0.2052 (0.2269) acc@1 0.9062 (0.9220) acc@5 1.0000 (0.9985)
[2022-06-11 09:59:36] __main__ INFO: Epoch 22 Step 300/390 lr 0.100000 loss 0.1581 (0.2299) acc@1 0.9297 (0.9211) acc@5 1.0000 (0.9985)
[2022-06-11 09:59:40] __main__ INFO: Epoch 22 Step 390/390 lr 0.100000 loss 0.3109 (0.2303) acc@1 0.9297 (0.9210) acc@5 0.9922 (0.9985)
[2022-06-11 09:59:40] __main__ INFO: Elapsed 15.63
[2022-06-11 09:59:40] __main__ INFO: Val 22
[2022-06-11 09:59:41] __main__ INFO: Epoch 22 loss 0.3968 acc@1 0.8791 acc@5 0.9952
[2022-06-11 09:59:41] __main__ INFO: Elapsed 0.97
[2022-06-11 09:59:41] __main__ INFO: Train 23 8580
[2022-06-11 09:59:45] __main__ INFO: Epoch 23 Step 100/390 lr 0.100000 loss 0.2176 (0.2012) acc@1 0.9219 (0.9317) acc@5 1.0000 (0.9988)
[2022-06-11 09:59:49] __main__ INFO: Epoch 23 Step 200/390 lr 0.100000 loss 0.2428 (0.2105) acc@1 0.9219 (0.9268) acc@5 0.9922 (0.9986)
[2022-06-11 09:59:53] __main__ INFO: Epoch 23 Step 300/390 lr 0.100000 loss 0.3369 (0.2157) acc@1 0.8906 (0.9245) acc@5 1.0000 (0.9986)
[2022-06-11 09:59:56] __main__ INFO: Epoch 23 Step 390/390 lr 0.100000 loss 0.1396 (0.2228) acc@1 0.9219 (0.9221) acc@5 1.0000 (0.9985)
[2022-06-11 09:59:56] __main__ INFO: Elapsed 15.50
[2022-06-11 09:59:56] __main__ INFO: Val 23
[2022-06-11 09:59:57] __main__ INFO: Epoch 23 loss 0.4814 acc@1 0.8582 acc@5 0.9936
[2022-06-11 09:59:57] __main__ INFO: Elapsed 0.94
[2022-06-11 09:59:57] __main__ INFO: Train 24 8970
[2022-06-11 10:00:01] __main__ INFO: Epoch 24 Step 100/390 lr 0.100000 loss 0.2640 (0.2123) acc@1 0.9219 (0.9257) acc@5 0.9922 (0.9984)
[2022-06-11 10:00:05] __main__ INFO: Epoch 24 Step 200/390 lr 0.100000 loss 0.2256 (0.2186) acc@1 0.8984 (0.9221) acc@5 1.0000 (0.9986)
[2022-06-11 10:00:09] __main__ INFO: Epoch 24 Step 300/390 lr 0.100000 loss 0.2190 (0.2230) acc@1 0.9219 (0.9201) acc@5 1.0000 (0.9986)
[2022-06-11 10:00:13] __main__ INFO: Epoch 24 Step 390/390 lr 0.100000 loss 0.2595 (0.2266) acc@1 0.9141 (0.9193) acc@5 1.0000 (0.9986)
[2022-06-11 10:00:13] __main__ INFO: Elapsed 15.50
[2022-06-11 10:00:13] __main__ INFO: Val 24
[2022-06-11 10:00:14] __main__ INFO: Epoch 24 loss 0.4320 acc@1 0.8675 acc@5 0.9939
[2022-06-11 10:00:14] __main__ INFO: Elapsed 0.97
[2022-06-11 10:00:14] __main__ INFO: Train 25 9360
[2022-06-11 10:00:18] __main__ INFO: Epoch 25 Step 100/390 lr 0.100000 loss 0.1483 (0.2158) acc@1 0.9609 (0.9260) acc@5 1.0000 (0.9986)
[2022-06-11 10:00:22] __main__ INFO: Epoch 25 Step 200/390 lr 0.100000 loss 0.2036 (0.2200) acc@1 0.9375 (0.9241) acc@5 1.0000 (0.9986)
[2022-06-11 10:00:26] __main__ INFO: Epoch 25 Step 300/390 lr 0.100000 loss 0.2547 (0.2236) acc@1 0.8828 (0.9220) acc@5 1.0000 (0.9986)
[2022-06-11 10:00:29] __main__ INFO: Epoch 25 Step 390/390 lr 0.100000 loss 0.2811 (0.2238) acc@1 0.9062 (0.9218) acc@5 0.9922 (0.9984)
[2022-06-11 10:00:29] __main__ INFO: Elapsed 15.53
[2022-06-11 10:00:29] __main__ INFO: Val 25
[2022-06-11 10:00:30] __main__ INFO: Epoch 25 loss 0.4351 acc@1 0.8652 acc@5 0.9954
[2022-06-11 10:00:30] __main__ INFO: Elapsed 0.93
[2022-06-11 10:00:30] __main__ INFO: Train 26 9750
[2022-06-11 10:00:34] __main__ INFO: Epoch 26 Step 100/390 lr 0.100000 loss 0.2207 (0.1958) acc@1 0.9297 (0.9322) acc@5 1.0000 (0.9991)
[2022-06-11 10:00:38] __main__ INFO: Epoch 26 Step 200/390 lr 0.100000 loss 0.2122 (0.2074) acc@1 0.9375 (0.9270) acc@5 1.0000 (0.9989)
[2022-06-11 10:00:42] __main__ INFO: Epoch 26 Step 300/390 lr 0.100000 loss 0.2799 (0.2132) acc@1 0.9453 (0.9246) acc@5 1.0000 (0.9986)
[2022-06-11 10:00:46] __main__ INFO: Epoch 26 Step 390/390 lr 0.100000 loss 0.1616 (0.2157) acc@1 0.9297 (0.9238) acc@5 1.0000 (0.9985)
[2022-06-11 10:00:46] __main__ INFO: Elapsed 15.56
[2022-06-11 10:00:46] __main__ INFO: Val 26
[2022-06-11 10:00:47] __main__ INFO: Epoch 26 loss 0.4064 acc@1 0.8751 acc@5 0.9946
[2022-06-11 10:00:47] __main__ INFO: Elapsed 0.95
[2022-06-11 10:00:47] __main__ INFO: Train 27 10140
[2022-06-11 10:00:51] __main__ INFO: Epoch 27 Step 100/390 lr 0.100000 loss 0.2962 (0.1829) acc@1 0.9062 (0.9359) acc@5 1.0000 (0.9989)
[2022-06-11 10:00:55] __main__ INFO: Epoch 27 Step 200/390 lr 0.100000 loss 0.2471 (0.1973) acc@1 0.9062 (0.9318) acc@5 1.0000 (0.9988)
[2022-06-11 10:00:59] __main__ INFO: Epoch 27 Step 300/390 lr 0.100000 loss 0.2016 (0.2016) acc@1 0.9375 (0.9293) acc@5 1.0000 (0.9990)
[2022-06-11 10:01:02] __main__ INFO: Epoch 27 Step 390/390 lr 0.100000 loss 0.2381 (0.2080) acc@1 0.8984 (0.9275) acc@5 1.0000 (0.9987)
[2022-06-11 10:01:02] __main__ INFO: Elapsed 15.60
[2022-06-11 10:01:02] __main__ INFO: Val 27
[2022-06-11 10:01:03] __main__ INFO: Epoch 27 loss 0.4610 acc@1 0.8660 acc@5 0.9921
[2022-06-11 10:01:03] __main__ INFO: Elapsed 0.92
[2022-06-11 10:01:03] __main__ INFO: Train 28 10530
[2022-06-11 10:01:07] __main__ INFO: Epoch 28 Step 100/390 lr 0.100000 loss 0.1918 (0.2094) acc@1 0.9609 (0.9268) acc@5 1.0000 (0.9990)
[2022-06-11 10:01:11] __main__ INFO: Epoch 28 Step 200/390 lr 0.100000 loss 0.3197 (0.2079) acc@1 0.8750 (0.9261) acc@5 1.0000 (0.9988)
[2022-06-11 10:01:15] __main__ INFO: Epoch 28 Step 300/390 lr 0.100000 loss 0.2019 (0.2101) acc@1 0.9219 (0.9263) acc@5 1.0000 (0.9986)
[2022-06-11 10:01:19] __main__ INFO: Epoch 28 Step 390/390 lr 0.100000 loss 0.1922 (0.2099) acc@1 0.9141 (0.9263) acc@5 1.0000 (0.9986)
[2022-06-11 10:01:19] __main__ INFO: Elapsed 15.55
[2022-06-11 10:01:19] __main__ INFO: Val 28
[2022-06-11 10:01:20] __main__ INFO: Epoch 28 loss 0.4958 acc@1 0.8564 acc@5 0.9928
[2022-06-11 10:01:20] __main__ INFO: Elapsed 0.95
[2022-06-11 10:01:20] __main__ INFO: Train 29 10920
[2022-06-11 10:01:24] __main__ INFO: Epoch 29 Step 100/390 lr 0.100000 loss 0.2178 (0.1924) acc@1 0.9219 (0.9321) acc@5 1.0000 (0.9991)
[2022-06-11 10:01:28] __main__ INFO: Epoch 29 Step 200/390 lr 0.100000 loss 0.2210 (0.2002) acc@1 0.9297 (0.9295) acc@5 1.0000 (0.9989)
[2022-06-11 10:01:32] __main__ INFO: Epoch 29 Step 300/390 lr 0.100000 loss 0.3052 (0.2045) acc@1 0.9141 (0.9283) acc@5 0.9922 (0.9989)
[2022-06-11 10:01:35] __main__ INFO: Epoch 29 Step 390/390 lr 0.100000 loss 0.2727 (0.2092) acc@1 0.9297 (0.9270) acc@5 1.0000 (0.9989)
[2022-06-11 10:01:35] __main__ INFO: Elapsed 15.75
[2022-06-11 10:01:35] __main__ INFO: Val 29
[2022-06-11 10:01:36] __main__ INFO: Epoch 29 loss 0.5581 acc@1 0.8409 acc@5 0.9924
[2022-06-11 10:01:36] __main__ INFO: Elapsed 0.98
[2022-06-11 10:01:36] __main__ INFO: Train 30 11310
[2022-06-11 10:01:41] __main__ INFO: Epoch 30 Step 100/390 lr 0.100000 loss 0.1827 (0.1978) acc@1 0.9375 (0.9315) acc@5 0.9922 (0.9990)
[2022-06-11 10:01:44] __main__ INFO: Epoch 30 Step 200/390 lr 0.100000 loss 0.1548 (0.2004) acc@1 0.9375 (0.9299) acc@5 1.0000 (0.9989)
[2022-06-11 10:01:48] __main__ INFO: Epoch 30 Step 300/390 lr 0.100000 loss 0.1607 (0.2051) acc@1 0.9297 (0.9287) acc@5 1.0000 (0.9988)
[2022-06-11 10:01:52] __main__ INFO: Epoch 30 Step 390/390 lr 0.100000 loss 0.2667 (0.2067) acc@1 0.8984 (0.9277) acc@5 1.0000 (0.9988)
[2022-06-11 10:01:52] __main__ INFO: Elapsed 15.77
[2022-06-11 10:01:52] __main__ INFO: Val 30
[2022-06-11 10:01:53] __main__ INFO: Epoch 30 loss 0.4164 acc@1 0.8700 acc@5 0.9945
[2022-06-11 10:01:53] __main__ INFO: Elapsed 0.96
[2022-06-11 10:01:53] __main__ INFO: Train 31 11700
[2022-06-11 10:01:57] __main__ INFO: Epoch 31 Step 100/390 lr 0.100000 loss 0.2197 (0.1952) acc@1 0.9375 (0.9316) acc@5 0.9922 (0.9992)
[2022-06-11 10:02:01] __main__ INFO: Epoch 31 Step 200/390 lr 0.100000 loss 0.1855 (0.2028) acc@1 0.9297 (0.9300) acc@5 1.0000 (0.9990)
[2022-06-11 10:02:05] __main__ INFO: Epoch 31 Step 300/390 lr 0.100000 loss 0.2148 (0.2064) acc@1 0.9062 (0.9276) acc@5 1.0000 (0.9991)
[2022-06-11 10:02:09] __main__ INFO: Epoch 31 Step 390/390 lr 0.100000 loss 0.1806 (0.2105) acc@1 0.9219 (0.9262) acc@5 1.0000 (0.9988)
[2022-06-11 10:02:09] __main__ INFO: Elapsed 15.62
[2022-06-11 10:02:09] __main__ INFO: Val 31
[2022-06-11 10:02:10] __main__ INFO: Epoch 31 loss 0.3997 acc@1 0.8771 acc@5 0.9951
[2022-06-11 10:02:10] __main__ INFO: Elapsed 0.94
[2022-06-11 10:02:10] __main__ INFO: Train 32 12090
[2022-06-11 10:02:14] __main__ INFO: Epoch 32 Step 100/390 lr 0.100000 loss 0.1306 (0.1904) acc@1 0.9531 (0.9348) acc@5 1.0000 (0.9991)
[2022-06-11 10:02:18] __main__ INFO: Epoch 32 Step 200/390 lr 0.100000 loss 0.1924 (0.1948) acc@1 0.9219 (0.9333) acc@5 1.0000 (0.9989)
[2022-06-11 10:02:22] __main__ INFO: Epoch 32 Step 300/390 lr 0.100000 loss 0.2958 (0.1998) acc@1 0.9297 (0.9305) acc@5 1.0000 (0.9990)
[2022-06-11 10:02:26] __main__ INFO: Epoch 32 Step 390/390 lr 0.100000 loss 0.1743 (0.2026) acc@1 0.9219 (0.9288) acc@5 0.9922 (0.9989)
[2022-06-11 10:02:26] __main__ INFO: Elapsed 15.95
[2022-06-11 10:02:26] __main__ INFO: Val 32
[2022-06-11 10:02:27] __main__ INFO: Epoch 32 loss 0.4223 acc@1 0.8786 acc@5 0.9954
[2022-06-11 10:02:27] __main__ INFO: Elapsed 0.92
[2022-06-11 10:02:27] __main__ INFO: Train 33 12480
[2022-06-11 10:02:31] __main__ INFO: Epoch 33 Step 100/390 lr 0.100000 loss 0.1936 (0.1829) acc@1 0.9297 (0.9362) acc@5 1.0000 (0.9990)
[2022-06-11 10:02:35] __main__ INFO: Epoch 33 Step 200/390 lr 0.100000 loss 0.3342 (0.1909) acc@1 0.8984 (0.9341) acc@5 1.0000 (0.9989)
[2022-06-11 10:02:39] __main__ INFO: Epoch 33 Step 300/390 lr 0.100000 loss 0.1701 (0.1961) acc@1 0.9375 (0.9311) acc@5 1.0000 (0.9990)
[2022-06-11 10:02:42] __main__ INFO: Epoch 33 Step 390/390 lr 0.100000 loss 0.2334 (0.2034) acc@1 0.9219 (0.9284) acc@5 1.0000 (0.9990)
[2022-06-11 10:02:42] __main__ INFO: Elapsed 15.72
[2022-06-11 10:02:42] __main__ INFO: Val 33
[2022-06-11 10:02:43] __main__ INFO: Epoch 33 loss 0.4228 acc@1 0.8734 acc@5 0.9954
[2022-06-11 10:02:43] __main__ INFO: Elapsed 0.90
[2022-06-11 10:02:43] __main__ INFO: Train 34 12870
[2022-06-11 10:02:47] __main__ INFO: Epoch 34 Step 100/390 lr 0.100000 loss 0.1577 (0.1904) acc@1 0.9375 (0.9362) acc@5 1.0000 (0.9991)
[2022-06-11 10:02:51] __main__ INFO: Epoch 34 Step 200/390 lr 0.100000 loss 0.3170 (0.1905) acc@1 0.8672 (0.9330) acc@5 1.0000 (0.9993)
[2022-06-11 10:02:55] __main__ INFO: Epoch 34 Step 300/390 lr 0.100000 loss 0.1309 (0.1966) acc@1 0.9453 (0.9306) acc@5 1.0000 (0.9990)
[2022-06-11 10:02:59] __main__ INFO: Epoch 34 Step 390/390 lr 0.100000 loss 0.2791 (0.2001) acc@1 0.8906 (0.9289) acc@5 1.0000 (0.9990)
[2022-06-11 10:02:59] __main__ INFO: Elapsed 15.79
[2022-06-11 10:02:59] __main__ INFO: Val 34
[2022-06-11 10:03:00] __main__ INFO: Epoch 34 loss 0.4272 acc@1 0.8689 acc@5 0.9940
[2022-06-11 10:03:00] __main__ INFO: Elapsed 0.99
[2022-06-11 10:03:00] __main__ INFO: Train 35 13260
[2022-06-11 10:03:04] __main__ INFO: Epoch 35 Step 100/390 lr 0.100000 loss 0.2388 (0.1837) acc@1 0.9141 (0.9379) acc@5 1.0000 (0.9992)
[2022-06-11 10:03:08] __main__ INFO: Epoch 35 Step 200/390 lr 0.100000 loss 0.2387 (0.1900) acc@1 0.9062 (0.9340) acc@5 1.0000 (0.9991)
[2022-06-11 10:03:12] __main__ INFO: Epoch 35 Step 300/390 lr 0.100000 loss 0.2687 (0.1965) acc@1 0.9453 (0.9315) acc@5 1.0000 (0.9992)
[2022-06-11 10:03:16] __main__ INFO: Epoch 35 Step 390/390 lr 0.100000 loss 0.2031 (0.2003) acc@1 0.9141 (0.9301) acc@5 1.0000 (0.9990)
[2022-06-11 10:03:16] __main__ INFO: Elapsed 15.65
[2022-06-11 10:03:16] __main__ INFO: Val 35
[2022-06-11 10:03:17] __main__ INFO: Epoch 35 loss 0.4054 acc@1 0.8770 acc@5 0.9956
[2022-06-11 10:03:17] __main__ INFO: Elapsed 0.95
[2022-06-11 10:03:17] __main__ INFO: Train 36 13650
[2022-06-11 10:03:21] __main__ INFO: Epoch 36 Step 100/390 lr 0.100000 loss 0.0925 (0.1793) acc@1 0.9531 (0.9370) acc@5 1.0000 (0.9991)
[2022-06-11 10:03:25] __main__ INFO: Epoch 36 Step 200/390 lr 0.100000 loss 0.1553 (0.1852) acc@1 0.9297 (0.9345) acc@5 1.0000 (0.9991)
[2022-06-11 10:03:29] __main__ INFO: Epoch 36 Step 300/390 lr 0.100000 loss 0.2535 (0.1913) acc@1 0.9062 (0.9318) acc@5 0.9922 (0.9990)
[2022-06-11 10:03:32] __main__ INFO: Epoch 36 Step 390/390 lr 0.100000 loss 0.2743 (0.1955) acc@1 0.8984 (0.9311) acc@5 1.0000 (0.9989)
[2022-06-11 10:03:32] __main__ INFO: Elapsed 15.78
[2022-06-11 10:03:32] __main__ INFO: Val 36
[2022-06-11 10:03:33] __main__ INFO: Epoch 36 loss 0.3772 acc@1 0.8797 acc@5 0.9959
[2022-06-11 10:03:33] __main__ INFO: Elapsed 0.93
[2022-06-11 10:03:33] __main__ INFO: Train 37 14040
[2022-06-11 10:03:37] __main__ INFO: Epoch 37 Step 100/390 lr 0.100000 loss 0.1979 (0.1819) acc@1 0.9141 (0.9353) acc@5 1.0000 (0.9995)
[2022-06-11 10:03:41] __main__ INFO: Epoch 37 Step 200/390 lr 0.100000 loss 0.1855 (0.1884) acc@1 0.9297 (0.9330) acc@5 1.0000 (0.9994)
[2022-06-11 10:03:45] __main__ INFO: Epoch 37 Step 300/390 lr 0.100000 loss 0.2755 (0.1912) acc@1 0.9219 (0.9326) acc@5 1.0000 (0.9994)
[2022-06-11 10:03:49] __main__ INFO: Epoch 37 Step 390/390 lr 0.100000 loss 0.0967 (0.1929) acc@1 0.9844 (0.9319) acc@5 1.0000 (0.9994)
[2022-06-11 10:03:49] __main__ INFO: Elapsed 15.84
[2022-06-11 10:03:49] __main__ INFO: Val 37
[2022-06-11 10:03:50] __main__ INFO: Epoch 37 loss 0.4156 acc@1 0.8763 acc@5 0.9949
[2022-06-11 10:03:50] __main__ INFO: Elapsed 0.89
[2022-06-11 10:03:50] __main__ INFO: Train 38 14430
[2022-06-11 10:03:54] __main__ INFO: Epoch 38 Step 100/390 lr 0.100000 loss 0.1796 (0.1760) acc@1 0.9297 (0.9382) acc@5 1.0000 (0.9995)
[2022-06-11 10:03:59] __main__ INFO: Epoch 38 Step 200/390 lr 0.100000 loss 0.1583 (0.1833) acc@1 0.9453 (0.9358) acc@5 1.0000 (0.9992)
[2022-06-11 10:04:03] __main__ INFO: Epoch 38 Step 300/390 lr 0.100000 loss 0.2516 (0.1901) acc@1 0.9297 (0.9332) acc@5 1.0000 (0.9991)
[2022-06-11 10:04:06] __main__ INFO: Epoch 38 Step 390/390 lr 0.100000 loss 0.1651 (0.1965) acc@1 0.9453 (0.9308) acc@5 1.0000 (0.9990)
[2022-06-11 10:04:06] __main__ INFO: Elapsed 16.26
[2022-06-11 10:04:06] __main__ INFO: Val 38
[2022-06-11 10:04:07] __main__ INFO: Epoch 38 loss 0.4688 acc@1 0.8599 acc@5 0.9940
[2022-06-11 10:04:07] __main__ INFO: Elapsed 0.98
[2022-06-11 10:04:07] __main__ INFO: Train 39 14820
[2022-06-11 10:04:12] __main__ INFO: Epoch 39 Step 100/390 lr 0.100000 loss 0.1424 (0.1797) acc@1 0.9453 (0.9363) acc@5 1.0000 (0.9989)
[2022-06-11 10:04:16] __main__ INFO: Epoch 39 Step 200/390 lr 0.100000 loss 0.3179 (0.1899) acc@1 0.8594 (0.9323) acc@5 1.0000 (0.9993)
[2022-06-11 10:04:20] __main__ INFO: Epoch 39 Step 300/390 lr 0.100000 loss 0.1436 (0.1925) acc@1 0.9375 (0.9316) acc@5 1.0000 (0.9993)
[2022-06-11 10:04:23] __main__ INFO: Epoch 39 Step 390/390 lr 0.100000 loss 0.2037 (0.1955) acc@1 0.9453 (0.9310) acc@5 1.0000 (0.9992)
[2022-06-11 10:04:23] __main__ INFO: Elapsed 15.79
[2022-06-11 10:04:23] __main__ INFO: Val 39
[2022-06-11 10:04:24] __main__ INFO: Epoch 39 loss 0.4232 acc@1 0.8701 acc@5 0.9951
[2022-06-11 10:04:24] __main__ INFO: Elapsed 0.95
[2022-06-11 10:04:24] __main__ INFO: Train 40 15210
[2022-06-11 10:04:28] __main__ INFO: Epoch 40 Step 100/390 lr 0.100000 loss 0.2878 (0.1744) acc@1 0.8984 (0.9397) acc@5 0.9922 (0.9992)
[2022-06-11 10:04:32] __main__ INFO: Epoch 40 Step 200/390 lr 0.100000 loss 0.2188 (0.1856) acc@1 0.9297 (0.9358) acc@5 1.0000 (0.9990)
[2022-06-11 10:04:36] __main__ INFO: Epoch 40 Step 300/390 lr 0.100000 loss 0.1971 (0.1897) acc@1 0.9375 (0.9339) acc@5 0.9922 (0.9989)
[2022-06-11 10:04:40] __main__ INFO: Epoch 40 Step 390/390 lr 0.100000 loss 0.2149 (0.1933) acc@1 0.9297 (0.9326) acc@5 1.0000 (0.9989)
[2022-06-11 10:04:40] __main__ INFO: Elapsed 15.85
[2022-06-11 10:04:40] __main__ INFO: Val 40
[2022-06-11 10:04:41] __main__ INFO: Epoch 40 loss 0.3929 acc@1 0.8760 acc@5 0.9955
[2022-06-11 10:04:41] __main__ INFO: Elapsed 0.91
[2022-06-11 10:04:41] __main__ INFO: Train 41 15600
[2022-06-11 10:04:45] __main__ INFO: Epoch 41 Step 100/390 lr 0.100000 loss 0.1115 (0.1816) acc@1 0.9766 (0.9355) acc@5 1.0000 (0.9994)
[2022-06-11 10:04:49] __main__ INFO: Epoch 41 Step 200/390 lr 0.100000 loss 0.1483 (0.1864) acc@1 0.9453 (0.9334) acc@5 1.0000 (0.9990)
[2022-06-11 10:04:53] __main__ INFO: Epoch 41 Step 300/390 lr 0.100000 loss 0.2876 (0.1903) acc@1 0.8672 (0.9323) acc@5 1.0000 (0.9989)
[2022-06-11 10:04:57] __main__ INFO: Epoch 41 Step 390/390 lr 0.100000 loss 0.3327 (0.1921) acc@1 0.9219 (0.9317) acc@5 0.9922 (0.9989)
[2022-06-11 10:04:57] __main__ INFO: Elapsed 15.83
[2022-06-11 10:04:57] __main__ INFO: Val 41
[2022-06-11 10:04:58] __main__ INFO: Epoch 41 loss 0.4218 acc@1 0.8714 acc@5 0.9947
[2022-06-11 10:04:58] __main__ INFO: Elapsed 0.96
[2022-06-11 10:04:58] __main__ INFO: Train 42 15990
[2022-06-11 10:05:02] __main__ INFO: Epoch 42 Step 100/390 lr 0.100000 loss 0.3850 (0.1839) acc@1 0.8359 (0.9380) acc@5 1.0000 (0.9989)
[2022-06-11 10:05:06] __main__ INFO: Epoch 42 Step 200/390 lr 0.100000 loss 0.1581 (0.1765) acc@1 0.9453 (0.9396) acc@5 1.0000 (0.9989)
[2022-06-11 10:05:10] __main__ INFO: Epoch 42 Step 300/390 lr 0.100000 loss 0.2368 (0.1817) acc@1 0.9297 (0.9372) acc@5 1.0000 (0.9990)
[2022-06-11 10:05:13] __main__ INFO: Epoch 42 Step 390/390 lr 0.100000 loss 0.2493 (0.1878) acc@1 0.9219 (0.9345) acc@5 1.0000 (0.9990)
[2022-06-11 10:05:13] __main__ INFO: Elapsed 15.66
[2022-06-11 10:05:13] __main__ INFO: Val 42
[2022-06-11 10:05:14] __main__ INFO: Epoch 42 loss 0.4103 acc@1 0.8779 acc@5 0.9953
[2022-06-11 10:05:14] __main__ INFO: Elapsed 0.94
[2022-06-11 10:05:14] __main__ INFO: Train 43 16380
[2022-06-11 10:05:19] __main__ INFO: Epoch 43 Step 100/390 lr 0.100000 loss 0.2776 (0.1786) acc@1 0.9297 (0.9345) acc@5 1.0000 (0.9995)
[2022-06-11 10:05:23] __main__ INFO: Epoch 43 Step 200/390 lr 0.100000 loss 0.2200 (0.1868) acc@1 0.9297 (0.9334) acc@5 1.0000 (0.9993)
[2022-06-11 10:05:26] __main__ INFO: Epoch 43 Step 300/390 lr 0.100000 loss 0.2154 (0.1880) acc@1 0.9062 (0.9334) acc@5 1.0000 (0.9991)
[2022-06-11 10:05:30] __main__ INFO: Epoch 43 Step 390/390 lr 0.100000 loss 0.2582 (0.1914) acc@1 0.8984 (0.9320) acc@5 1.0000 (0.9991)
[2022-06-11 10:05:30] __main__ INFO: Elapsed 15.87
[2022-06-11 10:05:30] __main__ INFO: Val 43
[2022-06-11 10:05:31] __main__ INFO: Epoch 43 loss 0.5084 acc@1 0.8622 acc@5 0.9936
[2022-06-11 10:05:31] __main__ INFO: Elapsed 0.96
[2022-06-11 10:05:31] __main__ INFO: Train 44 16770
[2022-06-11 10:05:35] __main__ INFO: Epoch 44 Step 100/390 lr 0.100000 loss 0.1208 (0.1703) acc@1 0.9453 (0.9409) acc@5 1.0000 (0.9993)
[2022-06-11 10:05:39] __main__ INFO: Epoch 44 Step 200/390 lr 0.100000 loss 0.1179 (0.1782) acc@1 0.9688 (0.9377) acc@5 1.0000 (0.9994)
[2022-06-11 10:05:43] __main__ INFO: Epoch 44 Step 300/390 lr 0.100000 loss 0.1647 (0.1836) acc@1 0.9688 (0.9357) acc@5 1.0000 (0.9992)
[2022-06-11 10:05:47] __main__ INFO: Epoch 44 Step 390/390 lr 0.100000 loss 0.2235 (0.1863) acc@1 0.9297 (0.9347) acc@5 1.0000 (0.9992)
[2022-06-11 10:05:47] __main__ INFO: Elapsed 15.79
[2022-06-11 10:05:47] __main__ INFO: Val 44
[2022-06-11 10:05:48] __main__ INFO: Epoch 44 loss 0.4580 acc@1 0.8690 acc@5 0.9946
[2022-06-11 10:05:48] __main__ INFO: Elapsed 0.97
[2022-06-11 10:05:48] __main__ INFO: Train 45 17160
[2022-06-11 10:05:52] __main__ INFO: Epoch 45 Step 100/390 lr 0.100000 loss 0.1519 (0.1650) acc@1 0.9375 (0.9432) acc@5 1.0000 (0.9993)
[2022-06-11 10:05:56] __main__ INFO: Epoch 45 Step 200/390 lr 0.100000 loss 0.1957 (0.1771) acc@1 0.9219 (0.9378) acc@5 0.9922 (0.9993)
[2022-06-11 10:06:00] __main__ INFO: Epoch 45 Step 300/390 lr 0.100000 loss 0.2230 (0.1845) acc@1 0.9141 (0.9360) acc@5 1.0000 (0.9992)
[2022-06-11 10:06:04] __main__ INFO: Epoch 45 Step 390/390 lr 0.100000 loss 0.1999 (0.1852) acc@1 0.9297 (0.9358) acc@5 1.0000 (0.9991)
[2022-06-11 10:06:04] __main__ INFO: Elapsed 15.83
[2022-06-11 10:06:04] __main__ INFO: Val 45
[2022-06-11 10:06:05] __main__ INFO: Epoch 45 loss 0.3907 acc@1 0.8825 acc@5 0.9967
[2022-06-11 10:06:05] __main__ INFO: Elapsed 0.92
[2022-06-11 10:06:05] __main__ INFO: Train 46 17550
[2022-06-11 10:06:09] __main__ INFO: Epoch 46 Step 100/390 lr 0.100000 loss 0.1677 (0.1566) acc@1 0.9453 (0.9447) acc@5 1.0000 (0.9995)
[2022-06-11 10:06:13] __main__ INFO: Epoch 46 Step 200/390 lr 0.100000 loss 0.2985 (0.1689) acc@1 0.9062 (0.9402) acc@5 1.0000 (0.9993)
[2022-06-11 10:06:17] __main__ INFO: Epoch 46 Step 300/390 lr 0.100000 loss 0.0896 (0.1797) acc@1 0.9531 (0.9368) acc@5 1.0000 (0.9992)
[2022-06-11 10:06:20] __main__ INFO: Epoch 46 Step 390/390 lr 0.100000 loss 0.2303 (0.1832) acc@1 0.9219 (0.9353) acc@5 1.0000 (0.9992)
[2022-06-11 10:06:20] __main__ INFO: Elapsed 15.65
[2022-06-11 10:06:20] __main__ INFO: Val 46
[2022-06-11 10:06:21] __main__ INFO: Epoch 46 loss 0.4425 acc@1 0.8647 acc@5 0.9939
[2022-06-11 10:06:21] __main__ INFO: Elapsed 0.95
[2022-06-11 10:06:21] __main__ INFO: Train 47 17940
[2022-06-11 10:06:25] __main__ INFO: Epoch 47 Step 100/390 lr 0.100000 loss 0.2178 (0.1695) acc@1 0.9141 (0.9415) acc@5 1.0000 (0.9992)
[2022-06-11 10:06:29] __main__ INFO: Epoch 47 Step 200/390 lr 0.100000 loss 0.1933 (0.1816) acc@1 0.9297 (0.9379) acc@5 1.0000 (0.9992)
[2022-06-11 10:06:33] __main__ INFO: Epoch 47 Step 300/390 lr 0.100000 loss 0.3105 (0.1862) acc@1 0.9062 (0.9351) acc@5 1.0000 (0.9992)
[2022-06-11 10:06:37] __main__ INFO: Epoch 47 Step 390/390 lr 0.100000 loss 0.1814 (0.1886) acc@1 0.9375 (0.9341) acc@5 1.0000 (0.9992)
[2022-06-11 10:06:37] __main__ INFO: Elapsed 15.85
[2022-06-11 10:06:37] __main__ INFO: Val 47
[2022-06-11 10:06:38] __main__ INFO: Epoch 47 loss 0.4524 acc@1 0.8668 acc@5 0.9920
[2022-06-11 10:06:38] __main__ INFO: Elapsed 0.94
[2022-06-11 10:06:38] __main__ INFO: Train 48 18330
[2022-06-11 10:06:42] __main__ INFO: Epoch 48 Step 100/390 lr 0.100000 loss 0.1483 (0.1804) acc@1 0.9453 (0.9360) acc@5 1.0000 (0.9991)
[2022-06-11 10:06:46] __main__ INFO: Epoch 48 Step 200/390 lr 0.100000 loss 0.1792 (0.1800) acc@1 0.9375 (0.9362) acc@5 1.0000 (0.9992)
[2022-06-11 10:06:50] __main__ INFO: Epoch 48 Step 300/390 lr 0.100000 loss 0.1676 (0.1832) acc@1 0.9375 (0.9355) acc@5 1.0000 (0.9993)
[2022-06-11 10:06:54] __main__ INFO: Epoch 48 Step 390/390 lr 0.100000 loss 0.1512 (0.1830) acc@1 0.9453 (0.9358) acc@5 1.0000 (0.9992)
[2022-06-11 10:06:54] __main__ INFO: Elapsed 15.77
[2022-06-11 10:06:54] __main__ INFO: Val 48
[2022-06-11 10:06:55] __main__ INFO: Epoch 48 loss 0.4523 acc@1 0.8715 acc@5 0.9946
[2022-06-11 10:06:55] __main__ INFO: Elapsed 0.93
[2022-06-11 10:06:55] __main__ INFO: Train 49 18720
[2022-06-11 10:06:59] __main__ INFO: Epoch 49 Step 100/390 lr 0.100000 loss 0.1473 (0.1686) acc@1 0.9531 (0.9405) acc@5 1.0000 (0.9995)
[2022-06-11 10:07:03] __main__ INFO: Epoch 49 Step 200/390 lr 0.100000 loss 0.2297 (0.1722) acc@1 0.8906 (0.9387) acc@5 1.0000 (0.9994)
[2022-06-11 10:07:07] __main__ INFO: Epoch 49 Step 300/390 lr 0.100000 loss 0.2212 (0.1798) acc@1 0.9141 (0.9367) acc@5 1.0000 (0.9994)
[2022-06-11 10:07:10] __main__ INFO: Epoch 49 Step 390/390 lr 0.100000 loss 0.1897 (0.1804) acc@1 0.9297 (0.9364) acc@5 1.0000 (0.9993)
[2022-06-11 10:07:10] __main__ INFO: Elapsed 15.75
[2022-06-11 10:07:10] __main__ INFO: Val 49
[2022-06-11 10:07:11] __main__ INFO: Epoch 49 loss 0.4338 acc@1 0.8710 acc@5 0.9948
[2022-06-11 10:07:11] __main__ INFO: Elapsed 0.91
[2022-06-11 10:07:11] __main__ INFO: Train 50 19110
[2022-06-11 10:07:15] __main__ INFO: Epoch 50 Step 100/390 lr 0.100000 loss 0.1651 (0.1608) acc@1 0.9219 (0.9437) acc@5 1.0000 (0.9995)
[2022-06-11 10:07:19] __main__ INFO: Epoch 50 Step 200/390 lr 0.100000 loss 0.0937 (0.1710) acc@1 0.9609 (0.9393) acc@5 1.0000 (0.9993)
[2022-06-11 10:07:23] __main__ INFO: Epoch 50 Step 300/390 lr 0.100000 loss 0.2250 (0.1719) acc@1 0.9297 (0.9389) acc@5 1.0000 (0.9994)
[2022-06-11 10:07:27] __main__ INFO: Epoch 50 Step 390/390 lr 0.100000 loss 0.2063 (0.1785) acc@1 0.9219 (0.9366) acc@5 1.0000 (0.9993)
[2022-06-11 10:07:27] __main__ INFO: Elapsed 15.67
[2022-06-11 10:07:27] __main__ INFO: Val 50
[2022-06-11 10:07:28] __main__ INFO: Epoch 50 loss 0.4181 acc@1 0.8736 acc@5 0.9954
[2022-06-11 10:07:28] __main__ INFO: Elapsed 0.95
[2022-06-11 10:07:28] __main__ INFO: Train 51 19500
[2022-06-11 10:07:32] __main__ INFO: Epoch 51 Step 100/390 lr 0.100000 loss 0.0999 (0.1558) acc@1 0.9609 (0.9446) acc@5 1.0000 (0.9995)
[2022-06-11 10:07:36] __main__ INFO: Epoch 51 Step 200/390 lr 0.100000 loss 0.1540 (0.1622) acc@1 0.9453 (0.9429) acc@5 1.0000 (0.9994)
[2022-06-11 10:07:40] __main__ INFO: Epoch 51 Step 300/390 lr 0.100000 loss 0.1172 (0.1710) acc@1 0.9688 (0.9401) acc@5 1.0000 (0.9992)
[2022-06-11 10:07:44] __main__ INFO: Epoch 51 Step 390/390 lr 0.100000 loss 0.1296 (0.1752) acc@1 0.9531 (0.9381) acc@5 1.0000 (0.9992)
[2022-06-11 10:07:44] __main__ INFO: Elapsed 15.64
[2022-06-11 10:07:44] __main__ INFO: Val 51
[2022-06-11 10:07:44] __main__ INFO: Epoch 51 loss 0.5047 acc@1 0.8587 acc@5 0.9935
[2022-06-11 10:07:44] __main__ INFO: Elapsed 0.88
[2022-06-11 10:07:44] __main__ INFO: Train 52 19890
[2022-06-11 10:07:49] __main__ INFO: Epoch 52 Step 100/390 lr 0.100000 loss 0.1697 (0.1621) acc@1 0.9531 (0.9424) acc@5 1.0000 (0.9995)
[2022-06-11 10:07:53] __main__ INFO: Epoch 52 Step 200/390 lr 0.100000 loss 0.1860 (0.1693) acc@1 0.9375 (0.9396) acc@5 1.0000 (0.9993)
[2022-06-11 10:07:56] __main__ INFO: Epoch 52 Step 300/390 lr 0.100000 loss 0.2052 (0.1763) acc@1 0.9453 (0.9371) acc@5 1.0000 (0.9994)
[2022-06-11 10:08:00] __main__ INFO: Epoch 52 Step 390/390 lr 0.100000 loss 0.2328 (0.1786) acc@1 0.9219 (0.9365) acc@5 1.0000 (0.9994)
[2022-06-11 10:08:00] __main__ INFO: Elapsed 15.53
[2022-06-11 10:08:00] __main__ INFO: Val 52
[2022-06-11 10:08:01] __main__ INFO: Epoch 52 loss 0.5347 acc@1 0.8564 acc@5 0.9940
[2022-06-11 10:08:01] __main__ INFO: Elapsed 0.91
[2022-06-11 10:08:01] __main__ INFO: Train 53 20280
[2022-06-11 10:08:05] __main__ INFO: Epoch 53 Step 100/390 lr 0.100000 loss 0.0901 (0.1650) acc@1 0.9688 (0.9424) acc@5 1.0000 (0.9995)
[2022-06-11 10:08:09] __main__ INFO: Epoch 53 Step 200/390 lr 0.100000 loss 0.1687 (0.1689) acc@1 0.9453 (0.9408) acc@5 1.0000 (0.9993)
[2022-06-11 10:08:13] __main__ INFO: Epoch 53 Step 300/390 lr 0.100000 loss 0.1484 (0.1790) acc@1 0.9453 (0.9369) acc@5 1.0000 (0.9992)
[2022-06-11 10:08:17] __main__ INFO: Epoch 53 Step 390/390 lr 0.100000 loss 0.2219 (0.1822) acc@1 0.9062 (0.9356) acc@5 1.0000 (0.9992)
[2022-06-11 10:08:17] __main__ INFO: Elapsed 16.00
[2022-06-11 10:08:17] __main__ INFO: Val 53
[2022-06-11 10:08:18] __main__ INFO: Epoch 53 loss 0.4402 acc@1 0.8657 acc@5 0.9958
[2022-06-11 10:08:18] __main__ INFO: Elapsed 0.96
[2022-06-11 10:08:18] __main__ INFO: Train 54 20670
[2022-06-11 10:08:22] __main__ INFO: Epoch 54 Step 100/390 lr 0.100000 loss 0.2007 (0.1632) acc@1 0.9375 (0.9425) acc@5 1.0000 (0.9994)
[2022-06-11 10:08:26] __main__ INFO: Epoch 54 Step 200/390 lr 0.100000 loss 0.1342 (0.1673) acc@1 0.9609 (0.9421) acc@5 1.0000 (0.9993)
[2022-06-11 10:08:30] __main__ INFO: Epoch 54 Step 300/390 lr 0.100000 loss 0.2482 (0.1771) acc@1 0.9219 (0.9384) acc@5 1.0000 (0.9991)
[2022-06-11 10:08:33] __main__ INFO: Epoch 54 Step 390/390 lr 0.100000 loss 0.1585 (0.1794) acc@1 0.9375 (0.9371) acc@5 1.0000 (0.9991)
[2022-06-11 10:08:33] __main__ INFO: Elapsed 15.54
[2022-06-11 10:08:33] __main__ INFO: Val 54
[2022-06-11 10:08:34] __main__ INFO: Epoch 54 loss 0.3966 acc@1 0.8816 acc@5 0.9946
[2022-06-11 10:08:34] __main__ INFO: Elapsed 0.97
[2022-06-11 10:08:34] __main__ INFO: Train 55 21060
[2022-06-11 10:08:39] __main__ INFO: Epoch 55 Step 100/390 lr 0.100000 loss 0.1921 (0.1562) acc@1 0.9375 (0.9463) acc@5 1.0000 (0.9994)
[2022-06-11 10:08:43] __main__ INFO: Epoch 55 Step 200/390 lr 0.100000 loss 0.1993 (0.1639) acc@1 0.9141 (0.9417) acc@5 1.0000 (0.9995)
[2022-06-11 10:08:46] __main__ INFO: Epoch 55 Step 300/390 lr 0.100000 loss 0.1708 (0.1721) acc@1 0.9453 (0.9389) acc@5 1.0000 (0.9992)
[2022-06-11 10:08:50] __main__ INFO: Epoch 55 Step 390/390 lr 0.100000 loss 0.1495 (0.1741) acc@1 0.9531 (0.9384) acc@5 1.0000 (0.9993)
[2022-06-11 10:08:50] __main__ INFO: Elapsed 15.75
[2022-06-11 10:08:50] __main__ INFO: Val 55
[2022-06-11 10:08:51] __main__ INFO: Epoch 55 loss 0.4541 acc@1 0.8705 acc@5 0.9954
[2022-06-11 10:08:51] __main__ INFO: Elapsed 0.99
[2022-06-11 10:08:51] __main__ INFO: Train 56 21450
[2022-06-11 10:08:55] __main__ INFO: Epoch 56 Step 100/390 lr 0.100000 loss 0.1482 (0.1601) acc@1 0.9453 (0.9433) acc@5 1.0000 (0.9994)
[2022-06-11 10:08:59] __main__ INFO: Epoch 56 Step 200/390 lr 0.100000 loss 0.1875 (0.1660) acc@1 0.9375 (0.9413) acc@5 1.0000 (0.9994)
[2022-06-11 10:09:03] __main__ INFO: Epoch 56 Step 300/390 lr 0.100000 loss 0.2170 (0.1720) acc@1 0.9062 (0.9394) acc@5 1.0000 (0.9993)
[2022-06-11 10:09:07] __main__ INFO: Epoch 56 Step 390/390 lr 0.100000 loss 0.2959 (0.1779) acc@1 0.8594 (0.9374) acc@5 1.0000 (0.9992)
[2022-06-11 10:09:07] __main__ INFO: Elapsed 15.72
[2022-06-11 10:09:07] __main__ INFO: Val 56
[2022-06-11 10:09:08] __main__ INFO: Epoch 56 loss 0.5320 acc@1 0.8482 acc@5 0.9941
[2022-06-11 10:09:08] __main__ INFO: Elapsed 0.90
[2022-06-11 10:09:08] __main__ INFO: Train 57 21840
[2022-06-11 10:09:12] __main__ INFO: Epoch 57 Step 100/390 lr 0.100000 loss 0.0682 (0.1587) acc@1 0.9688 (0.9470) acc@5 1.0000 (0.9994)
[2022-06-11 10:09:16] __main__ INFO: Epoch 57 Step 200/390 lr 0.100000 loss 0.1411 (0.1619) acc@1 0.9453 (0.9455) acc@5 1.0000 (0.9994)
[2022-06-11 10:09:20] __main__ INFO: Epoch 57 Step 300/390 lr 0.100000 loss 0.2608 (0.1711) acc@1 0.9062 (0.9416) acc@5 1.0000 (0.9992)
[2022-06-11 10:09:23] __main__ INFO: Epoch 57 Step 390/390 lr 0.100000 loss 0.1162 (0.1737) acc@1 0.9609 (0.9406) acc@5 1.0000 (0.9992)
[2022-06-11 10:09:23] __main__ INFO: Elapsed 15.50
[2022-06-11 10:09:23] __main__ INFO: Val 57
[2022-06-11 10:09:24] __main__ INFO: Epoch 57 loss 0.4003 acc@1 0.8831 acc@5 0.9942
[2022-06-11 10:09:24] __main__ INFO: Elapsed 0.97
[2022-06-11 10:09:24] __main__ INFO: Train 58 22230
[2022-06-11 10:09:28] __main__ INFO: Epoch 58 Step 100/390 lr 0.100000 loss 0.1705 (0.1586) acc@1 0.9375 (0.9434) acc@5 1.0000 (0.9998)
[2022-06-11 10:09:32] __main__ INFO: Epoch 58 Step 200/390 lr 0.100000 loss 0.1757 (0.1637) acc@1 0.9297 (0.9416) acc@5 1.0000 (0.9994)
[2022-06-11 10:09:36] __main__ INFO: Epoch 58 Step 300/390 lr 0.100000 loss 0.3046 (0.1701) acc@1 0.9141 (0.9399) acc@5 0.9922 (0.9992)
[2022-06-11 10:09:40] __main__ INFO: Epoch 58 Step 390/390 lr 0.100000 loss 0.1266 (0.1733) acc@1 0.9609 (0.9391) acc@5 1.0000 (0.9991)
[2022-06-11 10:09:40] __main__ INFO: Elapsed 15.95
[2022-06-11 10:09:40] __main__ INFO: Val 58
[2022-06-11 10:09:41] __main__ INFO: Epoch 58 loss 0.5951 acc@1 0.8389 acc@5 0.9862
[2022-06-11 10:09:41] __main__ INFO: Elapsed 0.99
[2022-06-11 10:09:41] __main__ INFO: Train 59 22620
[2022-06-11 10:09:45] __main__ INFO: Epoch 59 Step 100/390 lr 0.100000 loss 0.0636 (0.1499) acc@1 0.9922 (0.9469) acc@5 1.0000 (0.9995)
[2022-06-11 10:09:49] __main__ INFO: Epoch 59 Step 200/390 lr 0.100000 loss 0.2237 (0.1619) acc@1 0.9141 (0.9429) acc@5 1.0000 (0.9995)
[2022-06-11 10:09:53] __main__ INFO: Epoch 59 Step 300/390 lr 0.100000 loss 0.1630 (0.1697) acc@1 0.9297 (0.9405) acc@5 1.0000 (0.9995)
[2022-06-11 10:09:57] __main__ INFO: Epoch 59 Step 390/390 lr 0.100000 loss 0.2786 (0.1749) acc@1 0.8984 (0.9385) acc@5 0.9922 (0.9993)
[2022-06-11 10:09:57] __main__ INFO: Elapsed 15.70
[2022-06-11 10:09:57] __main__ INFO: Val 59
[2022-06-11 10:09:58] __main__ INFO: Epoch 59 loss 0.4630 acc@1 0.8669 acc@5 0.9956
[2022-06-11 10:09:58] __main__ INFO: Elapsed 0.97
[2022-06-11 10:09:58] __main__ INFO: Train 60 23010
[2022-06-11 10:10:02] __main__ INFO: Epoch 60 Step 100/390 lr 0.100000 loss 0.1661 (0.1598) acc@1 0.9297 (0.9427) acc@5 1.0000 (0.9994)
[2022-06-11 10:10:06] __main__ INFO: Epoch 60 Step 200/390 lr 0.100000 loss 0.1581 (0.1668) acc@1 0.9531 (0.9397) acc@5 1.0000 (0.9989)
[2022-06-11 10:10:10] __main__ INFO: Epoch 60 Step 300/390 lr 0.100000 loss 0.2078 (0.1707) acc@1 0.9297 (0.9392) acc@5 0.9922 (0.9990)
[2022-06-11 10:10:14] __main__ INFO: Epoch 60 Step 390/390 lr 0.100000 loss 0.1692 (0.1724) acc@1 0.9141 (0.9387) acc@5 1.0000 (0.9990)
[2022-06-11 10:10:14] __main__ INFO: Elapsed 15.73
[2022-06-11 10:10:14] __main__ INFO: Val 60
[2022-06-11 10:10:15] __main__ INFO: Epoch 60 loss 0.4576 acc@1 0.8678 acc@5 0.9952
[2022-06-11 10:10:15] __main__ INFO: Elapsed 0.93
[2022-06-11 10:10:15] __main__ INFO: Train 61 23400
[2022-06-11 10:10:19] __main__ INFO: Epoch 61 Step 100/390 lr 0.100000 loss 0.0885 (0.1610) acc@1 0.9766 (0.9437) acc@5 0.9922 (0.9992)
[2022-06-11 10:10:23] __main__ INFO: Epoch 61 Step 200/390 lr 0.100000 loss 0.1754 (0.1645) acc@1 0.9375 (0.9424) acc@5 1.0000 (0.9991)
[2022-06-11 10:10:27] __main__ INFO: Epoch 61 Step 300/390 lr 0.100000 loss 0.0850 (0.1688) acc@1 0.9688 (0.9408) acc@5 1.0000 (0.9991)
[2022-06-11 10:10:30] __main__ INFO: Epoch 61 Step 390/390 lr 0.100000 loss 0.1750 (0.1738) acc@1 0.9453 (0.9390) acc@5 1.0000 (0.9992)
[2022-06-11 10:10:30] __main__ INFO: Elapsed 15.68
[2022-06-11 10:10:30] __main__ INFO: Val 61
[2022-06-11 10:10:31] __main__ INFO: Epoch 61 loss 0.4547 acc@1 0.8651 acc@5 0.9948
[2022-06-11 10:10:31] __main__ INFO: Elapsed 1.01
[2022-06-11 10:10:31] __main__ INFO: Train 62 23790
[2022-06-11 10:10:35] __main__ INFO: Epoch 62 Step 100/390 lr 0.100000 loss 0.2101 (0.1668) acc@1 0.9375 (0.9403) acc@5 1.0000 (0.9991)
[2022-06-11 10:10:39] __main__ INFO: Epoch 62 Step 200/390 lr 0.100000 loss 0.1727 (0.1672) acc@1 0.9375 (0.9405) acc@5 1.0000 (0.9992)
[2022-06-11 10:10:43] __main__ INFO: Epoch 62 Step 300/390 lr 0.100000 loss 0.2856 (0.1712) acc@1 0.9141 (0.9394) acc@5 1.0000 (0.9991)
[2022-06-11 10:10:47] __main__ INFO: Epoch 62 Step 390/390 lr 0.100000 loss 0.1850 (0.1751) acc@1 0.9609 (0.9378) acc@5 0.9922 (0.9991)
[2022-06-11 10:10:47] __main__ INFO: Elapsed 15.79
[2022-06-11 10:10:47] __main__ INFO: Val 62
[2022-06-11 10:10:48] __main__ INFO: Epoch 62 loss 0.4471 acc@1 0.8674 acc@5 0.9929
[2022-06-11 10:10:48] __main__ INFO: Elapsed 0.97
[2022-06-11 10:10:48] __main__ INFO: Train 63 24180
[2022-06-11 10:10:52] __main__ INFO: Epoch 63 Step 100/390 lr 0.100000 loss 0.0745 (0.1544) acc@1 0.9766 (0.9455) acc@5 1.0000 (0.9994)
[2022-06-11 10:10:56] __main__ INFO: Epoch 63 Step 200/390 lr 0.100000 loss 0.1782 (0.1555) acc@1 0.9531 (0.9446) acc@5 1.0000 (0.9996)
[2022-06-11 10:11:00] __main__ INFO: Epoch 63 Step 300/390 lr 0.100000 loss 0.1254 (0.1653) acc@1 0.9531 (0.9412) acc@5 1.0000 (0.9995)
[2022-06-11 10:11:04] __main__ INFO: Epoch 63 Step 390/390 lr 0.100000 loss 0.2158 (0.1700) acc@1 0.9375 (0.9392) acc@5 1.0000 (0.9994)
[2022-06-11 10:11:04] __main__ INFO: Elapsed 15.72
[2022-06-11 10:11:04] __main__ INFO: Val 63
[2022-06-11 10:11:05] __main__ INFO: Epoch 63 loss 0.4124 acc@1 0.8758 acc@5 0.9947
[2022-06-11 10:11:05] __main__ INFO: Elapsed 0.90
[2022-06-11 10:11:05] __main__ INFO: Train 64 24570
[2022-06-11 10:11:09] __main__ INFO: Epoch 64 Step 100/390 lr 0.100000 loss 0.1533 (0.1548) acc@1 0.9531 (0.9468) acc@5 1.0000 (0.9995)
[2022-06-11 10:11:13] __main__ INFO: Epoch 64 Step 200/390 lr 0.100000 loss 0.1931 (0.1623) acc@1 0.9297 (0.9439) acc@5 1.0000 (0.9993)
[2022-06-11 10:11:17] __main__ INFO: Epoch 64 Step 300/390 lr 0.100000 loss 0.1510 (0.1670) acc@1 0.9219 (0.9424) acc@5 1.0000 (0.9994)
[2022-06-11 10:11:20] __main__ INFO: Epoch 64 Step 390/390 lr 0.100000 loss 0.1181 (0.1692) acc@1 0.9688 (0.9418) acc@5 1.0000 (0.9995)
[2022-06-11 10:11:20] __main__ INFO: Elapsed 15.71
[2022-06-11 10:11:20] __main__ INFO: Val 64
[2022-06-11 10:11:21] __main__ INFO: Epoch 64 loss 0.4547 acc@1 0.8683 acc@5 0.9954
[2022-06-11 10:11:21] __main__ INFO: Elapsed 0.95
[2022-06-11 10:11:21] __main__ INFO: Train 65 24960
[2022-06-11 10:11:25] __main__ INFO: Epoch 65 Step 100/390 lr 0.100000 loss 0.1940 (0.1515) acc@1 0.9062 (0.9464) acc@5 1.0000 (0.9992)
[2022-06-11 10:11:29] __main__ INFO: Epoch 65 Step 200/390 lr 0.100000 loss 0.2388 (0.1572) acc@1 0.9141 (0.9452) acc@5 1.0000 (0.9996)
[2022-06-11 10:11:33] __main__ INFO: Epoch 65 Step 300/390 lr 0.100000 loss 0.2012 (0.1633) acc@1 0.9297 (0.9430) acc@5 1.0000 (0.9994)
[2022-06-11 10:11:37] __main__ INFO: Epoch 65 Step 390/390 lr 0.100000 loss 0.1326 (0.1696) acc@1 0.9688 (0.9408) acc@5 1.0000 (0.9994)
[2022-06-11 10:11:37] __main__ INFO: Elapsed 15.79
[2022-06-11 10:11:37] __main__ INFO: Val 65
[2022-06-11 10:11:38] __main__ INFO: Epoch 65 loss 0.4387 acc@1 0.8740 acc@5 0.9938
[2022-06-11 10:11:38] __main__ INFO: Elapsed 0.96
[2022-06-11 10:11:38] __main__ INFO: Train 66 25350
[2022-06-11 10:11:42] __main__ INFO: Epoch 66 Step 100/390 lr 0.100000 loss 0.1973 (0.1620) acc@1 0.9453 (0.9423) acc@5 1.0000 (0.9995)
[2022-06-11 10:11:46] __main__ INFO: Epoch 66 Step 200/390 lr 0.100000 loss 0.1866 (0.1668) acc@1 0.9531 (0.9405) acc@5 1.0000 (0.9993)
[2022-06-11 10:11:50] __main__ INFO: Epoch 66 Step 300/390 lr 0.100000 loss 0.1529 (0.1724) acc@1 0.9375 (0.9388) acc@5 1.0000 (0.9993)
[2022-06-11 10:11:54] __main__ INFO: Epoch 66 Step 390/390 lr 0.100000 loss 0.2226 (0.1738) acc@1 0.9297 (0.9384) acc@5 1.0000 (0.9993)
[2022-06-11 10:11:54] __main__ INFO: Elapsed 15.73
[2022-06-11 10:11:54] __main__ INFO: Val 66
[2022-06-11 10:11:55] __main__ INFO: Epoch 66 loss 0.4242 acc@1 0.8764 acc@5 0.9960
[2022-06-11 10:11:55] __main__ INFO: Elapsed 0.91
[2022-06-11 10:11:55] __main__ INFO: Train 67 25740
[2022-06-11 10:11:59] __main__ INFO: Epoch 67 Step 100/390 lr 0.100000 loss 0.1892 (0.1595) acc@1 0.9375 (0.9452) acc@5 1.0000 (0.9997)
[2022-06-11 10:12:03] __main__ INFO: Epoch 67 Step 200/390 lr 0.100000 loss 0.1947 (0.1700) acc@1 0.8984 (0.9416) acc@5 1.0000 (0.9994)
[2022-06-11 10:12:07] __main__ INFO: Epoch 67 Step 300/390 lr 0.100000 loss 0.0885 (0.1698) acc@1 0.9844 (0.9410) acc@5 1.0000 (0.9993)
[2022-06-11 10:12:10] __main__ INFO: Epoch 67 Step 390/390 lr 0.100000 loss 0.1674 (0.1705) acc@1 0.9297 (0.9407) acc@5 1.0000 (0.9993)
[2022-06-11 10:12:10] __main__ INFO: Elapsed 15.77
[2022-06-11 10:12:10] __main__ INFO: Val 67
[2022-06-11 10:12:11] __main__ INFO: Epoch 67 loss 0.4086 acc@1 0.8769 acc@5 0.9950
[2022-06-11 10:12:11] __main__ INFO: Elapsed 0.91
[2022-06-11 10:12:11] __main__ INFO: Train 68 26130
[2022-06-11 10:12:16] __main__ INFO: Epoch 68 Step 100/390 lr 0.100000 loss 0.1197 (0.1533) acc@1 0.9688 (0.9489) acc@5 1.0000 (0.9994)
[2022-06-11 10:12:19] __main__ INFO: Epoch 68 Step 200/390 lr 0.100000 loss 0.2528 (0.1621) acc@1 0.9219 (0.9450) acc@5 1.0000 (0.9995)
[2022-06-11 10:12:23] __main__ INFO: Epoch 68 Step 300/390 lr 0.100000 loss 0.1191 (0.1685) acc@1 0.9688 (0.9421) acc@5 1.0000 (0.9995)
[2022-06-11 10:12:27] __main__ INFO: Epoch 68 Step 390/390 lr 0.100000 loss 0.2163 (0.1743) acc@1 0.9375 (0.9398) acc@5 1.0000 (0.9994)
[2022-06-11 10:12:27] __main__ INFO: Elapsed 15.64
[2022-06-11 10:12:27] __main__ INFO: Val 68
[2022-06-11 10:12:28] __main__ INFO: Epoch 68 loss 0.4890 acc@1 0.8668 acc@5 0.9950
[2022-06-11 10:12:28] __main__ INFO: Elapsed 1.00
[2022-06-11 10:12:28] __main__ INFO: Train 69 26520
[2022-06-11 10:12:32] __main__ INFO: Epoch 69 Step 100/390 lr 0.100000 loss 0.1154 (0.1541) acc@1 0.9844 (0.9462) acc@5 1.0000 (0.9994)
[2022-06-11 10:12:36] __main__ INFO: Epoch 69 Step 200/390 lr 0.100000 loss 0.1364 (0.1577) acc@1 0.9453 (0.9451) acc@5 1.0000 (0.9995)
[2022-06-11 10:12:40] __main__ INFO: Epoch 69 Step 300/390 lr 0.100000 loss 0.2041 (0.1620) acc@1 0.9375 (0.9426) acc@5 1.0000 (0.9994)
[2022-06-11 10:12:44] __main__ INFO: Epoch 69 Step 390/390 lr 0.100000 loss 0.2820 (0.1644) acc@1 0.8984 (0.9417) acc@5 1.0000 (0.9994)
[2022-06-11 10:12:44] __main__ INFO: Elapsed 15.81
[2022-06-11 10:12:44] __main__ INFO: Val 69
[2022-06-11 10:12:45] __main__ INFO: Epoch 69 loss 0.4136 acc@1 0.8826 acc@5 0.9956
[2022-06-11 10:12:45] __main__ INFO: Elapsed 0.94
[2022-06-11 10:12:45] __main__ INFO: Train 70 26910
[2022-06-11 10:12:49] __main__ INFO: Epoch 70 Step 100/390 lr 0.100000 loss 0.1851 (0.1507) acc@1 0.9531 (0.9485) acc@5 1.0000 (0.9994)
[2022-06-11 10:12:53] __main__ INFO: Epoch 70 Step 200/390 lr 0.100000 loss 0.2008 (0.1591) acc@1 0.9219 (0.9446) acc@5 1.0000 (0.9992)
[2022-06-11 10:12:57] __main__ INFO: Epoch 70 Step 300/390 lr 0.100000 loss 0.1953 (0.1631) acc@1 0.9453 (0.9429) acc@5 1.0000 (0.9993)
[2022-06-11 10:13:00] __main__ INFO: Epoch 70 Step 390/390 lr 0.100000 loss 0.1586 (0.1653) acc@1 0.9297 (0.9423) acc@5 1.0000 (0.9993)
[2022-06-11 10:13:00] __main__ INFO: Elapsed 15.61
[2022-06-11 10:13:00] __main__ INFO: Val 70
[2022-06-11 10:13:01] __main__ INFO: Epoch 70 loss 0.4163 acc@1 0.8764 acc@5 0.9947
[2022-06-11 10:13:01] __main__ INFO: Elapsed 0.98
[2022-06-11 10:13:01] __main__ INFO: Train 71 27300
[2022-06-11 10:13:05] __main__ INFO: Epoch 71 Step 100/390 lr 0.100000 loss 0.2445 (0.1573) acc@1 0.8984 (0.9456) acc@5 1.0000 (0.9989)
[2022-06-11 10:13:09] __main__ INFO: Epoch 71 Step 200/390 lr 0.100000 loss 0.1584 (0.1580) acc@1 0.9219 (0.9444) acc@5 1.0000 (0.9992)
[2022-06-11 10:13:13] __main__ INFO: Epoch 71 Step 300/390 lr 0.100000 loss 0.1556 (0.1608) acc@1 0.9375 (0.9434) acc@5 1.0000 (0.9993)
[2022-06-11 10:13:17] __main__ INFO: Epoch 71 Step 390/390 lr 0.100000 loss 0.1637 (0.1653) acc@1 0.9531 (0.9420) acc@5 1.0000 (0.9993)
[2022-06-11 10:13:17] __main__ INFO: Elapsed 15.74
[2022-06-11 10:13:17] __main__ INFO: Val 71
[2022-06-11 10:13:18] __main__ INFO: Epoch 71 loss 0.4424 acc@1 0.8694 acc@5 0.9961
[2022-06-11 10:13:18] __main__ INFO: Elapsed 0.97
[2022-06-11 10:13:18] __main__ INFO: Train 72 27690
[2022-06-11 10:13:22] __main__ INFO: Epoch 72 Step 100/390 lr 0.100000 loss 0.1730 (0.1564) acc@1 0.9375 (0.9463) acc@5 1.0000 (0.9989)
[2022-06-11 10:13:26] __main__ INFO: Epoch 72 Step 200/390 lr 0.100000 loss 0.1855 (0.1637) acc@1 0.9297 (0.9428) acc@5 1.0000 (0.9992)
[2022-06-11 10:13:30] __main__ INFO: Epoch 72 Step 300/390 lr 0.100000 loss 0.1909 (0.1673) acc@1 0.9219 (0.9412) acc@5 1.0000 (0.9992)
[2022-06-11 10:13:34] __main__ INFO: Epoch 72 Step 390/390 lr 0.100000 loss 0.1195 (0.1737) acc@1 0.9688 (0.9388) acc@5 1.0000 (0.9991)
[2022-06-11 10:13:34] __main__ INFO: Elapsed 16.00
[2022-06-11 10:13:34] __main__ INFO: Val 72
[2022-06-11 10:13:35] __main__ INFO: Epoch 72 loss 0.4116 acc@1 0.8795 acc@5 0.9945
[2022-06-11 10:13:35] __main__ INFO: Elapsed 0.94
[2022-06-11 10:13:35] __main__ INFO: Train 73 28080
[2022-06-11 10:13:39] __main__ INFO: Epoch 73 Step 100/390 lr 0.100000 loss 0.1155 (0.1535) acc@1 0.9609 (0.9478) acc@5 1.0000 (0.9994)
[2022-06-11 10:13:43] __main__ INFO: Epoch 73 Step 200/390 lr 0.100000 loss 0.2923 (0.1658) acc@1 0.8828 (0.9431) acc@5 1.0000 (0.9994)
[2022-06-11 10:13:47] __main__ INFO: Epoch 73 Step 300/390 lr 0.100000 loss 0.0928 (0.1658) acc@1 0.9688 (0.9423) acc@5 1.0000 (0.9994)
[2022-06-11 10:13:51] __main__ INFO: Epoch 73 Step 390/390 lr 0.100000 loss 0.1483 (0.1660) acc@1 0.9453 (0.9417) acc@5 1.0000 (0.9993)
[2022-06-11 10:13:51] __main__ INFO: Elapsed 15.68
[2022-06-11 10:13:51] __main__ INFO: Val 73
[2022-06-11 10:13:52] __main__ INFO: Epoch 73 loss 0.3855 acc@1 0.8893 acc@5 0.9964
[2022-06-11 10:13:52] __main__ INFO: Elapsed 0.95
[2022-06-11 10:13:52] __main__ INFO: Train 74 28470
[2022-06-11 10:13:56] __main__ INFO: Epoch 74 Step 100/390 lr 0.100000 loss 0.1283 (0.1567) acc@1 0.9688 (0.9450) acc@5 1.0000 (0.9995)
[2022-06-11 10:14:00] __main__ INFO: Epoch 74 Step 200/390 lr 0.100000 loss 0.2075 (0.1606) acc@1 0.9062 (0.9433) acc@5 1.0000 (0.9995)
[2022-06-11 10:14:04] __main__ INFO: Epoch 74 Step 300/390 lr 0.100000 loss 0.1245 (0.1633) acc@1 0.9453 (0.9422) acc@5 1.0000 (0.9994)
[2022-06-11 10:14:07] __main__ INFO: Epoch 74 Step 390/390 lr 0.100000 loss 0.1689 (0.1659) acc@1 0.9375 (0.9411) acc@5 1.0000 (0.9994)
[2022-06-11 10:14:08] __main__ INFO: Elapsed 15.88
[2022-06-11 10:14:08] __main__ INFO: Val 74
[2022-06-11 10:14:08] __main__ INFO: Epoch 74 loss 0.4028 acc@1 0.8822 acc@5 0.9947
[2022-06-11 10:14:08] __main__ INFO: Elapsed 0.87
[2022-06-11 10:14:08] __main__ INFO: Train 75 28860
[2022-06-11 10:14:12] __main__ INFO: Epoch 75 Step 100/390 lr 0.100000 loss 0.2284 (0.1518) acc@1 0.9375 (0.9463) acc@5 1.0000 (0.9996)
[2022-06-11 10:14:16] __main__ INFO: Epoch 75 Step 200/390 lr 0.100000 loss 0.1127 (0.1638) acc@1 0.9609 (0.9417) acc@5 1.0000 (0.9995)
[2022-06-11 10:14:20] __main__ INFO: Epoch 75 Step 300/390 lr 0.100000 loss 0.2286 (0.1649) acc@1 0.9297 (0.9409) acc@5 1.0000 (0.9994)
[2022-06-11 10:14:24] __main__ INFO: Epoch 75 Step 390/390 lr 0.100000 loss 0.1444 (0.1672) acc@1 0.9453 (0.9400) acc@5 1.0000 (0.9993)
[2022-06-11 10:14:24] __main__ INFO: Elapsed 15.41
[2022-06-11 10:14:24] __main__ INFO: Val 75
[2022-06-11 10:14:25] __main__ INFO: Epoch 75 loss 0.5518 acc@1 0.8532 acc@5 0.9930
[2022-06-11 10:14:25] __main__ INFO: Elapsed 0.94
[2022-06-11 10:14:25] __main__ INFO: Train 76 29250
[2022-06-11 10:14:29] __main__ INFO: Epoch 76 Step 100/390 lr 0.100000 loss 0.1722 (0.1435) acc@1 0.8984 (0.9504) acc@5 1.0000 (0.9992)
[2022-06-11 10:14:33] __main__ INFO: Epoch 76 Step 200/390 lr 0.100000 loss 0.0423 (0.1577) acc@1 0.9922 (0.9446) acc@5 1.0000 (0.9990)
[2022-06-11 10:14:37] __main__ INFO: Epoch 76 Step 300/390 lr 0.100000 loss 0.1654 (0.1618) acc@1 0.9375 (0.9435) acc@5 1.0000 (0.9990)
[2022-06-11 10:14:40] __main__ INFO: Epoch 76 Step 390/390 lr 0.100000 loss 0.2284 (0.1651) acc@1 0.9297 (0.9420) acc@5 1.0000 (0.9992)
[2022-06-11 10:14:40] __main__ INFO: Elapsed 15.62
[2022-06-11 10:14:40] __main__ INFO: Val 76
[2022-06-11 10:14:41] __main__ INFO: Epoch 76 loss 0.5314 acc@1 0.8520 acc@5 0.9902
[2022-06-11 10:14:41] __main__ INFO: Elapsed 0.94
[2022-06-11 10:14:41] __main__ INFO: Train 77 29640
[2022-06-11 10:14:46] __main__ INFO: Epoch 77 Step 100/390 lr 0.100000 loss 0.1711 (0.1479) acc@1 0.9453 (0.9481) acc@5 1.0000 (0.9997)
[2022-06-11 10:14:49] __main__ INFO: Epoch 77 Step 200/390 lr 0.100000 loss 0.1918 (0.1561) acc@1 0.9453 (0.9457) acc@5 1.0000 (0.9996)
[2022-06-11 10:14:53] __main__ INFO: Epoch 77 Step 300/390 lr 0.100000 loss 0.1137 (0.1575) acc@1 0.9609 (0.9452) acc@5 1.0000 (0.9995)
[2022-06-11 10:14:57] __main__ INFO: Epoch 77 Step 390/390 lr 0.100000 loss 0.2483 (0.1644) acc@1 0.9141 (0.9423) acc@5 1.0000 (0.9993)
[2022-06-11 10:14:57] __main__ INFO: Elapsed 15.67
[2022-06-11 10:14:57] __main__ INFO: Val 77
[2022-06-11 10:14:58] __main__ INFO: Epoch 77 loss 0.4007 acc@1 0.8793 acc@5 0.9942
[2022-06-11 10:14:58] __main__ INFO: Elapsed 0.96
[2022-06-11 10:14:58] __main__ INFO: Train 78 30030
[2022-06-11 10:15:02] __main__ INFO: Epoch 78 Step 100/390 lr 0.100000 loss 0.1286 (0.1560) acc@1 0.9688 (0.9437) acc@5 1.0000 (0.9996)
[2022-06-11 10:15:06] __main__ INFO: Epoch 78 Step 200/390 lr 0.100000 loss 0.1946 (0.1659) acc@1 0.9453 (0.9419) acc@5 1.0000 (0.9993)
[2022-06-11 10:15:10] __main__ INFO: Epoch 78 Step 300/390 lr 0.100000 loss 0.1194 (0.1641) acc@1 0.9453 (0.9424) acc@5 1.0000 (0.9993)
[2022-06-11 10:15:13] __main__ INFO: Epoch 78 Step 390/390 lr 0.100000 loss 0.1781 (0.1691) acc@1 0.9297 (0.9410) acc@5 0.9922 (0.9993)
[2022-06-11 10:15:13] __main__ INFO: Elapsed 15.51
[2022-06-11 10:15:13] __main__ INFO: Val 78
[2022-06-11 10:15:14] __main__ INFO: Epoch 78 loss 0.4610 acc@1 0.8641 acc@5 0.9939
[2022-06-11 10:15:14] __main__ INFO: Elapsed 0.96
[2022-06-11 10:15:14] __main__ INFO: Train 79 30420
[2022-06-11 10:15:19] __main__ INFO: Epoch 79 Step 100/390 lr 0.100000 loss 0.1092 (0.1518) acc@1 0.9453 (0.9446) acc@5 1.0000 (0.9996)
[2022-06-11 10:15:23] __main__ INFO: Epoch 79 Step 200/390 lr 0.100000 loss 0.2298 (0.1540) acc@1 0.9531 (0.9442) acc@5 1.0000 (0.9995)
[2022-06-11 10:15:27] __main__ INFO: Epoch 79 Step 300/390 lr 0.100000 loss 0.1178 (0.1587) acc@1 0.9844 (0.9434) acc@5 1.0000 (0.9995)
[2022-06-11 10:15:30] __main__ INFO: Epoch 79 Step 390/390 lr 0.100000 loss 0.1030 (0.1629) acc@1 0.9844 (0.9419) acc@5 1.0000 (0.9995)
[2022-06-11 10:15:30] __main__ INFO: Elapsed 15.64
[2022-06-11 10:15:30] __main__ INFO: Val 79
[2022-06-11 10:15:31] __main__ INFO: Epoch 79 loss 0.4525 acc@1 0.8770 acc@5 0.9948
[2022-06-11 10:15:31] __main__ INFO: Elapsed 0.87
[2022-06-11 10:15:31] __main__ INFO: Train 80 30810
[2022-06-11 10:15:35] __main__ INFO: Epoch 80 Step 100/390 lr 0.100000 loss 0.0857 (0.1653) acc@1 0.9688 (0.9413) acc@5 1.0000 (0.9994)
[2022-06-11 10:15:39] __main__ INFO: Epoch 80 Step 200/390 lr 0.100000 loss 0.1711 (0.1677) acc@1 0.9531 (0.9416) acc@5 1.0000 (0.9992)
[2022-06-11 10:15:43] __main__ INFO: Epoch 80 Step 300/390 lr 0.100000 loss 0.1633 (0.1639) acc@1 0.9219 (0.9422) acc@5 1.0000 (0.9994)
[2022-06-11 10:15:47] __main__ INFO: Epoch 80 Step 390/390 lr 0.100000 loss 0.2726 (0.1679) acc@1 0.9141 (0.9412) acc@5 1.0000 (0.9994)
[2022-06-11 10:15:47] __main__ INFO: Elapsed 15.80
[2022-06-11 10:15:47] __main__ INFO: Val 80
[2022-06-11 10:15:48] __main__ INFO: Epoch 80 loss 0.4470 acc@1 0.8739 acc@5 0.9944
[2022-06-11 10:15:48] __main__ INFO: Elapsed 0.92
[2022-06-11 10:15:48] __main__ INFO: Train 81 31200
[2022-06-11 10:15:52] __main__ INFO: Epoch 81 Step 100/390 lr 0.010000 loss 0.1312 (0.1110) acc@1 0.9453 (0.9637) acc@5 1.0000 (0.9997)
[2022-06-11 10:15:56] __main__ INFO: Epoch 81 Step 200/390 lr 0.010000 loss 0.1008 (0.1033) acc@1 0.9688 (0.9656) acc@5 1.0000 (0.9996)
[2022-06-11 10:16:00] __main__ INFO: Epoch 81 Step 300/390 lr 0.010000 loss 0.0895 (0.0951) acc@1 0.9766 (0.9686) acc@5 1.0000 (0.9997)
[2022-06-11 10:16:03] __main__ INFO: Epoch 81 Step 390/390 lr 0.010000 loss 0.1290 (0.0900) acc@1 0.9609 (0.9703) acc@5 0.9922 (0.9997)
[2022-06-11 10:16:03] __main__ INFO: Elapsed 15.76
[2022-06-11 10:16:03] __main__ INFO: Val 81
[2022-06-11 10:16:04] __main__ INFO: Epoch 81 loss 0.2692 acc@1 0.9189 acc@5 0.9980
[2022-06-11 10:16:04] __main__ INFO: Elapsed 0.94
[2022-06-11 10:16:04] __main__ INFO: Train 82 31590
[2022-06-11 10:16:08] __main__ INFO: Epoch 82 Step 100/390 lr 0.010000 loss 0.0511 (0.0664) acc@1 0.9688 (0.9798) acc@5 1.0000 (0.9999)
[2022-06-11 10:16:12] __main__ INFO: Epoch 82 Step 200/390 lr 0.010000 loss 0.0407 (0.0640) acc@1 0.9922 (0.9802) acc@5 1.0000 (0.9999)
[2022-06-11 10:16:16] __main__ INFO: Epoch 82 Step 300/390 lr 0.010000 loss 0.0616 (0.0620) acc@1 0.9844 (0.9805) acc@5 1.0000 (0.9999)
[2022-06-11 10:16:20] __main__ INFO: Epoch 82 Step 390/390 lr 0.010000 loss 0.0667 (0.0604) acc@1 0.9844 (0.9809) acc@5 1.0000 (0.9999)
[2022-06-11 10:16:20] __main__ INFO: Elapsed 15.69
[2022-06-11 10:16:20] __main__ INFO: Val 82
[2022-06-11 10:16:21] __main__ INFO: Epoch 82 loss 0.2662 acc@1 0.9216 acc@5 0.9978
[2022-06-11 10:16:21] __main__ INFO: Elapsed 0.94
[2022-06-11 10:16:21] __main__ INFO: Train 83 31980
[2022-06-11 10:16:25] __main__ INFO: Epoch 83 Step 100/390 lr 0.010000 loss 0.0720 (0.0500) acc@1 0.9766 (0.9849) acc@5 1.0000 (0.9998)
[2022-06-11 10:16:29] __main__ INFO: Epoch 83 Step 200/390 lr 0.010000 loss 0.0488 (0.0496) acc@1 0.9844 (0.9852) acc@5 1.0000 (0.9998)
[2022-06-11 10:16:33] __main__ INFO: Epoch 83 Step 300/390 lr 0.010000 loss 0.0369 (0.0505) acc@1 0.9922 (0.9848) acc@5 1.0000 (0.9999)
[2022-06-11 10:16:37] __main__ INFO: Epoch 83 Step 390/390 lr 0.010000 loss 0.0652 (0.0510) acc@1 0.9609 (0.9843) acc@5 1.0000 (0.9999)
[2022-06-11 10:16:37] __main__ INFO: Elapsed 15.67
[2022-06-11 10:16:37] __main__ INFO: Val 83
[2022-06-11 10:16:38] __main__ INFO: Epoch 83 loss 0.2687 acc@1 0.9226 acc@5 0.9982
[2022-06-11 10:16:38] __main__ INFO: Elapsed 0.97
[2022-06-11 10:16:38] __main__ INFO: Train 84 32370
[2022-06-11 10:16:42] __main__ INFO: Epoch 84 Step 100/390 lr 0.010000 loss 0.0351 (0.0445) acc@1 0.9922 (0.9857) acc@5 1.0000 (1.0000)
[2022-06-11 10:16:46] __main__ INFO: Epoch 84 Step 200/390 lr 0.010000 loss 0.0446 (0.0439) acc@1 0.9844 (0.9863) acc@5 1.0000 (1.0000)
[2022-06-11 10:16:50] __main__ INFO: Epoch 84 Step 300/390 lr 0.010000 loss 0.0233 (0.0445) acc@1 1.0000 (0.9862) acc@5 1.0000 (1.0000)
[2022-06-11 10:16:53] __main__ INFO: Epoch 84 Step 390/390 lr 0.010000 loss 0.0310 (0.0439) acc@1 0.9922 (0.9862) acc@5 1.0000 (1.0000)
[2022-06-11 10:16:53] __main__ INFO: Elapsed 15.64
[2022-06-11 10:16:53] __main__ INFO: Val 84
[2022-06-11 10:16:54] __main__ INFO: Epoch 84 loss 0.2741 acc@1 0.9240 acc@5 0.9983
[2022-06-11 10:16:54] __main__ INFO: Elapsed 0.90
[2022-06-11 10:16:54] __main__ INFO: Train 85 32760
[2022-06-11 10:16:58] __main__ INFO: Epoch 85 Step 100/390 lr 0.010000 loss 0.0217 (0.0396) acc@1 1.0000 (0.9883) acc@5 1.0000 (1.0000)
[2022-06-11 10:17:02] __main__ INFO: Epoch 85 Step 200/390 lr 0.010000 loss 0.0505 (0.0387) acc@1 0.9844 (0.9886) acc@5 1.0000 (1.0000)
[2022-06-11 10:17:06] __main__ INFO: Epoch 85 Step 300/390 lr 0.010000 loss 0.0119 (0.0391) acc@1 1.0000 (0.9885) acc@5 1.0000 (1.0000)
[2022-06-11 10:17:10] __main__ INFO: Epoch 85 Step 390/390 lr 0.010000 loss 0.0405 (0.0396) acc@1 0.9922 (0.9883) acc@5 1.0000 (1.0000)
[2022-06-11 10:17:10] __main__ INFO: Elapsed 15.61
[2022-06-11 10:17:10] __main__ INFO: Val 85
[2022-06-11 10:17:11] __main__ INFO: Epoch 85 loss 0.2732 acc@1 0.9250 acc@5 0.9980
[2022-06-11 10:17:11] __main__ INFO: Elapsed 0.97
[2022-06-11 10:17:11] __main__ INFO: Train 86 33150
[2022-06-11 10:17:15] __main__ INFO: Epoch 86 Step 100/390 lr 0.010000 loss 0.0420 (0.0334) acc@1 1.0000 (0.9904) acc@5 1.0000 (0.9999)
[2022-06-11 10:17:19] __main__ INFO: Epoch 86 Step 200/390 lr 0.010000 loss 0.0225 (0.0351) acc@1 0.9922 (0.9898) acc@5 1.0000 (1.0000)
[2022-06-11 10:17:23] __main__ INFO: Epoch 86 Step 300/390 lr 0.010000 loss 0.0333 (0.0352) acc@1 0.9922 (0.9897) acc@5 1.0000 (1.0000)
[2022-06-11 10:17:26] __main__ INFO: Epoch 86 Step 390/390 lr 0.010000 loss 0.0376 (0.0345) acc@1 0.9922 (0.9897) acc@5 1.0000 (1.0000)
[2022-06-11 10:17:27] __main__ INFO: Elapsed 15.78
[2022-06-11 10:17:27] __main__ INFO: Val 86
[2022-06-11 10:17:28] __main__ INFO: Epoch 86 loss 0.2806 acc@1 0.9228 acc@5 0.9976
[2022-06-11 10:17:28] __main__ INFO: Elapsed 0.94
[2022-06-11 10:17:28] __main__ INFO: Train 87 33540
[2022-06-11 10:17:32] __main__ INFO: Epoch 87 Step 100/390 lr 0.010000 loss 0.0558 (0.0305) acc@1 0.9844 (0.9914) acc@5 1.0000 (0.9999)
[2022-06-11 10:17:36] __main__ INFO: Epoch 87 Step 200/390 lr 0.010000 loss 0.0516 (0.0320) acc@1 0.9844 (0.9907) acc@5 1.0000 (1.0000)
[2022-06-11 10:17:40] __main__ INFO: Epoch 87 Step 300/390 lr 0.010000 loss 0.0618 (0.0331) acc@1 0.9844 (0.9902) acc@5 1.0000 (1.0000)
[2022-06-11 10:17:43] __main__ INFO: Epoch 87 Step 390/390 lr 0.010000 loss 0.0911 (0.0330) acc@1 0.9609 (0.9901) acc@5 1.0000 (1.0000)
[2022-06-11 10:17:43] __main__ INFO: Elapsed 15.72
[2022-06-11 10:17:43] __main__ INFO: Val 87
[2022-06-11 10:17:44] __main__ INFO: Epoch 87 loss 0.2785 acc@1 0.9240 acc@5 0.9981
[2022-06-11 10:17:44] __main__ INFO: Elapsed 0.92
[2022-06-11 10:17:44] __main__ INFO: Train 88 33930
[2022-06-11 10:17:48] __main__ INFO: Epoch 88 Step 100/390 lr 0.010000 loss 0.0578 (0.0300) acc@1 0.9766 (0.9912) acc@5 1.0000 (1.0000)
[2022-06-11 10:17:52] __main__ INFO: Epoch 88 Step 200/390 lr 0.010000 loss 0.0232 (0.0309) acc@1 0.9922 (0.9910) acc@5 1.0000 (1.0000)
[2022-06-11 10:17:56] __main__ INFO: Epoch 88 Step 300/390 lr 0.010000 loss 0.0469 (0.0314) acc@1 0.9844 (0.9906) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:00] __main__ INFO: Epoch 88 Step 390/390 lr 0.010000 loss 0.0201 (0.0306) acc@1 1.0000 (0.9908) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:00] __main__ INFO: Elapsed 15.70
[2022-06-11 10:18:00] __main__ INFO: Val 88
[2022-06-11 10:18:01] __main__ INFO: Epoch 88 loss 0.2954 acc@1 0.9225 acc@5 0.9979
[2022-06-11 10:18:01] __main__ INFO: Elapsed 0.97
[2022-06-11 10:18:01] __main__ INFO: Train 89 34320
[2022-06-11 10:18:05] __main__ INFO: Epoch 89 Step 100/390 lr 0.010000 loss 0.0360 (0.0287) acc@1 0.9922 (0.9915) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:09] __main__ INFO: Epoch 89 Step 200/390 lr 0.010000 loss 0.0474 (0.0284) acc@1 0.9766 (0.9913) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:13] __main__ INFO: Epoch 89 Step 300/390 lr 0.010000 loss 0.0416 (0.0287) acc@1 0.9844 (0.9913) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:16] __main__ INFO: Epoch 89 Step 390/390 lr 0.010000 loss 0.0227 (0.0287) acc@1 0.9922 (0.9912) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:16] __main__ INFO: Elapsed 15.60
[2022-06-11 10:18:16] __main__ INFO: Val 89
[2022-06-11 10:18:17] __main__ INFO: Epoch 89 loss 0.2878 acc@1 0.9241 acc@5 0.9981
[2022-06-11 10:18:17] __main__ INFO: Elapsed 0.97
[2022-06-11 10:18:17] __main__ INFO: Train 90 34710
[2022-06-11 10:18:22] __main__ INFO: Epoch 90 Step 100/390 lr 0.010000 loss 0.0301 (0.0265) acc@1 0.9922 (0.9918) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:26] __main__ INFO: Epoch 90 Step 200/390 lr 0.010000 loss 0.0109 (0.0266) acc@1 1.0000 (0.9914) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:30] __main__ INFO: Epoch 90 Step 300/390 lr 0.010000 loss 0.0110 (0.0269) acc@1 1.0000 (0.9915) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:33] __main__ INFO: Epoch 90 Step 390/390 lr 0.010000 loss 0.0377 (0.0269) acc@1 0.9922 (0.9916) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:33] __main__ INFO: Elapsed 15.83
[2022-06-11 10:18:33] __main__ INFO: Val 90
[2022-06-11 10:18:34] __main__ INFO: Epoch 90 loss 0.2952 acc@1 0.9240 acc@5 0.9980
[2022-06-11 10:18:34] __main__ INFO: Elapsed 0.94
[2022-06-11 10:18:34] __main__ INFO: Train 91 35100
[2022-06-11 10:18:38] __main__ INFO: Epoch 91 Step 100/390 lr 0.010000 loss 0.0487 (0.0245) acc@1 0.9844 (0.9934) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:42] __main__ INFO: Epoch 91 Step 200/390 lr 0.010000 loss 0.0078 (0.0249) acc@1 1.0000 (0.9931) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:46] __main__ INFO: Epoch 91 Step 300/390 lr 0.010000 loss 0.0336 (0.0258) acc@1 0.9844 (0.9928) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:50] __main__ INFO: Epoch 91 Step 390/390 lr 0.010000 loss 0.0142 (0.0262) acc@1 1.0000 (0.9926) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:50] __main__ INFO: Elapsed 15.77
[2022-06-11 10:18:50] __main__ INFO: Val 91
[2022-06-11 10:18:51] __main__ INFO: Epoch 91 loss 0.2955 acc@1 0.9237 acc@5 0.9986
[2022-06-11 10:18:51] __main__ INFO: Elapsed 0.95
[2022-06-11 10:18:51] __main__ INFO: Train 92 35490
[2022-06-11 10:18:55] __main__ INFO: Epoch 92 Step 100/390 lr 0.010000 loss 0.0458 (0.0249) acc@1 0.9844 (0.9927) acc@5 1.0000 (1.0000)
[2022-06-11 10:18:59] __main__ INFO: Epoch 92 Step 200/390 lr 0.010000 loss 0.0081 (0.0241) acc@1 1.0000 (0.9929) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:03] __main__ INFO: Epoch 92 Step 300/390 lr 0.010000 loss 0.0335 (0.0243) acc@1 0.9922 (0.9929) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:07] __main__ INFO: Epoch 92 Step 390/390 lr 0.010000 loss 0.0214 (0.0239) acc@1 1.0000 (0.9930) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:07] __main__ INFO: Elapsed 15.77
[2022-06-11 10:19:07] __main__ INFO: Val 92
[2022-06-11 10:19:08] __main__ INFO: Epoch 92 loss 0.2951 acc@1 0.9247 acc@5 0.9982
[2022-06-11 10:19:08] __main__ INFO: Elapsed 0.99
[2022-06-11 10:19:08] __main__ INFO: Train 93 35880
[2022-06-11 10:19:12] __main__ INFO: Epoch 93 Step 100/390 lr 0.010000 loss 0.0193 (0.0218) acc@1 1.0000 (0.9941) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:16] __main__ INFO: Epoch 93 Step 200/390 lr 0.010000 loss 0.0160 (0.0222) acc@1 1.0000 (0.9937) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:20] __main__ INFO: Epoch 93 Step 300/390 lr 0.010000 loss 0.0109 (0.0227) acc@1 1.0000 (0.9936) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:23] __main__ INFO: Epoch 93 Step 390/390 lr 0.010000 loss 0.0062 (0.0234) acc@1 1.0000 (0.9934) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:24] __main__ INFO: Elapsed 15.82
[2022-06-11 10:19:24] __main__ INFO: Val 93
[2022-06-11 10:19:24] __main__ INFO: Epoch 93 loss 0.2997 acc@1 0.9233 acc@5 0.9983
[2022-06-11 10:19:24] __main__ INFO: Elapsed 0.95
[2022-06-11 10:19:24] __main__ INFO: Train 94 36270
[2022-06-11 10:19:29] __main__ INFO: Epoch 94 Step 100/390 lr 0.010000 loss 0.0110 (0.0196) acc@1 0.9922 (0.9949) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:32] __main__ INFO: Epoch 94 Step 200/390 lr 0.010000 loss 0.0251 (0.0199) acc@1 0.9922 (0.9946) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:36] __main__ INFO: Epoch 94 Step 300/390 lr 0.010000 loss 0.0230 (0.0196) acc@1 0.9922 (0.9948) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:40] __main__ INFO: Epoch 94 Step 390/390 lr 0.010000 loss 0.0177 (0.0202) acc@1 1.0000 (0.9943) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:40] __main__ INFO: Elapsed 15.55
[2022-06-11 10:19:40] __main__ INFO: Val 94
[2022-06-11 10:19:41] __main__ INFO: Epoch 94 loss 0.3075 acc@1 0.9243 acc@5 0.9983
[2022-06-11 10:19:41] __main__ INFO: Elapsed 0.97
[2022-06-11 10:19:41] __main__ INFO: Train 95 36660
[2022-06-11 10:19:45] __main__ INFO: Epoch 95 Step 100/390 lr 0.010000 loss 0.0206 (0.0197) acc@1 0.9922 (0.9945) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:49] __main__ INFO: Epoch 95 Step 200/390 lr 0.010000 loss 0.0169 (0.0195) acc@1 1.0000 (0.9947) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:53] __main__ INFO: Epoch 95 Step 300/390 lr 0.010000 loss 0.0225 (0.0188) acc@1 0.9922 (0.9951) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:57] __main__ INFO: Epoch 95 Step 390/390 lr 0.010000 loss 0.0112 (0.0191) acc@1 1.0000 (0.9947) acc@5 1.0000 (1.0000)
[2022-06-11 10:19:57] __main__ INFO: Elapsed 16.08
[2022-06-11 10:19:57] __main__ INFO: Val 95
[2022-06-11 10:19:58] __main__ INFO: Epoch 95 loss 0.3033 acc@1 0.9251 acc@5 0.9986
[2022-06-11 10:19:58] __main__ INFO: Elapsed 0.90
[2022-06-11 10:19:58] __main__ INFO: Train 96 37050
[2022-06-11 10:20:02] __main__ INFO: Epoch 96 Step 100/390 lr 0.010000 loss 0.0192 (0.0196) acc@1 0.9922 (0.9951) acc@5 1.0000 (1.0000)
[2022-06-11 10:20:06] __main__ INFO: Epoch 96 Step 200/390 lr 0.010000 loss 0.0066 (0.0188) acc@1 1.0000 (0.9955) acc@5 1.0000 (1.0000)
[2022-06-11 10:20:10] __main__ INFO: Epoch 96 Step 300/390 lr 0.010000 loss 0.0357 (0.0186) acc@1 0.9844 (0.9954) acc@5 1.0000 (1.0000)
[2022-06-11 10:20:14] __main__ INFO: Epoch 96 Step 390/390 lr 0.010000 loss 0.0109 (0.0186) acc@1 1.0000 (0.9953) acc@5 1.0000 (1.0000)
[2022-06-11 10:20:14] __main__ INFO: Elapsed 15.85
[2022-06-11 10:20:14] __main__ INFO: Val 96
[2022-06-11 10:20:15] __main__ INFO: Epoch 96 loss 0.3079 acc@1 0.9249 acc@5 0.9982
[2022-06-11 10:20:15] __main__ INFO: Elapsed 0.96
[2022-06-11 10:20:15] __main__ INFO: Train 97 37440
[2022-06-11 10:20:19] __main__ INFO: Epoch 97 Step 100/390 lr 0.010000 loss 0.0209 (0.0174) acc@1 1.0000 (0.9962) acc@5 1.0000 (1.0000)
[2022-06-11 10:20:23] __main__ INFO: Epoch 97 Step 200/390 lr 0.010000 loss 0.0208 (0.0171) acc@1 0.9922 (0.9958) acc@5 1.0000 (1.0000)
[2022-06-11 10:20:27] __main__ INFO: Epoch 97 Step 300/390 lr 0.010000 loss 0.0121 (0.0173) acc@1 1.0000 (0.9956) acc@5 1.0000 (1.0000)
[2022-06-11 10:20:30] __main__ INFO: Epoch 97 Step 390/390 lr 0.010000 loss 0.0156 (0.0177) acc@1 1.0000 (0.9953) acc@5 1.0000 (1.0000)
[2022-06-11 10:20:31] __main__ INFO: Elapsed 15.76
[2022-06-11 10:20:31] __main__ INFO: Val 97
[2022-06-11 10:20:32] __main__ INFO: Epoch 97 loss 0.3126 acc@1 0.9248 acc@5 0.9985
[2022-06-11 10:20:32] __main__ INFO: Elapsed 0.98
[2022-06-11 10:20:32] __main__ INFO: Train 98 37830
[2022-06-11 10:20:36] __main__ INFO: Epoch 98 Step 100/390 lr 0.010000 loss 0.0034 (0.0186) acc@1 1.0000 (0.9955) acc@5 1.0000 (1.0000)
[2022-06-11 10:20:40] __main__ INFO: Epoch 98 Step 200/390 lr 0.010000 loss 0.0068 (0.0171) acc@1 1.0000 (0.9960) acc@5 1.0000 (1.0000)
[2022-06-11 10:20:43] __main__ INFO: Epoch 98 Step 300/390 lr 0.010000 loss 0.0238 (0.0168) acc@1 0.9922 (0.9957) acc@5 1.0000 (1.0000)
[2022-06-11 10:20:47] __main__ INFO: Epoch 98 Step 390/390 lr 0.010000 loss 0.0182 (0.0168) acc@1 0.9922 (0.9958) acc@5 1.0000 (1.0000)
[2022-06-11 10:20:47] __main__ INFO: Elapsed 15.51
[2022-06-11 10:20:47] __main__ INFO: Val 98
[2022-06-11 10:20:48] __main__ INFO: Epoch 98 loss 0.3147 acc@1 0.9257 acc@5 0.9984
[2022-06-11 10:20:48] __main__ INFO: Elapsed 0.98
[2022-06-11 10:20:48] __main__ INFO: Train 99 38220
[2022-06-11 10:20:52] __main__ INFO: Epoch 99 Step 100/390 lr 0.010000 loss 0.0130 (0.0157) acc@1 1.0000 (0.9959) acc@5 1.0000 (1.0000)
[2022-06-11 10:20:56] __main__ INFO: Epoch 99 Step 200/390 lr 0.010000 loss 0.0091 (0.0160) acc@1 1.0000 (0.9959) acc@5 1.0000 (1.0000)
[2022-06-11 10:21:00] __main__ INFO: Epoch 99 Step 300/390 lr 0.010000 loss 0.0119 (0.0158) acc@1 1.0000 (0.9959) acc@5 1.0000 (1.0000)
[2022-06-11 10:21:04] __main__ INFO: Epoch 99 Step 390/390 lr 0.010000 loss 0.0271 (0.0159) acc@1 0.9922 (0.9959) acc@5 1.0000 (1.0000)
[2022-06-11 10:21:04] __main__ INFO: Elapsed 15.82
[2022-06-11 10:21:04] __main__ INFO: Val 99
[2022-06-11 10:21:05] __main__ INFO: Epoch 99 loss 0.3089 acc@1 0.9257 acc@5 0.9984
[2022-06-11 10:21:05] __main__ INFO: Elapsed 0.93
[2022-06-11 10:21:05] __main__ INFO: Train 100 38610
[2022-06-11 10:21:09] __main__ INFO: Epoch 100 Step 100/390 lr 0.010000 loss 0.0340 (0.0130) acc@1 0.9844 (0.9969) acc@5 1.0000 (1.0000)
[2022-06-11 10:21:13] __main__ INFO: Epoch 100 Step 200/390 lr 0.010000 loss 0.0062 (0.0142) acc@1 1.0000 (0.9965) acc@5 1.0000 (1.0000)
[2022-06-11 10:21:17] __main__ INFO: Epoch 100 Step 300/390 lr 0.010000 loss 0.0227 (0.0144) acc@1 1.0000 (0.9963) acc@5 1.0000 (1.0000)
[2022-06-11 10:21:20] __main__ INFO: Epoch 100 Step 390/390 lr 0.010000 loss 0.0256 (0.0148) acc@1 0.9844 (0.9961) acc@5 1.0000 (1.0000)
[2022-06-11 10:21:21] __main__ INFO: Elapsed 15.78
[2022-06-11 10:21:21] __main__ INFO: Val 100
[2022-06-11 10:21:22] __main__ INFO: Epoch 100 loss 0.3121 acc@1 0.9258 acc@5 0.9982
[2022-06-11 10:21:22] __main__ INFO: Elapsed 0.96
[2022-06-11 10:21:22] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00100.pth
[2022-06-11 10:21:22] __main__ INFO: Train 101 39000
[2022-06-11 10:21:26] __main__ INFO: Epoch 101 Step 100/390 lr 0.010000 loss 0.0396 (0.0149) acc@1 0.9922 (0.9961) acc@5 1.0000 (0.9999)
[2022-06-11 10:21:30] __main__ INFO: Epoch 101 Step 200/390 lr 0.010000 loss 0.0291 (0.0150) acc@1 0.9922 (0.9959) acc@5 1.0000 (1.0000)
[2022-06-11 10:21:34] __main__ INFO: Epoch 101 Step 300/390 lr 0.010000 loss 0.0304 (0.0155) acc@1 0.9922 (0.9958) acc@5 1.0000 (1.0000)
[2022-06-11 10:21:37] __main__ INFO: Epoch 101 Step 390/390 lr 0.010000 loss 0.0062 (0.0157) acc@1 1.0000 (0.9958) acc@5 1.0000 (1.0000)
[2022-06-11 10:21:37] __main__ INFO: Elapsed 15.74
[2022-06-11 10:21:37] __main__ INFO: Val 101
[2022-06-11 10:21:38] __main__ INFO: Epoch 101 loss 0.3245 acc@1 0.9236 acc@5 0.9982
[2022-06-11 10:21:38] __main__ INFO: Elapsed 0.97
[2022-06-11 10:21:38] __main__ INFO: Train 102 39390
[2022-06-11 10:21:43] __main__ INFO: Epoch 102 Step 100/390 lr 0.010000 loss 0.0160 (0.0143) acc@1 0.9922 (0.9967) acc@5 1.0000 (1.0000)
[2022-06-11 10:21:47] __main__ INFO: Epoch 102 Step 200/390 lr 0.010000 loss 0.0140 (0.0152) acc@1 1.0000 (0.9961) acc@5 1.0000 (1.0000)
[2022-06-11 10:21:51] __main__ INFO: Epoch 102 Step 300/390 lr 0.010000 loss 0.0241 (0.0152) acc@1 0.9922 (0.9961) acc@5 1.0000 (1.0000)
[2022-06-11 10:21:54] __main__ INFO: Epoch 102 Step 390/390 lr 0.010000 loss 0.0056 (0.0152) acc@1 1.0000 (0.9961) acc@5 1.0000 (1.0000)
[2022-06-11 10:21:54] __main__ INFO: Elapsed 15.79
[2022-06-11 10:21:54] __main__ INFO: Val 102
[2022-06-11 10:21:55] __main__ INFO: Epoch 102 loss 0.3157 acc@1 0.9243 acc@5 0.9984
[2022-06-11 10:21:55] __main__ INFO: Elapsed 0.96
[2022-06-11 10:21:55] __main__ INFO: Train 103 39780
[2022-06-11 10:21:59] __main__ INFO: Epoch 103 Step 100/390 lr 0.010000 loss 0.0052 (0.0144) acc@1 1.0000 (0.9959) acc@5 1.0000 (1.0000)
[2022-06-11 10:22:03] __main__ INFO: Epoch 103 Step 200/390 lr 0.010000 loss 0.0146 (0.0152) acc@1 0.9922 (0.9956) acc@5 1.0000 (1.0000)
[2022-06-11 10:22:07] __main__ INFO: Epoch 103 Step 300/390 lr 0.010000 loss 0.0121 (0.0153) acc@1 1.0000 (0.9958) acc@5 1.0000 (1.0000)
[2022-06-11 10:22:11] __main__ INFO: Epoch 103 Step 390/390 lr 0.010000 loss 0.0110 (0.0150) acc@1 1.0000 (0.9959) acc@5 1.0000 (1.0000)
[2022-06-11 10:22:11] __main__ INFO: Elapsed 15.70
[2022-06-11 10:22:11] __main__ INFO: Val 103
[2022-06-11 10:22:12] __main__ INFO: Epoch 103 loss 0.3255 acc@1 0.9215 acc@5 0.9984
[2022-06-11 10:22:12] __main__ INFO: Elapsed 0.97
[2022-06-11 10:22:12] __main__ INFO: Train 104 40170
[2022-06-11 10:22:16] __main__ INFO: Epoch 104 Step 100/390 lr 0.010000 loss 0.0080 (0.0146) acc@1 1.0000 (0.9962) acc@5 1.0000 (1.0000)
[2022-06-11 10:22:20] __main__ INFO: Epoch 104 Step 200/390 lr 0.010000 loss 0.0050 (0.0135) acc@1 1.0000 (0.9965) acc@5 1.0000 (1.0000)
[2022-06-11 10:22:24] __main__ INFO: Epoch 104 Step 300/390 lr 0.010000 loss 0.0224 (0.0131) acc@1 0.9922 (0.9968) acc@5 1.0000 (1.0000)
[2022-06-11 10:22:27] __main__ INFO: Epoch 104 Step 390/390 lr 0.010000 loss 0.0241 (0.0132) acc@1 0.9844 (0.9967) acc@5 1.0000 (1.0000)
[2022-06-11 10:22:27] __main__ INFO: Elapsed 15.54
[2022-06-11 10:22:27] __main__ INFO: Val 104
[2022-06-11 10:22:28] __main__ INFO: Epoch 104 loss 0.3291 acc@1 0.9229 acc@5 0.9981
[2022-06-11 10:22:28] __main__ INFO: Elapsed 0.95
[2022-06-11 10:22:28] __main__ INFO: Train 105 40560
[2022-06-11 10:22:32] __main__ INFO: Epoch 105 Step 100/390 lr 0.010000 loss 0.0369 (0.0129) acc@1 0.9844 (0.9965) acc@5 1.0000 (1.0000)
[2022-06-11 10:22:36] __main__ INFO: Epoch 105 Step 200/390 lr 0.010000 loss 0.0176 (0.0128) acc@1 0.9922 (0.9967) acc@5 1.0000 (1.0000)
[2022-06-11 10:22:40] __main__ INFO: Epoch 105 Step 300/390 lr 0.010000 loss 0.0210 (0.0131) acc@1 0.9844 (0.9967) acc@5 1.0000 (1.0000)
[2022-06-11 10:22:44] __main__ INFO: Epoch 105 Step 390/390 lr 0.010000 loss 0.0223 (0.0136) acc@1 0.9922 (0.9966) acc@5 1.0000 (1.0000)
[2022-06-11 10:22:44] __main__ INFO: Elapsed 15.77
[2022-06-11 10:22:44] __main__ INFO: Val 105
[2022-06-11 10:22:45] __main__ INFO: Epoch 105 loss 0.3259 acc@1 0.9234 acc@5 0.9980
[2022-06-11 10:22:45] __main__ INFO: Elapsed 0.92
[2022-06-11 10:22:45] __main__ INFO: Train 106 40950
[2022-06-11 10:22:49] __main__ INFO: Epoch 106 Step 100/390 lr 0.010000 loss 0.0076 (0.0118) acc@1 1.0000 (0.9971) acc@5 1.0000 (1.0000)
[2022-06-11 10:22:53] __main__ INFO: Epoch 106 Step 200/390 lr 0.010000 loss 0.0049 (0.0114) acc@1 1.0000 (0.9972) acc@5 1.0000 (1.0000)
[2022-06-11 10:22:57] __main__ INFO: Epoch 106 Step 300/390 lr 0.010000 loss 0.0101 (0.0121) acc@1 1.0000 (0.9970) acc@5 1.0000 (1.0000)
[2022-06-11 10:23:00] __main__ INFO: Epoch 106 Step 390/390 lr 0.010000 loss 0.0011 (0.0125) acc@1 1.0000 (0.9969) acc@5 1.0000 (1.0000)
[2022-06-11 10:23:01] __main__ INFO: Elapsed 15.64
[2022-06-11 10:23:01] __main__ INFO: Val 106
[2022-06-11 10:23:02] __main__ INFO: Epoch 106 loss 0.3271 acc@1 0.9249 acc@5 0.9978
[2022-06-11 10:23:02] __main__ INFO: Elapsed 0.96
[2022-06-11 10:23:02] __main__ INFO: Train 107 41340
[2022-06-11 10:23:06] __main__ INFO: Epoch 107 Step 100/390 lr 0.010000 loss 0.0056 (0.0108) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)
[2022-06-11 10:23:10] __main__ INFO: Epoch 107 Step 200/390 lr 0.010000 loss 0.0076 (0.0114) acc@1 1.0000 (0.9974) acc@5 1.0000 (1.0000)
[2022-06-11 10:23:14] __main__ INFO: Epoch 107 Step 300/390 lr 0.010000 loss 0.0186 (0.0114) acc@1 0.9922 (0.9970) acc@5 1.0000 (1.0000)
[2022-06-11 10:23:17] __main__ INFO: Epoch 107 Step 390/390 lr 0.010000 loss 0.0027 (0.0117) acc@1 1.0000 (0.9969) acc@5 1.0000 (1.0000)
[2022-06-11 10:23:17] __main__ INFO: Elapsed 15.92
[2022-06-11 10:23:17] __main__ INFO: Val 107
[2022-06-11 10:23:18] __main__ INFO: Epoch 107 loss 0.3328 acc@1 0.9237 acc@5 0.9977
[2022-06-11 10:23:18] __main__ INFO: Elapsed 0.91
[2022-06-11 10:23:18] __main__ INFO: Train 108 41730
[2022-06-11 10:23:23] __main__ INFO: Epoch 108 Step 100/390 lr 0.010000 loss 0.0044 (0.0143) acc@1 1.0000 (0.9958) acc@5 1.0000 (1.0000)
[2022-06-11 10:23:26] __main__ INFO: Epoch 108 Step 200/390 lr 0.010000 loss 0.0060 (0.0134) acc@1 1.0000 (0.9962) acc@5 1.0000 (1.0000)
[2022-06-11 10:23:30] __main__ INFO: Epoch 108 Step 300/390 lr 0.010000 loss 0.0194 (0.0128) acc@1 0.9922 (0.9965) acc@5 1.0000 (1.0000)
[2022-06-11 10:23:34] __main__ INFO: Epoch 108 Step 390/390 lr 0.010000 loss 0.0053 (0.0125) acc@1 1.0000 (0.9967) acc@5 1.0000 (1.0000)
[2022-06-11 10:23:34] __main__ INFO: Elapsed 15.69
[2022-06-11 10:23:34] __main__ INFO: Val 108
[2022-06-11 10:23:35] __main__ INFO: Epoch 108 loss 0.3304 acc@1 0.9239 acc@5 0.9977
[2022-06-11 10:23:35] __main__ INFO: Elapsed 0.91
[2022-06-11 10:23:35] __main__ INFO: Train 109 42120
[2022-06-11 10:23:39] __main__ INFO: Epoch 109 Step 100/390 lr 0.010000 loss 0.0232 (0.0108) acc@1 0.9844 (0.9974) acc@5 1.0000 (1.0000)
[2022-06-11 10:23:43] __main__ INFO: Epoch 109 Step 200/390 lr 0.010000 loss 0.0030 (0.0118) acc@1 1.0000 (0.9970) acc@5 1.0000 (1.0000)
[2022-06-11 10:23:47] __main__ INFO: Epoch 109 Step 300/390 lr 0.010000 loss 0.0125 (0.0118) acc@1 0.9922 (0.9968) acc@5 1.0000 (1.0000)
[2022-06-11 10:23:50] __main__ INFO: Epoch 109 Step 390/390 lr 0.010000 loss 0.0112 (0.0122) acc@1 1.0000 (0.9968) acc@5 1.0000 (1.0000)
[2022-06-11 10:23:51] __main__ INFO: Elapsed 15.61
[2022-06-11 10:23:51] __main__ INFO: Val 109
[2022-06-11 10:23:52] __main__ INFO: Epoch 109 loss 0.3293 acc@1 0.9226 acc@5 0.9973
[2022-06-11 10:23:52] __main__ INFO: Elapsed 0.96
[2022-06-11 10:23:52] __main__ INFO: Train 110 42510
[2022-06-11 10:23:56] __main__ INFO: Epoch 110 Step 100/390 lr 0.010000 loss 0.0127 (0.0104) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:00] __main__ INFO: Epoch 110 Step 200/390 lr 0.010000 loss 0.0354 (0.0106) acc@1 0.9922 (0.9976) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:04] __main__ INFO: Epoch 110 Step 300/390 lr 0.010000 loss 0.0714 (0.0113) acc@1 0.9766 (0.9972) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:07] __main__ INFO: Epoch 110 Step 390/390 lr 0.010000 loss 0.0118 (0.0112) acc@1 0.9922 (0.9973) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:07] __main__ INFO: Elapsed 15.63
[2022-06-11 10:24:07] __main__ INFO: Val 110
[2022-06-11 10:24:08] __main__ INFO: Epoch 110 loss 0.3347 acc@1 0.9238 acc@5 0.9981
[2022-06-11 10:24:08] __main__ INFO: Elapsed 0.93
[2022-06-11 10:24:08] __main__ INFO: Train 111 42900
[2022-06-11 10:24:12] __main__ INFO: Epoch 111 Step 100/390 lr 0.010000 loss 0.0154 (0.0119) acc@1 0.9922 (0.9967) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:16] __main__ INFO: Epoch 111 Step 200/390 lr 0.010000 loss 0.0149 (0.0116) acc@1 0.9922 (0.9968) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:20] __main__ INFO: Epoch 111 Step 300/390 lr 0.010000 loss 0.0379 (0.0112) acc@1 0.9922 (0.9971) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:24] __main__ INFO: Epoch 111 Step 390/390 lr 0.010000 loss 0.0229 (0.0114) acc@1 0.9922 (0.9970) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:24] __main__ INFO: Elapsed 15.54
[2022-06-11 10:24:24] __main__ INFO: Val 111
[2022-06-11 10:24:25] __main__ INFO: Epoch 111 loss 0.3314 acc@1 0.9236 acc@5 0.9980
[2022-06-11 10:24:25] __main__ INFO: Elapsed 0.95
[2022-06-11 10:24:25] __main__ INFO: Train 112 43290
[2022-06-11 10:24:29] __main__ INFO: Epoch 112 Step 100/390 lr 0.010000 loss 0.0054 (0.0097) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:33] __main__ INFO: Epoch 112 Step 200/390 lr 0.010000 loss 0.0113 (0.0101) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:37] __main__ INFO: Epoch 112 Step 300/390 lr 0.010000 loss 0.0124 (0.0100) acc@1 0.9922 (0.9979) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:40] __main__ INFO: Epoch 112 Step 390/390 lr 0.010000 loss 0.0067 (0.0104) acc@1 1.0000 (0.9976) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:41] __main__ INFO: Elapsed 15.96
[2022-06-11 10:24:41] __main__ INFO: Val 112
[2022-06-11 10:24:42] __main__ INFO: Epoch 112 loss 0.3295 acc@1 0.9269 acc@5 0.9978
[2022-06-11 10:24:42] __main__ INFO: Elapsed 0.94
[2022-06-11 10:24:42] __main__ INFO: Train 113 43680
[2022-06-11 10:24:46] __main__ INFO: Epoch 113 Step 100/390 lr 0.010000 loss 0.0218 (0.0108) acc@1 0.9922 (0.9974) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:50] __main__ INFO: Epoch 113 Step 200/390 lr 0.010000 loss 0.0106 (0.0103) acc@1 1.0000 (0.9975) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:54] __main__ INFO: Epoch 113 Step 300/390 lr 0.010000 loss 0.0092 (0.0103) acc@1 1.0000 (0.9976) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:58] __main__ INFO: Epoch 113 Step 390/390 lr 0.010000 loss 0.0017 (0.0104) acc@1 1.0000 (0.9976) acc@5 1.0000 (1.0000)
[2022-06-11 10:24:58] __main__ INFO: Elapsed 16.11
[2022-06-11 10:24:58] __main__ INFO: Val 113
[2022-06-11 10:24:59] __main__ INFO: Epoch 113 loss 0.3368 acc@1 0.9232 acc@5 0.9979
[2022-06-11 10:24:59] __main__ INFO: Elapsed 0.93
[2022-06-11 10:24:59] __main__ INFO: Train 114 44070
[2022-06-11 10:25:03] __main__ INFO: Epoch 114 Step 100/390 lr 0.010000 loss 0.0130 (0.0112) acc@1 0.9922 (0.9970) acc@5 1.0000 (1.0000)
[2022-06-11 10:25:07] __main__ INFO: Epoch 114 Step 200/390 lr 0.010000 loss 0.0103 (0.0106) acc@1 0.9922 (0.9973) acc@5 1.0000 (1.0000)
[2022-06-11 10:25:11] __main__ INFO: Epoch 114 Step 300/390 lr 0.010000 loss 0.0056 (0.0104) acc@1 1.0000 (0.9974) acc@5 1.0000 (1.0000)
[2022-06-11 10:25:14] __main__ INFO: Epoch 114 Step 390/390 lr 0.010000 loss 0.0054 (0.0105) acc@1 1.0000 (0.9975) acc@5 1.0000 (1.0000)
[2022-06-11 10:25:14] __main__ INFO: Elapsed 15.86
[2022-06-11 10:25:14] __main__ INFO: Val 114
[2022-06-11 10:25:15] __main__ INFO: Epoch 114 loss 0.3349 acc@1 0.9241 acc@5 0.9982
[2022-06-11 10:25:15] __main__ INFO: Elapsed 0.94
[2022-06-11 10:25:15] __main__ INFO: Train 115 44460
[2022-06-11 10:25:20] __main__ INFO: Epoch 115 Step 100/390 lr 0.010000 loss 0.0038 (0.0092) acc@1 1.0000 (0.9983) acc@5 1.0000 (1.0000)
[2022-06-11 10:25:23] __main__ INFO: Epoch 115 Step 200/390 lr 0.010000 loss 0.0056 (0.0091) acc@1 1.0000 (0.9983) acc@5 1.0000 (1.0000)
[2022-06-11 10:25:27] __main__ INFO: Epoch 115 Step 300/390 lr 0.010000 loss 0.0078 (0.0093) acc@1 1.0000 (0.9980) acc@5 1.0000 (1.0000)
[2022-06-11 10:25:31] __main__ INFO: Epoch 115 Step 390/390 lr 0.010000 loss 0.0032 (0.0099) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)
[2022-06-11 10:25:31] __main__ INFO: Elapsed 15.70
[2022-06-11 10:25:31] __main__ INFO: Val 115
[2022-06-11 10:25:32] __main__ INFO: Epoch 115 loss 0.3382 acc@1 0.9232 acc@5 0.9980
[2022-06-11 10:25:32] __main__ INFO: Elapsed 0.89
[2022-06-11 10:25:32] __main__ INFO: Train 116 44850
[2022-06-11 10:25:36] __main__ INFO: Epoch 116 Step 100/390 lr 0.010000 loss 0.0104 (0.0091) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)
[2022-06-11 10:25:40] __main__ INFO: Epoch 116 Step 200/390 lr 0.010000 loss 0.0052 (0.0097) acc@1 1.0000 (0.9974) acc@5 1.0000 (1.0000)
[2022-06-11 10:25:44] __main__ INFO: Epoch 116 Step 300/390 lr 0.010000 loss 0.0034 (0.0091) acc@1 1.0000 (0.9978) acc@5 1.0000 (1.0000)
[2022-06-11 10:25:48] __main__ INFO: Epoch 116 Step 390/390 lr 0.010000 loss 0.0038 (0.0094) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)
[2022-06-11 10:25:48] __main__ INFO: Elapsed 16.21
[2022-06-11 10:25:48] __main__ INFO: Val 116
[2022-06-11 10:25:49] __main__ INFO: Epoch 116 loss 0.3378 acc@1 0.9256 acc@5 0.9981
[2022-06-11 10:25:49] __main__ INFO: Elapsed 1.01
[2022-06-11 10:25:49] __main__ INFO: Train 117 45240
[2022-06-11 10:25:53] __main__ INFO: Epoch 117 Step 100/390 lr 0.010000 loss 0.0078 (0.0095) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)
[2022-06-11 10:25:57] __main__ INFO: Epoch 117 Step 200/390 lr 0.010000 loss 0.0057 (0.0097) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:02] __main__ INFO: Epoch 117 Step 300/390 lr 0.010000 loss 0.0104 (0.0095) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:05] __main__ INFO: Epoch 117 Step 390/390 lr 0.010000 loss 0.0465 (0.0098) acc@1 0.9766 (0.9975) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:05] __main__ INFO: Elapsed 16.14
[2022-06-11 10:26:05] __main__ INFO: Val 117
[2022-06-11 10:26:06] __main__ INFO: Epoch 117 loss 0.3420 acc@1 0.9234 acc@5 0.9975
[2022-06-11 10:26:06] __main__ INFO: Elapsed 1.00
[2022-06-11 10:26:06] __main__ INFO: Train 118 45630
[2022-06-11 10:26:10] __main__ INFO: Epoch 118 Step 100/390 lr 0.010000 loss 0.0028 (0.0096) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:14] __main__ INFO: Epoch 118 Step 200/390 lr 0.010000 loss 0.0137 (0.0091) acc@1 0.9922 (0.9979) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:18] __main__ INFO: Epoch 118 Step 300/390 lr 0.010000 loss 0.0096 (0.0090) acc@1 0.9922 (0.9980) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:22] __main__ INFO: Epoch 118 Step 390/390 lr 0.010000 loss 0.0078 (0.0092) acc@1 1.0000 (0.9978) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:22] __main__ INFO: Elapsed 15.50
[2022-06-11 10:26:22] __main__ INFO: Val 118
[2022-06-11 10:26:23] __main__ INFO: Epoch 118 loss 0.3374 acc@1 0.9246 acc@5 0.9981
[2022-06-11 10:26:23] __main__ INFO: Elapsed 0.93
[2022-06-11 10:26:23] __main__ INFO: Train 119 46020
[2022-06-11 10:26:27] __main__ INFO: Epoch 119 Step 100/390 lr 0.010000 loss 0.0241 (0.0096) acc@1 0.9922 (0.9981) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:31] __main__ INFO: Epoch 119 Step 200/390 lr 0.010000 loss 0.0026 (0.0095) acc@1 1.0000 (0.9978) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:35] __main__ INFO: Epoch 119 Step 300/390 lr 0.010000 loss 0.0066 (0.0094) acc@1 1.0000 (0.9978) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:38] __main__ INFO: Epoch 119 Step 390/390 lr 0.010000 loss 0.0066 (0.0091) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:39] __main__ INFO: Elapsed 15.84
[2022-06-11 10:26:39] __main__ INFO: Val 119
[2022-06-11 10:26:40] __main__ INFO: Epoch 119 loss 0.3417 acc@1 0.9243 acc@5 0.9982
[2022-06-11 10:26:40] __main__ INFO: Elapsed 0.96
[2022-06-11 10:26:40] __main__ INFO: Train 120 46410
[2022-06-11 10:26:44] __main__ INFO: Epoch 120 Step 100/390 lr 0.010000 loss 0.0080 (0.0086) acc@1 0.9922 (0.9981) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:48] __main__ INFO: Epoch 120 Step 200/390 lr 0.010000 loss 0.0103 (0.0091) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:52] __main__ INFO: Epoch 120 Step 300/390 lr 0.010000 loss 0.0150 (0.0089) acc@1 0.9922 (0.9978) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:55] __main__ INFO: Epoch 120 Step 390/390 lr 0.010000 loss 0.0097 (0.0091) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)
[2022-06-11 10:26:55] __main__ INFO: Elapsed 15.94
[2022-06-11 10:26:55] __main__ INFO: Val 120
[2022-06-11 10:26:56] __main__ INFO: Epoch 120 loss 0.3371 acc@1 0.9245 acc@5 0.9983
[2022-06-11 10:26:56] __main__ INFO: Elapsed 0.93
[2022-06-11 10:26:56] __main__ INFO: Train 121 46800
[2022-06-11 10:27:01] __main__ INFO: Epoch 121 Step 100/390 lr 0.001000 loss 0.0052 (0.0080) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)
[2022-06-11 10:27:04] __main__ INFO: Epoch 121 Step 200/390 lr 0.001000 loss 0.0036 (0.0078) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)
[2022-06-11 10:27:08] __main__ INFO: Epoch 121 Step 300/390 lr 0.001000 loss 0.0076 (0.0074) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)
[2022-06-11 10:27:12] __main__ INFO: Epoch 121 Step 390/390 lr 0.001000 loss 0.0044 (0.0077) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)
[2022-06-11 10:27:12] __main__ INFO: Elapsed 15.51
[2022-06-11 10:27:12] __main__ INFO: Val 121
[2022-06-11 10:27:13] __main__ INFO: Epoch 121 loss 0.3372 acc@1 0.9249 acc@5 0.9982
[2022-06-11 10:27:13] __main__ INFO: Elapsed 0.97
[2022-06-11 10:27:13] __main__ INFO: Train 122 47190
[2022-06-11 10:27:17] __main__ INFO: Epoch 122 Step 100/390 lr 0.001000 loss 0.0092 (0.0073) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)
[2022-06-11 10:27:21] __main__ INFO: Epoch 122 Step 200/390 lr 0.001000 loss 0.0058 (0.0075) acc@1 1.0000 (0.9983) acc@5 1.0000 (1.0000)
[2022-06-11 10:27:25] __main__ INFO: Epoch 122 Step 300/390 lr 0.001000 loss 0.0053 (0.0074) acc@1 1.0000 (0.9983) acc@5 1.0000 (1.0000)
[2022-06-11 10:27:29] __main__ INFO: Epoch 122 Step 390/390 lr 0.001000 loss 0.0069 (0.0075) acc@1 1.0000 (0.9983) acc@5 1.0000 (1.0000)
[2022-06-11 10:27:29] __main__ INFO: Elapsed 15.77
[2022-06-11 10:27:29] __main__ INFO: Val 122
[2022-06-11 10:27:30] __main__ INFO: Epoch 122 loss 0.3346 acc@1 0.9258 acc@5 0.9979
[2022-06-11 10:27:30] __main__ INFO: Elapsed 0.96
[2022-06-11 10:27:30] __main__ INFO: Train 123 47580
[2022-06-11 10:27:34] __main__ INFO: Epoch 123 Step 100/390 lr 0.001000 loss 0.0082 (0.0062) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:27:38] __main__ INFO: Epoch 123 Step 200/390 lr 0.001000 loss 0.0059 (0.0065) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:27:42] __main__ INFO: Epoch 123 Step 300/390 lr 0.001000 loss 0.0020 (0.0070) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)
[2022-06-11 10:27:45] __main__ INFO: Epoch 123 Step 390/390 lr 0.001000 loss 0.0136 (0.0071) acc@1 0.9922 (0.9986) acc@5 1.0000 (1.0000)
[2022-06-11 10:27:45] __main__ INFO: Elapsed 15.72
[2022-06-11 10:27:45] __main__ INFO: Val 123
[2022-06-11 10:27:46] __main__ INFO: Epoch 123 loss 0.3341 acc@1 0.9247 acc@5 0.9980
[2022-06-11 10:27:46] __main__ INFO: Elapsed 0.96
[2022-06-11 10:27:46] __main__ INFO: Train 124 47970
[2022-06-11 10:27:50] __main__ INFO: Epoch 124 Step 100/390 lr 0.001000 loss 0.0045 (0.0086) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)
[2022-06-11 10:27:54] __main__ INFO: Epoch 124 Step 200/390 lr 0.001000 loss 0.0027 (0.0076) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)
[2022-06-11 10:27:58] __main__ INFO: Epoch 124 Step 300/390 lr 0.001000 loss 0.0223 (0.0074) acc@1 0.9922 (0.9984) acc@5 1.0000 (1.0000)
[2022-06-11 10:28:02] __main__ INFO: Epoch 124 Step 390/390 lr 0.001000 loss 0.0039 (0.0073) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)
[2022-06-11 10:28:02] __main__ INFO: Elapsed 15.82
[2022-06-11 10:28:02] __main__ INFO: Val 124
[2022-06-11 10:28:03] __main__ INFO: Epoch 124 loss 0.3348 acc@1 0.9258 acc@5 0.9982
[2022-06-11 10:28:03] __main__ INFO: Elapsed 0.94
[2022-06-11 10:28:03] __main__ INFO: Train 125 48360
[2022-06-11 10:28:07] __main__ INFO: Epoch 125 Step 100/390 lr 0.001000 loss 0.0354 (0.0077) acc@1 0.9922 (0.9983) acc@5 1.0000 (1.0000)
[2022-06-11 10:28:11] __main__ INFO: Epoch 125 Step 200/390 lr 0.001000 loss 0.0044 (0.0068) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:28:15] __main__ INFO: Epoch 125 Step 300/390 lr 0.001000 loss 0.0058 (0.0069) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)
[2022-06-11 10:28:19] __main__ INFO: Epoch 125 Step 390/390 lr 0.001000 loss 0.0120 (0.0071) acc@1 0.9922 (0.9986) acc@5 1.0000 (1.0000)
[2022-06-11 10:28:19] __main__ INFO: Elapsed 15.77
[2022-06-11 10:28:19] __main__ INFO: Val 125
[2022-06-11 10:28:20] __main__ INFO: Epoch 125 loss 0.3356 acc@1 0.9260 acc@5 0.9982
[2022-06-11 10:28:20] __main__ INFO: Elapsed 0.94
[2022-06-11 10:28:20] __main__ INFO: Train 126 48750
[2022-06-11 10:28:24] __main__ INFO: Epoch 126 Step 100/390 lr 0.001000 loss 0.0039 (0.0072) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)
[2022-06-11 10:28:28] __main__ INFO: Epoch 126 Step 200/390 lr 0.001000 loss 0.0036 (0.0076) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)
[2022-06-11 10:28:32] __main__ INFO: Epoch 126 Step 300/390 lr 0.001000 loss 0.0049 (0.0076) acc@1 1.0000 (0.9983) acc@5 1.0000 (1.0000)
[2022-06-11 10:28:36] __main__ INFO: Epoch 126 Step 390/390 lr 0.001000 loss 0.0028 (0.0076) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)
[2022-06-11 10:28:36] __main__ INFO: Elapsed 15.83
[2022-06-11 10:28:36] __main__ INFO: Val 126
[2022-06-11 10:28:37] __main__ INFO: Epoch 126 loss 0.3332 acc@1 0.9261 acc@5 0.9980
[2022-06-11 10:28:37] __main__ INFO: Elapsed 0.96
[2022-06-11 10:28:37] __main__ INFO: Train 127 49140
[2022-06-11 10:28:41] __main__ INFO: Epoch 127 Step 100/390 lr 0.001000 loss 0.0024 (0.0068) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)
[2022-06-11 10:28:45] __main__ INFO: Epoch 127 Step 200/390 lr 0.001000 loss 0.0094 (0.0069) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)
[2022-06-11 10:28:49] __main__ INFO: Epoch 127 Step 300/390 lr 0.001000 loss 0.0051 (0.0072) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)
[2022-06-11 10:28:52] __main__ INFO: Epoch 127 Step 390/390 lr 0.001000 loss 0.0033 (0.0071) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)
[2022-06-11 10:28:52] __main__ INFO: Elapsed 15.64
[2022-06-11 10:28:52] __main__ INFO: Val 127
[2022-06-11 10:28:53] __main__ INFO: Epoch 127 loss 0.3333 acc@1 0.9273 acc@5 0.9983
[2022-06-11 10:28:53] __main__ INFO: Elapsed 0.98
[2022-06-11 10:28:53] __main__ INFO: Train 128 49530
[2022-06-11 10:28:57] __main__ INFO: Epoch 128 Step 100/390 lr 0.001000 loss 0.0028 (0.0067) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:29:02] __main__ INFO: Epoch 128 Step 200/390 lr 0.001000 loss 0.0021 (0.0067) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)
[2022-06-11 10:29:06] __main__ INFO: Epoch 128 Step 300/390 lr 0.001000 loss 0.0020 (0.0066) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:29:09] __main__ INFO: Epoch 128 Step 390/390 lr 0.001000 loss 0.0038 (0.0064) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:29:10] __main__ INFO: Elapsed 16.38
[2022-06-11 10:29:10] __main__ INFO: Val 128
[2022-06-11 10:29:11] __main__ INFO: Epoch 128 loss 0.3336 acc@1 0.9268 acc@5 0.9982
[2022-06-11 10:29:11] __main__ INFO: Elapsed 0.95
[2022-06-11 10:29:11] __main__ INFO: Train 129 49920
[2022-06-11 10:29:15] __main__ INFO: Epoch 129 Step 100/390 lr 0.001000 loss 0.0073 (0.0072) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)
[2022-06-11 10:29:19] __main__ INFO: Epoch 129 Step 200/390 lr 0.001000 loss 0.0060 (0.0071) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)
[2022-06-11 10:29:23] __main__ INFO: Epoch 129 Step 300/390 lr 0.001000 loss 0.0015 (0.0067) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)
[2022-06-11 10:29:26] __main__ INFO: Epoch 129 Step 390/390 lr 0.001000 loss 0.0021 (0.0065) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:29:26] __main__ INFO: Elapsed 15.72
[2022-06-11 10:29:26] __main__ INFO: Val 129
[2022-06-11 10:29:27] __main__ INFO: Epoch 129 loss 0.3328 acc@1 0.9267 acc@5 0.9982
[2022-06-11 10:29:27] __main__ INFO: Elapsed 0.92
[2022-06-11 10:29:27] __main__ INFO: Train 130 50310
[2022-06-11 10:29:31] __main__ INFO: Epoch 130 Step 100/390 lr 0.001000 loss 0.0047 (0.0057) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:29:35] __main__ INFO: Epoch 130 Step 200/390 lr 0.001000 loss 0.0048 (0.0056) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:29:40] __main__ INFO: Epoch 130 Step 300/390 lr 0.001000 loss 0.0022 (0.0061) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:29:43] __main__ INFO: Epoch 130 Step 390/390 lr 0.001000 loss 0.0029 (0.0061) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:29:43] __main__ INFO: Elapsed 16.03
[2022-06-11 10:29:43] __main__ INFO: Val 130
[2022-06-11 10:29:44] __main__ INFO: Epoch 130 loss 0.3332 acc@1 0.9258 acc@5 0.9982
[2022-06-11 10:29:44] __main__ INFO: Elapsed 0.96
[2022-06-11 10:29:44] __main__ INFO: Train 131 50700
[2022-06-11 10:29:48] __main__ INFO: Epoch 131 Step 100/390 lr 0.001000 loss 0.0112 (0.0061) acc@1 0.9922 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:29:52] __main__ INFO: Epoch 131 Step 200/390 lr 0.001000 loss 0.0026 (0.0059) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:29:56] __main__ INFO: Epoch 131 Step 300/390 lr 0.001000 loss 0.0196 (0.0068) acc@1 0.9922 (0.9986) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:00] __main__ INFO: Epoch 131 Step 390/390 lr 0.001000 loss 0.0069 (0.0068) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:00] __main__ INFO: Elapsed 15.98
[2022-06-11 10:30:00] __main__ INFO: Val 131
[2022-06-11 10:30:01] __main__ INFO: Epoch 131 loss 0.3339 acc@1 0.9273 acc@5 0.9983
[2022-06-11 10:30:01] __main__ INFO: Elapsed 0.90
[2022-06-11 10:30:01] __main__ INFO: Train 132 51090
[2022-06-11 10:30:05] __main__ INFO: Epoch 132 Step 100/390 lr 0.001000 loss 0.0064 (0.0062) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:09] __main__ INFO: Epoch 132 Step 200/390 lr 0.001000 loss 0.0073 (0.0064) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:13] __main__ INFO: Epoch 132 Step 300/390 lr 0.001000 loss 0.0027 (0.0063) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:17] __main__ INFO: Epoch 132 Step 390/390 lr 0.001000 loss 0.0036 (0.0065) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:17] __main__ INFO: Elapsed 15.63
[2022-06-11 10:30:17] __main__ INFO: Val 132
[2022-06-11 10:30:18] __main__ INFO: Epoch 132 loss 0.3362 acc@1 0.9260 acc@5 0.9981
[2022-06-11 10:30:18] __main__ INFO: Elapsed 0.96
[2022-06-11 10:30:18] __main__ INFO: Train 133 51480
[2022-06-11 10:30:22] __main__ INFO: Epoch 133 Step 100/390 lr 0.001000 loss 0.0055 (0.0067) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:26] __main__ INFO: Epoch 133 Step 200/390 lr 0.001000 loss 0.0054 (0.0062) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:30] __main__ INFO: Epoch 133 Step 300/390 lr 0.001000 loss 0.0036 (0.0062) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:33] __main__ INFO: Epoch 133 Step 390/390 lr 0.001000 loss 0.0072 (0.0062) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:33] __main__ INFO: Elapsed 15.64
[2022-06-11 10:30:33] __main__ INFO: Val 133
[2022-06-11 10:30:34] __main__ INFO: Epoch 133 loss 0.3328 acc@1 0.9266 acc@5 0.9982
[2022-06-11 10:30:34] __main__ INFO: Elapsed 0.93
[2022-06-11 10:30:34] __main__ INFO: Train 134 51870
[2022-06-11 10:30:38] __main__ INFO: Epoch 134 Step 100/390 lr 0.001000 loss 0.0065 (0.0062) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:42] __main__ INFO: Epoch 134 Step 200/390 lr 0.001000 loss 0.0072 (0.0060) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:46] __main__ INFO: Epoch 134 Step 300/390 lr 0.001000 loss 0.0050 (0.0063) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:50] __main__ INFO: Epoch 134 Step 390/390 lr 0.001000 loss 0.0045 (0.0062) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:50] __main__ INFO: Elapsed 15.77
[2022-06-11 10:30:50] __main__ INFO: Val 134
[2022-06-11 10:30:51] __main__ INFO: Epoch 134 loss 0.3339 acc@1 0.9271 acc@5 0.9981
[2022-06-11 10:30:51] __main__ INFO: Elapsed 0.94
[2022-06-11 10:30:51] __main__ INFO: Train 135 52260
[2022-06-11 10:30:55] __main__ INFO: Epoch 135 Step 100/390 lr 0.001000 loss 0.0108 (0.0056) acc@1 0.9922 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:30:59] __main__ INFO: Epoch 135 Step 200/390 lr 0.001000 loss 0.0101 (0.0058) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:03] __main__ INFO: Epoch 135 Step 300/390 lr 0.001000 loss 0.0070 (0.0059) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:07] __main__ INFO: Epoch 135 Step 390/390 lr 0.001000 loss 0.0052 (0.0059) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:07] __main__ INFO: Elapsed 15.70
[2022-06-11 10:31:07] __main__ INFO: Val 135
[2022-06-11 10:31:08] __main__ INFO: Epoch 135 loss 0.3347 acc@1 0.9272 acc@5 0.9982
[2022-06-11 10:31:08] __main__ INFO: Elapsed 0.92
[2022-06-11 10:31:08] __main__ INFO: Train 136 52650
[2022-06-11 10:31:12] __main__ INFO: Epoch 136 Step 100/390 lr 0.001000 loss 0.0041 (0.0059) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:16] __main__ INFO: Epoch 136 Step 200/390 lr 0.001000 loss 0.0026 (0.0062) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:20] __main__ INFO: Epoch 136 Step 300/390 lr 0.001000 loss 0.0223 (0.0063) acc@1 0.9922 (0.9987) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:23] __main__ INFO: Epoch 136 Step 390/390 lr 0.001000 loss 0.0054 (0.0063) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:23] __main__ INFO: Elapsed 15.69
[2022-06-11 10:31:23] __main__ INFO: Val 136
[2022-06-11 10:31:24] __main__ INFO: Epoch 136 loss 0.3359 acc@1 0.9266 acc@5 0.9981
[2022-06-11 10:31:24] __main__ INFO: Elapsed 1.02
[2022-06-11 10:31:24] __main__ INFO: Train 137 53040
[2022-06-11 10:31:28] __main__ INFO: Epoch 137 Step 100/390 lr 0.001000 loss 0.0048 (0.0056) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:32] __main__ INFO: Epoch 137 Step 200/390 lr 0.001000 loss 0.0014 (0.0056) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:36] __main__ INFO: Epoch 137 Step 300/390 lr 0.001000 loss 0.0034 (0.0057) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:40] __main__ INFO: Epoch 137 Step 390/390 lr 0.001000 loss 0.0210 (0.0057) acc@1 0.9844 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:40] __main__ INFO: Elapsed 15.73
[2022-06-11 10:31:40] __main__ INFO: Val 137
[2022-06-11 10:31:41] __main__ INFO: Epoch 137 loss 0.3332 acc@1 0.9271 acc@5 0.9982
[2022-06-11 10:31:41] __main__ INFO: Elapsed 0.98
[2022-06-11 10:31:41] __main__ INFO: Train 138 53430
[2022-06-11 10:31:45] __main__ INFO: Epoch 138 Step 100/390 lr 0.001000 loss 0.0024 (0.0071) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:49] __main__ INFO: Epoch 138 Step 200/390 lr 0.001000 loss 0.0046 (0.0068) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:53] __main__ INFO: Epoch 138 Step 300/390 lr 0.001000 loss 0.0037 (0.0070) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:57] __main__ INFO: Epoch 138 Step 390/390 lr 0.001000 loss 0.0058 (0.0067) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:31:57] __main__ INFO: Elapsed 15.71
[2022-06-11 10:31:57] __main__ INFO: Val 138
[2022-06-11 10:31:58] __main__ INFO: Epoch 138 loss 0.3346 acc@1 0.9269 acc@5 0.9982
[2022-06-11 10:31:58] __main__ INFO: Elapsed 0.92
[2022-06-11 10:31:58] __main__ INFO: Train 139 53820
[2022-06-11 10:32:02] __main__ INFO: Epoch 139 Step 100/390 lr 0.001000 loss 0.0025 (0.0059) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:32:06] __main__ INFO: Epoch 139 Step 200/390 lr 0.001000 loss 0.0043 (0.0058) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:32:10] __main__ INFO: Epoch 139 Step 300/390 lr 0.001000 loss 0.0031 (0.0061) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:32:13] __main__ INFO: Epoch 139 Step 390/390 lr 0.001000 loss 0.0043 (0.0060) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:32:13] __main__ INFO: Elapsed 15.72
[2022-06-11 10:32:13] __main__ INFO: Val 139
[2022-06-11 10:32:14] __main__ INFO: Epoch 139 loss 0.3363 acc@1 0.9266 acc@5 0.9981
[2022-06-11 10:32:14] __main__ INFO: Elapsed 0.96
[2022-06-11 10:32:14] __main__ INFO: Train 140 54210
[2022-06-11 10:32:19] __main__ INFO: Epoch 140 Step 100/390 lr 0.001000 loss 0.0022 (0.0064) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:32:22] __main__ INFO: Epoch 140 Step 200/390 lr 0.001000 loss 0.0044 (0.0063) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:32:26] __main__ INFO: Epoch 140 Step 300/390 lr 0.001000 loss 0.0017 (0.0064) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:32:30] __main__ INFO: Epoch 140 Step 390/390 lr 0.001000 loss 0.0125 (0.0062) acc@1 0.9922 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:32:30] __main__ INFO: Elapsed 15.76
[2022-06-11 10:32:30] __main__ INFO: Val 140
[2022-06-11 10:32:31] __main__ INFO: Epoch 140 loss 0.3373 acc@1 0.9267 acc@5 0.9981
[2022-06-11 10:32:31] __main__ INFO: Elapsed 0.92
[2022-06-11 10:32:31] __main__ INFO: Train 141 54600
[2022-06-11 10:32:35] __main__ INFO: Epoch 141 Step 100/390 lr 0.001000 loss 0.0035 (0.0053) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:32:39] __main__ INFO: Epoch 141 Step 200/390 lr 0.001000 loss 0.0054 (0.0061) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:32:43] __main__ INFO: Epoch 141 Step 300/390 lr 0.001000 loss 0.0061 (0.0059) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:32:47] __main__ INFO: Epoch 141 Step 390/390 lr 0.001000 loss 0.0043 (0.0058) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:32:47] __main__ INFO: Elapsed 16.02
[2022-06-11 10:32:47] __main__ INFO: Val 141
[2022-06-11 10:32:48] __main__ INFO: Epoch 141 loss 0.3360 acc@1 0.9276 acc@5 0.9981
[2022-06-11 10:32:48] __main__ INFO: Elapsed 1.02
[2022-06-11 10:32:48] __main__ INFO: Train 142 54990
[2022-06-11 10:32:52] __main__ INFO: Epoch 142 Step 100/390 lr 0.001000 loss 0.0058 (0.0057) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:32:56] __main__ INFO: Epoch 142 Step 200/390 lr 0.001000 loss 0.0057 (0.0064) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:00] __main__ INFO: Epoch 142 Step 300/390 lr 0.001000 loss 0.0074 (0.0063) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:04] __main__ INFO: Epoch 142 Step 390/390 lr 0.001000 loss 0.0032 (0.0063) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:04] __main__ INFO: Elapsed 15.77
[2022-06-11 10:33:04] __main__ INFO: Val 142
[2022-06-11 10:33:05] __main__ INFO: Epoch 142 loss 0.3345 acc@1 0.9268 acc@5 0.9981
[2022-06-11 10:33:05] __main__ INFO: Elapsed 0.93
[2022-06-11 10:33:05] __main__ INFO: Train 143 55380
[2022-06-11 10:33:09] __main__ INFO: Epoch 143 Step 100/390 lr 0.001000 loss 0.0029 (0.0059) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:13] __main__ INFO: Epoch 143 Step 200/390 lr 0.001000 loss 0.0023 (0.0057) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:17] __main__ INFO: Epoch 143 Step 300/390 lr 0.001000 loss 0.0222 (0.0058) acc@1 0.9922 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:20] __main__ INFO: Epoch 143 Step 390/390 lr 0.001000 loss 0.0018 (0.0058) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:21] __main__ INFO: Elapsed 15.79
[2022-06-11 10:33:21] __main__ INFO: Val 143
[2022-06-11 10:33:22] __main__ INFO: Epoch 143 loss 0.3364 acc@1 0.9265 acc@5 0.9980
[2022-06-11 10:33:22] __main__ INFO: Elapsed 0.93
[2022-06-11 10:33:22] __main__ INFO: Train 144 55770
[2022-06-11 10:33:26] __main__ INFO: Epoch 144 Step 100/390 lr 0.001000 loss 0.0027 (0.0057) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:30] __main__ INFO: Epoch 144 Step 200/390 lr 0.001000 loss 0.0042 (0.0064) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:34] __main__ INFO: Epoch 144 Step 300/390 lr 0.001000 loss 0.0040 (0.0061) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:37] __main__ INFO: Epoch 144 Step 390/390 lr 0.001000 loss 0.0034 (0.0059) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:37] __main__ INFO: Elapsed 15.73
[2022-06-11 10:33:37] __main__ INFO: Val 144
[2022-06-11 10:33:38] __main__ INFO: Epoch 144 loss 0.3348 acc@1 0.9269 acc@5 0.9981
[2022-06-11 10:33:38] __main__ INFO: Elapsed 0.96
[2022-06-11 10:33:38] __main__ INFO: Train 145 56160
[2022-06-11 10:33:42] __main__ INFO: Epoch 145 Step 100/390 lr 0.001000 loss 0.0053 (0.0065) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:46] __main__ INFO: Epoch 145 Step 200/390 lr 0.001000 loss 0.0062 (0.0066) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:50] __main__ INFO: Epoch 145 Step 300/390 lr 0.001000 loss 0.0048 (0.0063) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:54] __main__ INFO: Epoch 145 Step 390/390 lr 0.001000 loss 0.0046 (0.0064) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)
[2022-06-11 10:33:54] __main__ INFO: Elapsed 15.69
[2022-06-11 10:33:54] __main__ INFO: Val 145
[2022-06-11 10:33:55] __main__ INFO: Epoch 145 loss 0.3356 acc@1 0.9269 acc@5 0.9979
[2022-06-11 10:33:55] __main__ INFO: Elapsed 0.93
[2022-06-11 10:33:55] __main__ INFO: Train 146 56550
[2022-06-11 10:33:59] __main__ INFO: Epoch 146 Step 100/390 lr 0.001000 loss 0.0037 (0.0056) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)
[2022-06-11 10:34:03] __main__ INFO: Epoch 146 Step 200/390 lr 0.001000 loss 0.0009 (0.0059) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)
[2022-06-11 10:34:07] __main__ INFO: Epoch 146 Step 300/390 lr 0.001000 loss 0.0023 (0.0059) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:34:10] __main__ INFO: Epoch 146 Step 390/390 lr 0.001000 loss 0.0045 (0.0058) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:34:11] __main__ INFO: Elapsed 15.74
[2022-06-11 10:34:11] __main__ INFO: Val 146
[2022-06-11 10:34:12] __main__ INFO: Epoch 146 loss 0.3340 acc@1 0.9270 acc@5 0.9979
[2022-06-11 10:34:12] __main__ INFO: Elapsed 0.92
[2022-06-11 10:34:12] __main__ INFO: Train 147 56940
[2022-06-11 10:34:16] __main__ INFO: Epoch 147 Step 100/390 lr 0.001000 loss 0.0075 (0.0062) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:34:20] __main__ INFO: Epoch 147 Step 200/390 lr 0.001000 loss 0.0040 (0.0056) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:34:24] __main__ INFO: Epoch 147 Step 300/390 lr 0.001000 loss 0.0028 (0.0059) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:34:27] __main__ INFO: Epoch 147 Step 390/390 lr 0.001000 loss 0.0010 (0.0059) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:34:27] __main__ INFO: Elapsed 15.77
[2022-06-11 10:34:27] __main__ INFO: Val 147
[2022-06-11 10:34:28] __main__ INFO: Epoch 147 loss 0.3350 acc@1 0.9273 acc@5 0.9981
[2022-06-11 10:34:28] __main__ INFO: Elapsed 0.88
[2022-06-11 10:34:28] __main__ INFO: Train 148 57330
[2022-06-11 10:34:32] __main__ INFO: Epoch 148 Step 100/390 lr 0.001000 loss 0.0044 (0.0063) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:34:36] __main__ INFO: Epoch 148 Step 200/390 lr 0.001000 loss 0.0027 (0.0059) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:34:40] __main__ INFO: Epoch 148 Step 300/390 lr 0.001000 loss 0.0059 (0.0057) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)
[2022-06-11 10:34:44] __main__ INFO: Epoch 148 Step 390/390 lr 0.001000 loss 0.0237 (0.0059) acc@1 0.9922 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:34:44] __main__ INFO: Elapsed 16.07
[2022-06-11 10:34:44] __main__ INFO: Val 148
[2022-06-11 10:34:45] __main__ INFO: Epoch 148 loss 0.3374 acc@1 0.9266 acc@5 0.9981
[2022-06-11 10:34:45] __main__ INFO: Elapsed 0.95
[2022-06-11 10:34:45] __main__ INFO: Train 149 57720
[2022-06-11 10:34:49] __main__ INFO: Epoch 149 Step 100/390 lr 0.001000 loss 0.0026 (0.0054) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)
[2022-06-11 10:34:53] __main__ INFO: Epoch 149 Step 200/390 lr 0.001000 loss 0.0056 (0.0057) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:34:57] __main__ INFO: Epoch 149 Step 300/390 lr 0.001000 loss 0.0048 (0.0057) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:35:01] __main__ INFO: Epoch 149 Step 390/390 lr 0.001000 loss 0.0046 (0.0057) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:35:01] __main__ INFO: Elapsed 15.63
[2022-06-11 10:35:01] __main__ INFO: Val 149
[2022-06-11 10:35:02] __main__ INFO: Epoch 149 loss 0.3367 acc@1 0.9266 acc@5 0.9983
[2022-06-11 10:35:02] __main__ INFO: Elapsed 0.98
[2022-06-11 10:35:02] __main__ INFO: Train 150 58110
[2022-06-11 10:35:06] __main__ INFO: Epoch 150 Step 100/390 lr 0.001000 loss 0.0055 (0.0053) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:35:10] __main__ INFO: Epoch 150 Step 200/390 lr 0.001000 loss 0.0041 (0.0051) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)
[2022-06-11 10:35:14] __main__ INFO: Epoch 150 Step 300/390 lr 0.001000 loss 0.0046 (0.0053) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)
[2022-06-11 10:35:17] __main__ INFO: Epoch 150 Step 390/390 lr 0.001000 loss 0.0030 (0.0055) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:35:18] __main__ INFO: Elapsed 15.80
[2022-06-11 10:35:18] __main__ INFO: Val 150
[2022-06-11 10:35:19] __main__ INFO: Epoch 150 loss 0.3352 acc@1 0.9272 acc@5 0.9981
[2022-06-11 10:35:19] __main__ INFO: Elapsed 0.96
[2022-06-11 10:35:19] __main__ INFO: Train 151 58500
[2022-06-11 10:35:23] __main__ INFO: Epoch 151 Step 100/390 lr 0.001000 loss 0.0057 (0.0057) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:35:27] __main__ INFO: Epoch 151 Step 200/390 lr 0.001000 loss 0.0042 (0.0056) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:35:31] __main__ INFO: Epoch 151 Step 300/390 lr 0.001000 loss 0.0042 (0.0054) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:35:34] __main__ INFO: Epoch 151 Step 390/390 lr 0.001000 loss 0.0060 (0.0054) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:35:34] __main__ INFO: Elapsed 15.92
[2022-06-11 10:35:34] __main__ INFO: Val 151
[2022-06-11 10:35:36] __main__ INFO: Epoch 151 loss 0.3377 acc@1 0.9272 acc@5 0.9981
[2022-06-11 10:35:36] __main__ INFO: Elapsed 1.04
[2022-06-11 10:35:36] __main__ INFO: Train 152 58890
[2022-06-11 10:35:40] __main__ INFO: Epoch 152 Step 100/390 lr 0.001000 loss 0.0056 (0.0056) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:35:44] __main__ INFO: Epoch 152 Step 200/390 lr 0.001000 loss 0.0035 (0.0055) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:35:48] __main__ INFO: Epoch 152 Step 300/390 lr 0.001000 loss 0.0013 (0.0057) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:35:51] __main__ INFO: Epoch 152 Step 390/390 lr 0.001000 loss 0.0038 (0.0055) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:35:51] __main__ INFO: Elapsed 15.76
[2022-06-11 10:35:51] __main__ INFO: Val 152
[2022-06-11 10:35:52] __main__ INFO: Epoch 152 loss 0.3377 acc@1 0.9270 acc@5 0.9982
[2022-06-11 10:35:52] __main__ INFO: Elapsed 0.97
[2022-06-11 10:35:52] __main__ INFO: Train 153 59280
[2022-06-11 10:35:56] __main__ INFO: Epoch 153 Step 100/390 lr 0.001000 loss 0.0153 (0.0063) acc@1 0.9922 (0.9987) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:00] __main__ INFO: Epoch 153 Step 200/390 lr 0.001000 loss 0.0035 (0.0059) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:04] __main__ INFO: Epoch 153 Step 300/390 lr 0.001000 loss 0.0062 (0.0057) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:08] __main__ INFO: Epoch 153 Step 390/390 lr 0.001000 loss 0.0128 (0.0057) acc@1 0.9922 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:08] __main__ INFO: Elapsed 15.73
[2022-06-11 10:36:08] __main__ INFO: Val 153
[2022-06-11 10:36:09] __main__ INFO: Epoch 153 loss 0.3357 acc@1 0.9265 acc@5 0.9983
[2022-06-11 10:36:09] __main__ INFO: Elapsed 0.94
[2022-06-11 10:36:09] __main__ INFO: Train 154 59670
[2022-06-11 10:36:13] __main__ INFO: Epoch 154 Step 100/390 lr 0.001000 loss 0.0048 (0.0051) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:17] __main__ INFO: Epoch 154 Step 200/390 lr 0.001000 loss 0.0085 (0.0051) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:21] __main__ INFO: Epoch 154 Step 300/390 lr 0.001000 loss 0.0046 (0.0053) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:25] __main__ INFO: Epoch 154 Step 390/390 lr 0.001000 loss 0.0077 (0.0054) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:25] __main__ INFO: Elapsed 15.85
[2022-06-11 10:36:25] __main__ INFO: Val 154
[2022-06-11 10:36:26] __main__ INFO: Epoch 154 loss 0.3375 acc@1 0.9269 acc@5 0.9982
[2022-06-11 10:36:26] __main__ INFO: Elapsed 0.96
[2022-06-11 10:36:26] __main__ INFO: Train 155 60060
[2022-06-11 10:36:30] __main__ INFO: Epoch 155 Step 100/390 lr 0.001000 loss 0.0028 (0.0061) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:34] __main__ INFO: Epoch 155 Step 200/390 lr 0.001000 loss 0.0175 (0.0060) acc@1 0.9922 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:38] __main__ INFO: Epoch 155 Step 300/390 lr 0.001000 loss 0.0041 (0.0061) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:41] __main__ INFO: Epoch 155 Step 390/390 lr 0.001000 loss 0.0038 (0.0059) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:42] __main__ INFO: Elapsed 15.75
[2022-06-11 10:36:42] __main__ INFO: Val 155
[2022-06-11 10:36:42] __main__ INFO: Epoch 155 loss 0.3379 acc@1 0.9256 acc@5 0.9981
[2022-06-11 10:36:42] __main__ INFO: Elapsed 0.95
[2022-06-11 10:36:42] __main__ INFO: Train 156 60450
[2022-06-11 10:36:47] __main__ INFO: Epoch 156 Step 100/390 lr 0.001000 loss 0.0027 (0.0050) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:50] __main__ INFO: Epoch 156 Step 200/390 lr 0.001000 loss 0.0033 (0.0057) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:54] __main__ INFO: Epoch 156 Step 300/390 lr 0.001000 loss 0.0081 (0.0059) acc@1 0.9922 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:58] __main__ INFO: Epoch 156 Step 390/390 lr 0.001000 loss 0.0036 (0.0057) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:36:58] __main__ INFO: Elapsed 15.58
[2022-06-11 10:36:58] __main__ INFO: Val 156
[2022-06-11 10:36:59] __main__ INFO: Epoch 156 loss 0.3391 acc@1 0.9267 acc@5 0.9980
[2022-06-11 10:36:59] __main__ INFO: Elapsed 0.91
[2022-06-11 10:36:59] __main__ INFO: Train 157 60840
[2022-06-11 10:37:03] __main__ INFO: Epoch 157 Step 100/390 lr 0.001000 loss 0.0097 (0.0055) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)
[2022-06-11 10:37:07] __main__ INFO: Epoch 157 Step 200/390 lr 0.001000 loss 0.0103 (0.0053) acc@1 0.9922 (0.9994) acc@5 1.0000 (1.0000)
[2022-06-11 10:37:11] __main__ INFO: Epoch 157 Step 300/390 lr 0.001000 loss 0.0024 (0.0054) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)
[2022-06-11 10:37:15] __main__ INFO: Epoch 157 Step 390/390 lr 0.001000 loss 0.0007 (0.0054) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)
[2022-06-11 10:37:15] __main__ INFO: Elapsed 15.76
[2022-06-11 10:37:15] __main__ INFO: Val 157
[2022-06-11 10:37:16] __main__ INFO: Epoch 157 loss 0.3397 acc@1 0.9260 acc@5 0.9981
[2022-06-11 10:37:16] __main__ INFO: Elapsed 0.95
[2022-06-11 10:37:16] __main__ INFO: Train 158 61230
[2022-06-11 10:37:20] __main__ INFO: Epoch 158 Step 100/390 lr 0.001000 loss 0.0058 (0.0058) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:37:24] __main__ INFO: Epoch 158 Step 200/390 lr 0.001000 loss 0.0295 (0.0059) acc@1 0.9922 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:37:28] __main__ INFO: Epoch 158 Step 300/390 lr 0.001000 loss 0.0022 (0.0059) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:37:31] __main__ INFO: Epoch 158 Step 390/390 lr 0.001000 loss 0.0085 (0.0061) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)
[2022-06-11 10:37:31] __main__ INFO: Elapsed 15.76
[2022-06-11 10:37:31] __main__ INFO: Val 158
[2022-06-11 10:37:32] __main__ INFO: Epoch 158 loss 0.3394 acc@1 0.9261 acc@5 0.9982
[2022-06-11 10:37:32] __main__ INFO: Elapsed 0.93
[2022-06-11 10:37:32] __main__ INFO: Train 159 61620
[2022-06-11 10:37:37] __main__ INFO: Epoch 159 Step 100/390 lr 0.001000 loss 0.0038 (0.0056) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:37:41] __main__ INFO: Epoch 159 Step 200/390 lr 0.001000 loss 0.0041 (0.0055) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:37:45] __main__ INFO: Epoch 159 Step 300/390 lr 0.001000 loss 0.0081 (0.0055) acc@1 0.9922 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:37:48] __main__ INFO: Epoch 159 Step 390/390 lr 0.001000 loss 0.0014 (0.0055) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)
[2022-06-11 10:37:48] __main__ INFO: Elapsed 15.96
[2022-06-11 10:37:48] __main__ INFO: Val 159
[2022-06-11 10:37:49] __main__ INFO: Epoch 159 loss 0.3392 acc@1 0.9253 acc@5 0.9982
[2022-06-11 10:37:49] __main__ INFO: Elapsed 0.97
[2022-06-11 10:37:49] __main__ INFO: Train 160 62010
[2022-06-11 10:37:54] __main__ INFO: Epoch 160 Step 100/390 lr 0.001000 loss 0.0034 (0.0054) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)
[2022-06-11 10:37:58] __main__ INFO: Epoch 160 Step 200/390 lr 0.001000 loss 0.0026 (0.0053) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:38:02] __main__ INFO: Epoch 160 Step 300/390 lr 0.001000 loss 0.0014 (0.0051) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)
[2022-06-11 10:38:05] __main__ INFO: Epoch 160 Step 390/390 lr 0.001000 loss 0.0021 (0.0053) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)
[2022-06-11 10:38:05] __main__ INFO: Elapsed 16.19
[2022-06-11 10:38:05] __main__ INFO: Val 160
[2022-06-11 10:38:07] __main__ INFO: Epoch 160 loss 0.3397 acc@1 0.9269 acc@5 0.9983
[2022-06-11 10:38:07] __main__ INFO: Elapsed 1.02
[2022-06-11 10:38:07] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00160.pth
[2022-06-11 10:40:57] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 10:40:57] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 10:41:01] __main__ INFO: MACs   : 112.57M
[2022-06-11 10:41:01] __main__ INFO: #params: 758.55K
[2022-06-11 10:41:01] __main__ INFO: Val 0
[2022-06-11 10:41:02] __main__ INFO: Epoch 0 loss 16761.8818 acc@1 0.1000 acc@5 0.5000
[2022-06-11 10:41:02] __main__ INFO: Elapsed 1.02
[2022-06-11 10:41:02] __main__ INFO: Train 1 0
[2022-06-11 10:41:06] __main__ INFO: Epoch 1 Step 100/390 lr 0.100000 loss 2.0921 (3.0445) acc@1 0.2266 (0.1339) acc@5 0.7344 (0.5749)
[2022-06-11 10:41:10] __main__ INFO: Epoch 1 Step 200/390 lr 0.100000 loss 1.9818 (2.5336) acc@1 0.2109 (0.1777) acc@5 0.7891 (0.6750)
[2022-06-11 10:41:14] __main__ INFO: Epoch 1 Step 300/390 lr 0.100000 loss 1.8391 (2.3184) acc@1 0.3594 (0.2121) acc@5 0.8359 (0.7251)
[2022-06-11 10:41:18] __main__ INFO: Epoch 1 Step 390/390 lr 0.100000 loss 1.7720 (2.2011) acc@1 0.3672 (0.2393) acc@5 0.8984 (0.7548)
[2022-06-11 10:41:18] __main__ INFO: Elapsed 15.83
[2022-06-11 10:41:18] __main__ INFO: Val 1
[2022-06-11 10:41:19] __main__ INFO: Epoch 1 loss 1.7234 acc@1 0.3559 acc@5 0.8697
[2022-06-11 10:41:19] __main__ INFO: Elapsed 0.91
[2022-06-11 10:41:19] __main__ INFO: Train 2 390
[2022-06-11 10:41:39] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 10:41:39] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 10:41:43] __main__ INFO: MACs   : 112.57M
[2022-06-11 10:41:43] __main__ INFO: #params: 758.55K
[2022-06-11 10:41:56] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 10:41:56] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 10:42:00] __main__ INFO: MACs   : 112.57M
[2022-06-11 10:42:00] __main__ INFO: #params: 758.55K
[2022-06-11 10:42:00] __main__ INFO: Val 0
[2022-06-11 10:42:01] __main__ INFO: Epoch 0 loss 2.9075 acc@1 0.1334 acc@5 0.5883
[2022-06-11 10:42:01] __main__ INFO: Elapsed 1.02
[2022-06-11 10:42:01] __main__ INFO: Train 1 0
[2022-06-11 10:42:05] __main__ INFO: Epoch 1 Step 100/390 lr 0.100000 loss 0.6691 (1.1213) acc@1 0.8047 (0.6034) acc@5 0.9844 (0.9481)
[2022-06-11 10:42:09] __main__ INFO: Epoch 1 Step 200/390 lr 0.100000 loss 0.7345 (0.9261) acc@1 0.7266 (0.6763) acc@5 0.9766 (0.9654)
[2022-06-11 10:42:13] __main__ INFO: Epoch 1 Step 300/390 lr 0.100000 loss 0.6907 (0.8324) acc@1 0.7344 (0.7102) acc@5 1.0000 (0.9728)
[2022-06-11 10:42:17] __main__ INFO: Epoch 1 Step 390/390 lr 0.100000 loss 0.5141 (0.7787) acc@1 0.8203 (0.7294) acc@5 0.9922 (0.9766)
[2022-06-11 10:42:17] __main__ INFO: Elapsed 15.89
[2022-06-11 10:42:17] __main__ INFO: Val 1
[2022-06-11 10:42:18] __main__ INFO: Epoch 1 loss 0.9312 acc@1 0.7042 acc@5 0.9808
[2022-06-11 10:42:18] __main__ INFO: Elapsed 0.98
[2022-06-11 10:42:18] __main__ INFO: Train 2 390
[2022-06-11 10:42:22] __main__ INFO: Epoch 2 Step 100/390 lr 0.100000 loss 0.5724 (0.5403) acc@1 0.7891 (0.8145) acc@5 0.9844 (0.9914)
[2022-06-11 10:42:26] __main__ INFO: Epoch 2 Step 200/390 lr 0.100000 loss 0.4236 (0.5283) acc@1 0.8359 (0.8182) acc@5 0.9922 (0.9916)
[2022-06-11 10:42:30] __main__ INFO: Epoch 2 Step 300/390 lr 0.100000 loss 0.3453 (0.5233) acc@1 0.9062 (0.8201) acc@5 1.0000 (0.9918)
[2022-06-11 10:42:33] __main__ INFO: Epoch 2 Step 390/390 lr 0.100000 loss 0.4180 (0.5189) acc@1 0.8438 (0.8216) acc@5 1.0000 (0.9917)
[2022-06-11 10:42:33] __main__ INFO: Elapsed 15.38
[2022-06-11 10:42:33] __main__ INFO: Val 2
[2022-06-11 10:42:34] __main__ INFO: Epoch 2 loss 0.6407 acc@1 0.7932 acc@5 0.9863
[2022-06-11 10:42:34] __main__ INFO: Elapsed 0.93
[2022-06-11 10:42:34] __main__ INFO: Train 3 780
[2022-06-11 10:42:38] __main__ INFO: Epoch 3 Step 100/390 lr 0.100000 loss 0.4105 (0.4575) acc@1 0.8438 (0.8416) acc@5 0.9922 (0.9945)
[2022-06-11 10:42:43] __main__ INFO: Epoch 3 Step 200/390 lr 0.100000 loss 0.4895 (0.4595) acc@1 0.8438 (0.8409) acc@5 1.0000 (0.9937)
[2022-06-11 10:42:46] __main__ INFO: Epoch 3 Step 300/390 lr 0.100000 loss 0.6639 (0.4528) acc@1 0.7578 (0.8445) acc@5 0.9844 (0.9935)
[2022-06-11 10:42:50] __main__ INFO: Epoch 3 Step 390/390 lr 0.100000 loss 0.5132 (0.4524) acc@1 0.8047 (0.8439) acc@5 0.9922 (0.9934)
[2022-06-11 10:42:50] __main__ INFO: Elapsed 15.83
[2022-06-11 10:42:50] __main__ INFO: Val 3
[2022-06-11 10:42:51] __main__ INFO: Epoch 3 loss 0.6262 acc@1 0.7988 acc@5 0.9880
[2022-06-11 10:42:51] __main__ INFO: Elapsed 0.93
[2022-06-11 10:42:51] __main__ INFO: Train 4 1170
[2022-06-11 10:42:55] __main__ INFO: Epoch 4 Step 100/390 lr 0.100000 loss 0.4061 (0.4096) acc@1 0.8516 (0.8577) acc@5 1.0000 (0.9963)
[2022-06-11 10:42:59] __main__ INFO: Epoch 4 Step 200/390 lr 0.100000 loss 0.4589 (0.4128) acc@1 0.8750 (0.8575) acc@5 0.9922 (0.9953)
[2022-06-11 10:43:03] __main__ INFO: Epoch 4 Step 300/390 lr 0.100000 loss 0.5466 (0.4142) acc@1 0.8359 (0.8571) acc@5 0.9922 (0.9949)
[2022-06-11 10:43:07] __main__ INFO: Epoch 4 Step 390/390 lr 0.100000 loss 0.3279 (0.4123) acc@1 0.8906 (0.8575) acc@5 0.9922 (0.9947)
[2022-06-11 10:43:07] __main__ INFO: Elapsed 15.88
[2022-06-11 10:43:07] __main__ INFO: Val 4
[2022-06-11 10:43:08] __main__ INFO: Epoch 4 loss 0.6014 acc@1 0.8002 acc@5 0.9893
[2022-06-11 10:43:08] __main__ INFO: Elapsed 0.94
[2022-06-11 10:43:08] __main__ INFO: Train 5 1560
[2022-06-11 10:43:12] __main__ INFO: Epoch 5 Step 100/390 lr 0.100000 loss 0.4272 (0.3719) acc@1 0.8750 (0.8752) acc@5 0.9922 (0.9950)
[2022-06-11 10:43:16] __main__ INFO: Epoch 5 Step 200/390 lr 0.100000 loss 0.3475 (0.3740) acc@1 0.8750 (0.8723) acc@5 0.9922 (0.9954)
[2022-06-11 10:43:20] __main__ INFO: Epoch 5 Step 300/390 lr 0.100000 loss 0.2680 (0.3766) acc@1 0.9141 (0.8716) acc@5 0.9922 (0.9949)
[2022-06-11 10:43:24] __main__ INFO: Epoch 5 Step 390/390 lr 0.100000 loss 0.3758 (0.3763) acc@1 0.8906 (0.8713) acc@5 0.9922 (0.9951)
[2022-06-11 10:43:24] __main__ INFO: Elapsed 15.82
[2022-06-11 10:43:24] __main__ INFO: Val 5
[2022-06-11 10:43:25] __main__ INFO: Epoch 5 loss 0.5451 acc@1 0.8250 acc@5 0.9886
[2022-06-11 10:43:25] __main__ INFO: Elapsed 0.93
[2022-06-11 10:43:25] __main__ INFO: Train 6 1950
[2022-06-11 10:43:29] __main__ INFO: Epoch 6 Step 100/390 lr 0.100000 loss 0.4855 (0.3594) acc@1 0.8516 (0.8762) acc@5 1.0000 (0.9962)
[2022-06-11 10:43:33] __main__ INFO: Epoch 6 Step 200/390 lr 0.100000 loss 0.2561 (0.3590) acc@1 0.9062 (0.8753) acc@5 1.0000 (0.9959)
[2022-06-11 10:43:37] __main__ INFO: Epoch 6 Step 300/390 lr 0.100000 loss 0.3281 (0.3557) acc@1 0.8750 (0.8768) acc@5 0.9922 (0.9957)
[2022-06-11 10:43:40] __main__ INFO: Epoch 6 Step 390/390 lr 0.100000 loss 0.3798 (0.3583) acc@1 0.8828 (0.8751) acc@5 1.0000 (0.9957)
[2022-06-11 10:43:40] __main__ INFO: Elapsed 15.68
[2022-06-11 10:43:40] __main__ INFO: Val 6
[2022-06-11 10:43:41] __main__ INFO: Epoch 6 loss 0.4687 acc@1 0.8438 acc@5 0.9932
[2022-06-11 10:43:41] __main__ INFO: Elapsed 0.93
[2022-06-11 10:43:41] __main__ INFO: Train 7 2340
[2022-06-11 10:43:45] __main__ INFO: Epoch 7 Step 100/390 lr 0.100000 loss 0.4040 (0.3372) acc@1 0.8516 (0.8834) acc@5 1.0000 (0.9959)
[2022-06-11 10:43:49] __main__ INFO: Epoch 7 Step 200/390 lr 0.100000 loss 0.2708 (0.3400) acc@1 0.9141 (0.8826) acc@5 0.9922 (0.9959)
[2022-06-11 10:56:27] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 10:56:27] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 10:56:31] __main__ INFO: MACs   : 112.57M
[2022-06-11 10:56:31] __main__ INFO: #params: 758.55K
[2022-06-11 10:56:31] __main__ INFO: Val 0
[2022-06-11 10:56:32] __main__ INFO: Epoch 0 loss 2.9075 acc@1 0.1334 acc@5 0.5883
[2022-06-11 10:56:32] __main__ INFO: Elapsed 1.01
[2022-06-11 10:56:32] __main__ INFO: Train 1 0
[2022-06-11 10:56:36] __main__ INFO: Epoch 1 Step 100/390 lr 0.100000 loss 0.6951 (1.1207) acc@1 0.7891 (0.6041) acc@5 0.9922 (0.9481)
[2022-06-11 10:56:40] __main__ INFO: Epoch 1 Step 200/390 lr 0.100000 loss 0.6980 (0.9291) acc@1 0.7578 (0.6738) acc@5 0.9922 (0.9647)
[2022-06-11 10:56:44] __main__ INFO: Epoch 1 Step 300/390 lr 0.100000 loss 0.6337 (0.8341) acc@1 0.7734 (0.7078) acc@5 1.0000 (0.9726)
[2022-06-11 10:56:47] __main__ INFO: Epoch 1 Step 390/390 lr 0.100000 loss 0.5439 (0.7788) acc@1 0.8359 (0.7276) acc@5 1.0000 (0.9764)
[2022-06-11 10:56:47] __main__ INFO: Elapsed 15.91
[2022-06-11 10:56:47] __main__ INFO: Val 1
[2022-06-11 10:56:48] __main__ INFO: Epoch 1 loss 0.8453 acc@1 0.7230 acc@5 0.9805
[2022-06-11 10:56:48] __main__ INFO: Elapsed 0.97
[2022-06-11 10:56:48] __main__ INFO: Train 2 390
[2022-06-11 10:56:53] __main__ INFO: Epoch 2 Step 100/390 lr 0.100000 loss 0.6121 (0.5395) acc@1 0.7969 (0.8155) acc@5 0.9844 (0.9912)
[2022-06-11 10:56:57] __main__ INFO: Epoch 2 Step 200/390 lr 0.100000 loss 0.4845 (0.5274) acc@1 0.8438 (0.8182) acc@5 0.9922 (0.9912)
[2022-06-11 10:57:01] __main__ INFO: Epoch 2 Step 300/390 lr 0.100000 loss 0.3920 (0.5210) acc@1 0.8828 (0.8208) acc@5 0.9922 (0.9913)
[2022-06-11 10:57:04] __main__ INFO: Epoch 2 Step 390/390 lr 0.100000 loss 0.3620 (0.5155) acc@1 0.8672 (0.8228) acc@5 1.0000 (0.9915)
[2022-06-11 10:57:04] __main__ INFO: Elapsed 15.82
[2022-06-11 10:57:04] __main__ INFO: Val 2
[2022-06-11 10:57:05] __main__ INFO: Epoch 2 loss 0.6125 acc@1 0.8026 acc@5 0.9878
[2022-06-11 10:57:05] __main__ INFO: Elapsed 0.89
[2022-06-11 10:57:05] __main__ INFO: Train 3 780
[2022-06-11 10:57:09] __main__ INFO: Epoch 3 Step 100/390 lr 0.100000 loss 0.4144 (0.4560) acc@1 0.8594 (0.8421) acc@5 0.9844 (0.9934)
[2022-06-11 10:57:13] __main__ INFO: Epoch 3 Step 200/390 lr 0.100000 loss 0.5340 (0.4564) acc@1 0.8438 (0.8409) acc@5 0.9922 (0.9931)
[2022-06-11 10:57:17] __main__ INFO: Epoch 3 Step 300/390 lr 0.100000 loss 0.5588 (0.4509) acc@1 0.7812 (0.8440) acc@5 0.9766 (0.9932)
[2022-06-11 10:57:21] __main__ INFO: Epoch 3 Step 390/390 lr 0.100000 loss 0.5174 (0.4521) acc@1 0.7812 (0.8432) acc@5 0.9922 (0.9932)
[2022-06-11 10:57:21] __main__ INFO: Elapsed 15.63
[2022-06-11 10:57:21] __main__ INFO: Val 3
[2022-06-11 10:57:22] __main__ INFO: Epoch 3 loss 0.6062 acc@1 0.8054 acc@5 0.9853
[2022-06-11 10:57:22] __main__ INFO: Elapsed 0.94
[2022-06-11 10:57:22] __main__ INFO: Train 4 1170
[2022-06-11 10:57:26] __main__ INFO: Epoch 4 Step 100/390 lr 0.100000 loss 0.4386 (0.4043) acc@1 0.8672 (0.8594) acc@5 0.9844 (0.9952)
[2022-06-11 10:57:30] __main__ INFO: Epoch 4 Step 200/390 lr 0.100000 loss 0.4525 (0.4087) acc@1 0.8359 (0.8574) acc@5 1.0000 (0.9950)
[2022-06-11 10:57:34] __main__ INFO: Epoch 4 Step 300/390 lr 0.100000 loss 0.4729 (0.4107) acc@1 0.8516 (0.8561) acc@5 0.9922 (0.9946)
[2022-06-11 10:57:37] __main__ INFO: Epoch 4 Step 390/390 lr 0.100000 loss 0.2988 (0.4105) acc@1 0.8984 (0.8572) acc@5 1.0000 (0.9946)
[2022-06-11 10:57:37] __main__ INFO: Elapsed 15.48
[2022-06-11 10:57:37] __main__ INFO: Val 4
[2022-06-11 10:57:38] __main__ INFO: Epoch 4 loss 0.5357 acc@1 0.8176 acc@5 0.9896
[2022-06-11 10:57:38] __main__ INFO: Elapsed 0.94
[2022-06-11 10:57:38] __main__ INFO: Train 5 1560
[2022-06-11 10:57:42] __main__ INFO: Epoch 5 Step 100/390 lr 0.100000 loss 0.3871 (0.3745) acc@1 0.8906 (0.8741) acc@5 0.9922 (0.9955)
[2022-06-11 10:57:46] __main__ INFO: Epoch 5 Step 200/390 lr 0.100000 loss 0.2828 (0.3717) acc@1 0.9297 (0.8736) acc@5 0.9922 (0.9952)
[2022-06-11 10:57:50] __main__ INFO: Epoch 5 Step 300/390 lr 0.100000 loss 0.2867 (0.3730) acc@1 0.9062 (0.8727) acc@5 0.9922 (0.9951)
[2022-06-11 10:57:54] __main__ INFO: Epoch 5 Step 390/390 lr 0.100000 loss 0.3977 (0.3736) acc@1 0.8750 (0.8719) acc@5 0.9922 (0.9951)
[2022-06-11 10:57:54] __main__ INFO: Elapsed 15.85
[2022-06-11 10:57:54] __main__ INFO: Val 5
[2022-06-11 10:57:55] __main__ INFO: Epoch 5 loss 0.6330 acc@1 0.7968 acc@5 0.9832
[2022-06-11 10:57:55] __main__ INFO: Elapsed 0.91
[2022-06-11 10:57:55] __main__ INFO: Train 6 1950
[2022-06-11 10:57:59] __main__ INFO: Epoch 6 Step 100/390 lr 0.100000 loss 0.4442 (0.3665) acc@1 0.8359 (0.8715) acc@5 1.0000 (0.9958)
[2022-06-11 10:58:03] __main__ INFO: Epoch 6 Step 200/390 lr 0.100000 loss 0.2820 (0.3629) acc@1 0.8828 (0.8736) acc@5 1.0000 (0.9957)
[2022-06-11 10:58:07] __main__ INFO: Epoch 6 Step 300/390 lr 0.100000 loss 0.2700 (0.3601) acc@1 0.8906 (0.8750) acc@5 0.9922 (0.9953)
[2022-06-11 10:58:10] __main__ INFO: Epoch 6 Step 390/390 lr 0.100000 loss 0.3269 (0.3596) acc@1 0.8984 (0.8749) acc@5 0.9922 (0.9954)
[2022-06-11 10:58:11] __main__ INFO: Elapsed 15.64
[2022-06-11 10:58:11] __main__ INFO: Val 6
[2022-06-11 10:58:12] __main__ INFO: Epoch 6 loss 0.5733 acc@1 0.8152 acc@5 0.9924
[2022-06-11 10:58:12] __main__ INFO: Elapsed 0.94
[2022-06-11 10:58:12] __main__ INFO: Train 7 2340
[2022-06-11 10:58:16] __main__ INFO: Epoch 7 Step 100/390 lr 0.100000 loss 0.4802 (0.3335) acc@1 0.8281 (0.8825) acc@5 0.9922 (0.9968)
[2022-06-11 10:58:19] __main__ INFO: Epoch 7 Step 200/390 lr 0.100000 loss 0.2045 (0.3386) acc@1 0.9531 (0.8829) acc@5 0.9922 (0.9961)
[2022-06-11 10:58:23] __main__ INFO: Epoch 7 Step 300/390 lr 0.100000 loss 0.4583 (0.3393) acc@1 0.8438 (0.8824) acc@5 1.0000 (0.9961)
[2022-06-11 10:58:27] __main__ INFO: Epoch 7 Step 390/390 lr 0.100000 loss 0.2662 (0.3389) acc@1 0.9219 (0.8823) acc@5 0.9922 (0.9962)
[2022-06-11 10:58:27] __main__ INFO: Elapsed 15.52
[2022-06-11 10:58:27] __main__ INFO: Val 7
[2022-06-11 10:58:28] __main__ INFO: Epoch 7 loss 0.5119 acc@1 0.8397 acc@5 0.9914
[2022-06-11 10:58:28] __main__ INFO: Elapsed 0.98
[2022-06-11 10:58:28] __main__ INFO: Train 8 2730
[2022-06-11 10:58:32] __main__ INFO: Epoch 8 Step 100/390 lr 0.100000 loss 0.4452 (0.3094) acc@1 0.8281 (0.8938) acc@5 0.9922 (0.9968)
[2022-06-11 10:58:36] __main__ INFO: Epoch 8 Step 200/390 lr 0.100000 loss 0.2973 (0.3230) acc@1 0.8984 (0.8889) acc@5 1.0000 (0.9962)
[2022-06-11 10:58:40] __main__ INFO: Epoch 8 Step 300/390 lr 0.100000 loss 0.3904 (0.3244) acc@1 0.8750 (0.8890) acc@5 0.9922 (0.9961)
[2022-06-11 10:58:43] __main__ INFO: Epoch 8 Step 390/390 lr 0.100000 loss 0.3229 (0.3242) acc@1 0.9062 (0.8897) acc@5 0.9844 (0.9964)
[2022-06-11 10:58:44] __main__ INFO: Elapsed 15.54
[2022-06-11 10:58:44] __main__ INFO: Val 8
[2022-06-11 10:58:45] __main__ INFO: Epoch 8 loss 0.4766 acc@1 0.8468 acc@5 0.9921
[2022-06-11 10:58:45] __main__ INFO: Elapsed 0.95
[2022-06-11 10:58:45] __main__ INFO: Train 9 3120
[2022-06-11 10:58:49] __main__ INFO: Epoch 9 Step 100/390 lr 0.100000 loss 0.4175 (0.3099) acc@1 0.8828 (0.8935) acc@5 0.9844 (0.9970)
[2022-06-11 10:58:53] __main__ INFO: Epoch 9 Step 200/390 lr 0.100000 loss 0.2971 (0.3112) acc@1 0.9141 (0.8927) acc@5 1.0000 (0.9972)
[2022-06-11 10:58:57] __main__ INFO: Epoch 9 Step 300/390 lr 0.100000 loss 0.3992 (0.3152) acc@1 0.8672 (0.8915) acc@5 1.0000 (0.9968)
[2022-06-11 10:59:00] __main__ INFO: Epoch 9 Step 390/390 lr 0.100000 loss 0.2723 (0.3143) acc@1 0.9141 (0.8917) acc@5 1.0000 (0.9968)
[2022-06-11 10:59:00] __main__ INFO: Elapsed 15.66
[2022-06-11 10:59:00] __main__ INFO: Val 9
[2022-06-11 10:59:01] __main__ INFO: Epoch 9 loss 0.4917 acc@1 0.8401 acc@5 0.9942
[2022-06-11 10:59:01] __main__ INFO: Elapsed 0.93
[2022-06-11 10:59:01] __main__ INFO: Train 10 3510
[2022-06-11 10:59:05] __main__ INFO: Epoch 10 Step 100/390 lr 0.100000 loss 0.3100 (0.2918) acc@1 0.8750 (0.8984) acc@5 1.0000 (0.9975)
[2022-06-11 10:59:09] __main__ INFO: Epoch 10 Step 200/390 lr 0.100000 loss 0.3057 (0.2993) acc@1 0.8984 (0.8969) acc@5 1.0000 (0.9969)
[2022-06-11 10:59:13] __main__ INFO: Epoch 10 Step 300/390 lr 0.100000 loss 0.2057 (0.2991) acc@1 0.9219 (0.8959) acc@5 1.0000 (0.9973)
[2022-06-11 10:59:17] __main__ INFO: Epoch 10 Step 390/390 lr 0.100000 loss 0.2768 (0.3041) acc@1 0.9062 (0.8941) acc@5 1.0000 (0.9972)
[2022-06-11 10:59:17] __main__ INFO: Elapsed 15.70
[2022-06-11 10:59:17] __main__ INFO: Val 10
[2022-06-11 10:59:18] __main__ INFO: Epoch 10 loss 0.6311 acc@1 0.8086 acc@5 0.9872
[2022-06-11 10:59:18] __main__ INFO: Elapsed 0.90
[2022-06-11 10:59:18] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00010.pth
[2022-06-11 11:07:48] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 11:07:48] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 11:07:52] __main__ INFO: MACs   : 112.57M
[2022-06-11 11:07:52] __main__ INFO: #params: 758.55K
[2022-06-11 11:07:52] __main__ INFO: Val 0
[2022-06-11 11:07:53] __main__ INFO: Epoch 0 loss 2.9075 acc@1 0.1334 acc@5 0.5883
[2022-06-11 11:07:53] __main__ INFO: Elapsed 1.05
[2022-06-11 11:07:53] __main__ INFO: Train 1 0
[2022-06-11 11:07:57] __main__ INFO: Epoch 1 Step 100/390 lr 0.100000 loss 0.6641 (1.1200) acc@1 0.7969 (0.6042) acc@5 0.9922 (0.9484)
[2022-06-11 11:08:01] __main__ INFO: Epoch 1 Step 200/390 lr 0.100000 loss 0.7145 (0.9269) acc@1 0.7656 (0.6759) acc@5 0.9922 (0.9649)
[2022-06-11 11:08:05] __main__ INFO: Epoch 1 Step 300/390 lr 0.100000 loss 0.6042 (0.8352) acc@1 0.7266 (0.7097) acc@5 1.0000 (0.9723)
[2022-06-11 11:08:09] __main__ INFO: Epoch 1 Step 390/390 lr 0.100000 loss 0.5422 (0.7822) acc@1 0.8125 (0.7281) acc@5 1.0000 (0.9762)
[2022-06-11 11:08:09] __main__ INFO: Elapsed 16.07
[2022-06-11 11:08:09] __main__ INFO: Val 1
[2022-06-11 11:08:10] __main__ INFO: Epoch 1 loss 0.7418 acc@1 0.7514 acc@5 0.9830
[2022-06-11 11:08:10] __main__ INFO: Elapsed 0.94
[2022-06-11 11:08:10] __main__ INFO: Train 2 390
[2022-06-11 11:08:14] __main__ INFO: Epoch 2 Step 100/390 lr 0.100000 loss 0.7173 (0.5433) acc@1 0.7422 (0.8114) acc@5 0.9922 (0.9912)
[2022-06-11 11:08:18] __main__ INFO: Epoch 2 Step 200/390 lr 0.100000 loss 0.4550 (0.5308) acc@1 0.8594 (0.8161) acc@5 0.9922 (0.9907)
[2022-06-11 11:08:22] __main__ INFO: Epoch 2 Step 300/390 lr 0.100000 loss 0.3489 (0.5249) acc@1 0.9062 (0.8193) acc@5 0.9922 (0.9913)
[2022-06-11 11:08:25] __main__ INFO: Epoch 2 Step 390/390 lr 0.100000 loss 0.4092 (0.5187) acc@1 0.8203 (0.8216) acc@5 1.0000 (0.9912)
[2022-06-11 11:08:26] __main__ INFO: Elapsed 15.71
[2022-06-11 11:08:26] __main__ INFO: Val 2
[2022-06-11 11:08:27] __main__ INFO: Epoch 2 loss 0.7899 acc@1 0.7582 acc@5 0.9773
[2022-06-11 11:08:27] __main__ INFO: Elapsed 0.93
[2022-06-11 11:08:27] __main__ INFO: Train 3 780
[2022-06-11 11:08:31] __main__ INFO: Epoch 3 Step 100/390 lr 0.100000 loss 0.3938 (0.4555) acc@1 0.8594 (0.8454) acc@5 0.9922 (0.9943)
[2022-06-11 11:08:35] __main__ INFO: Epoch 3 Step 200/390 lr 0.100000 loss 0.4942 (0.4565) acc@1 0.8281 (0.8434) acc@5 0.9922 (0.9936)
[2022-06-11 11:08:39] __main__ INFO: Epoch 3 Step 300/390 lr 0.100000 loss 0.6109 (0.4499) acc@1 0.7578 (0.8452) acc@5 0.9844 (0.9939)
[2022-06-11 11:08:42] __main__ INFO: Epoch 3 Step 390/390 lr 0.100000 loss 0.4959 (0.4518) acc@1 0.8281 (0.8447) acc@5 1.0000 (0.9939)
[2022-06-11 11:08:42] __main__ INFO: Elapsed 15.95
[2022-06-11 11:08:42] __main__ INFO: Val 3
[2022-06-11 11:08:43] __main__ INFO: Epoch 3 loss 0.6889 acc@1 0.7752 acc@5 0.9861
[2022-06-11 11:08:43] __main__ INFO: Elapsed 0.97
[2022-06-11 11:08:43] __main__ INFO: Train 4 1170
[2022-06-11 11:08:48] __main__ INFO: Epoch 4 Step 100/390 lr 0.100000 loss 0.4060 (0.4074) acc@1 0.8750 (0.8597) acc@5 0.9922 (0.9946)
[2022-06-11 11:08:52] __main__ INFO: Epoch 4 Step 200/390 lr 0.100000 loss 0.4291 (0.4097) acc@1 0.8516 (0.8597) acc@5 1.0000 (0.9946)
[2022-06-11 11:08:55] __main__ INFO: Epoch 4 Step 300/390 lr 0.100000 loss 0.4992 (0.4119) acc@1 0.8438 (0.8579) acc@5 0.9922 (0.9945)
[2022-06-11 11:08:59] __main__ INFO: Epoch 4 Step 390/390 lr 0.100000 loss 0.3074 (0.4104) acc@1 0.8984 (0.8586) acc@5 0.9922 (0.9946)
[2022-06-11 11:08:59] __main__ INFO: Elapsed 15.66
[2022-06-11 11:08:59] __main__ INFO: Val 4
[2022-06-11 11:09:00] __main__ INFO: Epoch 4 loss 0.5217 acc@1 0.8262 acc@5 0.9915
[2022-06-11 11:09:00] __main__ INFO: Elapsed 0.95
[2022-06-11 11:09:00] __main__ INFO: Train 5 1560
[2022-06-11 11:09:04] __main__ INFO: Epoch 5 Step 100/390 lr 0.100000 loss 0.4267 (0.3737) acc@1 0.8828 (0.8723) acc@5 0.9922 (0.9951)
[2022-06-11 11:09:08] __main__ INFO: Epoch 5 Step 200/390 lr 0.100000 loss 0.2756 (0.3737) acc@1 0.9375 (0.8712) acc@5 0.9922 (0.9953)
[2022-06-11 11:09:12] __main__ INFO: Epoch 5 Step 300/390 lr 0.100000 loss 0.2928 (0.3770) acc@1 0.8828 (0.8699) acc@5 0.9922 (0.9952)
[2022-06-11 11:09:16] __main__ INFO: Epoch 5 Step 390/390 lr 0.100000 loss 0.4317 (0.3776) acc@1 0.8516 (0.8698) acc@5 0.9922 (0.9949)
[2022-06-11 11:09:16] __main__ INFO: Elapsed 15.63
[2022-06-11 11:09:16] __main__ INFO: Val 5
[2022-06-11 11:09:17] __main__ INFO: Epoch 5 loss 0.5635 acc@1 0.8131 acc@5 0.9898
[2022-06-11 11:09:17] __main__ INFO: Elapsed 0.97
[2022-06-11 11:09:17] __main__ INFO: Train 6 1950
[2022-06-11 11:09:21] __main__ INFO: Epoch 6 Step 100/390 lr 0.100000 loss 0.4078 (0.3572) acc@1 0.8281 (0.8753) acc@5 1.0000 (0.9962)
[2022-06-11 11:09:25] __main__ INFO: Epoch 6 Step 200/390 lr 0.100000 loss 0.2983 (0.3572) acc@1 0.8516 (0.8752) acc@5 1.0000 (0.9962)
[2022-06-11 11:09:29] __main__ INFO: Epoch 6 Step 300/390 lr 0.100000 loss 0.3187 (0.3543) acc@1 0.8672 (0.8772) acc@5 1.0000 (0.9963)
[2022-06-11 11:09:32] __main__ INFO: Epoch 6 Step 390/390 lr 0.100000 loss 0.3353 (0.3550) acc@1 0.8828 (0.8765) acc@5 0.9922 (0.9961)
[2022-06-11 11:09:32] __main__ INFO: Elapsed 15.75
[2022-06-11 11:09:32] __main__ INFO: Val 6
[2022-06-11 11:09:33] __main__ INFO: Epoch 6 loss 0.5299 acc@1 0.8240 acc@5 0.9914
[2022-06-11 11:09:33] __main__ INFO: Elapsed 0.95
[2022-06-11 11:09:33] __main__ INFO: Train 7 2340
[2022-06-11 11:09:38] __main__ INFO: Epoch 7 Step 100/390 lr 0.100000 loss 0.4683 (0.3395) acc@1 0.8125 (0.8820) acc@5 1.0000 (0.9964)
[2022-06-11 11:09:42] __main__ INFO: Epoch 7 Step 200/390 lr 0.100000 loss 0.2793 (0.3389) acc@1 0.9141 (0.8820) acc@5 0.9922 (0.9962)
[2022-06-11 11:09:46] __main__ INFO: Epoch 7 Step 300/390 lr 0.100000 loss 0.4390 (0.3415) acc@1 0.8672 (0.8807) acc@5 0.9922 (0.9961)
[2022-06-11 11:09:49] __main__ INFO: Epoch 7 Step 390/390 lr 0.100000 loss 0.2575 (0.3419) acc@1 0.9062 (0.8807) acc@5 1.0000 (0.9961)
[2022-06-11 11:09:49] __main__ INFO: Elapsed 15.94
[2022-06-11 11:09:49] __main__ INFO: Val 7
[2022-06-11 11:09:50] __main__ INFO: Epoch 7 loss 0.5152 acc@1 0.8341 acc@5 0.9914
[2022-06-11 11:09:50] __main__ INFO: Elapsed 0.93
[2022-06-11 11:09:50] __main__ INFO: Train 8 2730
[2022-06-11 11:09:54] __main__ INFO: Epoch 8 Step 100/390 lr 0.100000 loss 0.4600 (0.3088) acc@1 0.8203 (0.8924) acc@5 0.9844 (0.9974)
[2022-06-11 11:09:58] __main__ INFO: Epoch 8 Step 200/390 lr 0.100000 loss 0.3304 (0.3197) acc@1 0.8672 (0.8892) acc@5 1.0000 (0.9969)
[2022-06-11 11:10:02] __main__ INFO: Epoch 8 Step 300/390 lr 0.100000 loss 0.3306 (0.3223) acc@1 0.8672 (0.8882) acc@5 1.0000 (0.9967)
[2022-06-11 11:10:06] __main__ INFO: Epoch 8 Step 390/390 lr 0.100000 loss 0.3190 (0.3219) acc@1 0.8984 (0.8885) acc@5 0.9844 (0.9968)
[2022-06-11 11:10:06] __main__ INFO: Elapsed 15.66
[2022-06-11 11:10:06] __main__ INFO: Val 8
[2022-06-11 11:10:07] __main__ INFO: Epoch 8 loss 0.4809 acc@1 0.8474 acc@5 0.9927
[2022-06-11 11:10:07] __main__ INFO: Elapsed 0.94
[2022-06-11 11:10:07] __main__ INFO: Train 9 3120
[2022-06-11 11:10:11] __main__ INFO: Epoch 9 Step 100/390 lr 0.100000 loss 0.3982 (0.3073) acc@1 0.8672 (0.8923) acc@5 0.9922 (0.9978)
[2022-06-11 11:10:15] __main__ INFO: Epoch 9 Step 200/390 lr 0.100000 loss 0.2549 (0.3071) acc@1 0.9375 (0.8917) acc@5 1.0000 (0.9977)
[2022-06-11 11:10:19] __main__ INFO: Epoch 9 Step 300/390 lr 0.100000 loss 0.3600 (0.3092) acc@1 0.8828 (0.8909) acc@5 0.9844 (0.9973)
[2022-06-11 11:10:22] __main__ INFO: Epoch 9 Step 390/390 lr 0.100000 loss 0.2504 (0.3076) acc@1 0.9453 (0.8920) acc@5 0.9922 (0.9971)
[2022-06-11 11:10:22] __main__ INFO: Elapsed 15.57
[2022-06-11 11:10:22] __main__ INFO: Val 9
[2022-06-11 11:10:23] __main__ INFO: Epoch 9 loss 0.4815 acc@1 0.8467 acc@5 0.9951
[2022-06-11 11:10:23] __main__ INFO: Elapsed 0.92
[2022-06-11 11:10:23] __main__ INFO: Train 10 3510
[2022-06-11 11:10:28] __main__ INFO: Epoch 10 Step 100/390 lr 0.100000 loss 0.3211 (0.2920) acc@1 0.8750 (0.8979) acc@5 1.0000 (0.9975)
[2022-06-11 11:10:31] __main__ INFO: Epoch 10 Step 200/390 lr 0.100000 loss 0.3215 (0.2973) acc@1 0.8906 (0.8980) acc@5 1.0000 (0.9969)
[2022-06-11 11:10:35] __main__ INFO: Epoch 10 Step 300/390 lr 0.100000 loss 0.1521 (0.2968) acc@1 0.9609 (0.8976) acc@5 1.0000 (0.9972)
[2022-06-11 11:10:39] __main__ INFO: Epoch 10 Step 390/390 lr 0.100000 loss 0.2432 (0.3031) acc@1 0.9062 (0.8956) acc@5 1.0000 (0.9971)
[2022-06-11 11:10:39] __main__ INFO: Elapsed 15.83
[2022-06-11 11:10:39] __main__ INFO: Val 10
[2022-06-11 11:10:40] __main__ INFO: Epoch 10 loss 0.4984 acc@1 0.8449 acc@5 0.9928
[2022-06-11 11:10:40] __main__ INFO: Elapsed 0.89
[2022-06-11 11:10:40] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00010.pth
[2022-06-11 11:11:08] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 11:11:08] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 11:11:12] __main__ INFO: MACs   : 112.57M
[2022-06-11 11:11:12] __main__ INFO: #params: 758.55K
[2022-06-11 11:11:12] __main__ INFO: Val 0
[2022-06-11 11:11:13] __main__ INFO: Epoch 0 loss 2.9075 acc@1 0.1334 acc@5 0.5883
[2022-06-11 11:11:13] __main__ INFO: Elapsed 0.98
[2022-06-11 11:11:13] __main__ INFO: Train 1 0
[2022-06-11 11:11:17] __main__ INFO: Epoch 1 Step 100/390 lr 0.100000 loss 0.6881 (1.1215) acc@1 0.7891 (0.6053) acc@5 1.0000 (0.9480)
[2022-06-11 11:11:21] __main__ INFO: Epoch 1 Step 200/390 lr 0.100000 loss 0.6868 (0.9272) acc@1 0.7422 (0.6771) acc@5 0.9922 (0.9648)
[2022-06-11 11:11:25] __main__ INFO: Epoch 1 Step 300/390 lr 0.100000 loss 0.6476 (0.8340) acc@1 0.7344 (0.7090) acc@5 1.0000 (0.9721)
[2022-06-11 11:11:29] __main__ INFO: Epoch 1 Step 390/390 lr 0.100000 loss 0.5383 (0.7801) acc@1 0.8203 (0.7280) acc@5 0.9922 (0.9762)
[2022-06-11 11:11:29] __main__ INFO: Elapsed 15.72
[2022-06-11 11:11:29] __main__ INFO: Val 1
[2022-06-11 11:11:30] __main__ INFO: Epoch 1 loss 0.8620 acc@1 0.7167 acc@5 0.9823
[2022-06-11 11:11:30] __main__ INFO: Elapsed 0.98
[2022-06-11 11:11:30] __main__ INFO: Train 2 390
[2022-06-11 11:11:34] __main__ INFO: Epoch 2 Step 100/390 lr 0.100000 loss 0.6999 (0.5464) acc@1 0.7656 (0.8143) acc@5 0.9844 (0.9912)
[2022-06-11 11:11:38] __main__ INFO: Epoch 2 Step 200/390 lr 0.100000 loss 0.5270 (0.5310) acc@1 0.8281 (0.8179) acc@5 0.9922 (0.9910)
[2022-06-11 11:11:42] __main__ INFO: Epoch 2 Step 300/390 lr 0.100000 loss 0.3923 (0.5229) acc@1 0.8906 (0.8205) acc@5 1.0000 (0.9913)
[2022-06-11 11:11:45] __main__ INFO: Epoch 2 Step 390/390 lr 0.100000 loss 0.4069 (0.5205) acc@1 0.8359 (0.8212) acc@5 1.0000 (0.9913)
[2022-06-11 11:11:46] __main__ INFO: Elapsed 15.89
[2022-06-11 11:11:46] __main__ INFO: Val 2
[2022-06-11 11:11:46] __main__ INFO: Epoch 2 loss 0.7687 acc@1 0.7622 acc@5 0.9843
[2022-06-11 11:11:46] __main__ INFO: Elapsed 0.92
[2022-06-11 11:11:46] __main__ INFO: Train 3 780
[2022-06-11 11:11:51] __main__ INFO: Epoch 3 Step 100/390 lr 0.100000 loss 0.4542 (0.4567) acc@1 0.8438 (0.8429) acc@5 0.9844 (0.9937)
[2022-06-11 11:11:55] __main__ INFO: Epoch 3 Step 200/390 lr 0.100000 loss 0.5747 (0.4598) acc@1 0.7891 (0.8427) acc@5 0.9922 (0.9930)
[2022-06-11 11:11:58] __main__ INFO: Epoch 3 Step 300/390 lr 0.100000 loss 0.5849 (0.4531) acc@1 0.7734 (0.8438) acc@5 0.9922 (0.9933)
[2022-06-11 11:12:02] __main__ INFO: Epoch 3 Step 390/390 lr 0.100000 loss 0.4755 (0.4548) acc@1 0.8203 (0.8441) acc@5 1.0000 (0.9930)
[2022-06-11 11:12:02] __main__ INFO: Elapsed 15.75
[2022-06-11 11:12:02] __main__ INFO: Val 3
[2022-06-11 11:12:03] __main__ INFO: Epoch 3 loss 0.5518 acc@1 0.8180 acc@5 0.9890
[2022-06-11 11:12:03] __main__ INFO: Elapsed 0.94
[2022-06-11 11:12:03] __main__ INFO: Train 4 1170
[2022-06-11 11:12:07] __main__ INFO: Epoch 4 Step 100/390 lr 0.100000 loss 0.3491 (0.4053) acc@1 0.9141 (0.8606) acc@5 0.9922 (0.9957)
[2022-06-11 11:12:11] __main__ INFO: Epoch 4 Step 200/390 lr 0.100000 loss 0.4388 (0.4076) acc@1 0.8672 (0.8606) acc@5 1.0000 (0.9954)
[2022-06-11 11:12:15] __main__ INFO: Epoch 4 Step 300/390 lr 0.100000 loss 0.4700 (0.4125) acc@1 0.8672 (0.8586) acc@5 0.9922 (0.9949)
[2022-06-11 11:12:19] __main__ INFO: Epoch 4 Step 390/390 lr 0.100000 loss 0.3007 (0.4121) acc@1 0.8984 (0.8575) acc@5 0.9922 (0.9947)
[2022-06-11 11:12:19] __main__ INFO: Elapsed 15.85
[2022-06-11 11:12:19] __main__ INFO: Val 4
[2022-06-11 11:12:20] __main__ INFO: Epoch 4 loss 0.6579 acc@1 0.7870 acc@5 0.9881
[2022-06-11 11:12:20] __main__ INFO: Elapsed 0.96
[2022-06-11 11:12:20] __main__ INFO: Train 5 1560
[2022-06-11 11:12:24] __main__ INFO: Epoch 5 Step 100/390 lr 0.100000 loss 0.3672 (0.3731) acc@1 0.8828 (0.8707) acc@5 1.0000 (0.9949)
[2022-06-11 11:12:28] __main__ INFO: Epoch 5 Step 200/390 lr 0.100000 loss 0.3230 (0.3762) acc@1 0.8984 (0.8693) acc@5 0.9922 (0.9952)
[2022-06-11 11:12:32] __main__ INFO: Epoch 5 Step 300/390 lr 0.100000 loss 0.2219 (0.3773) acc@1 0.9453 (0.8705) acc@5 0.9922 (0.9951)
[2022-06-11 11:12:35] __main__ INFO: Epoch 5 Step 390/390 lr 0.100000 loss 0.3911 (0.3806) acc@1 0.8984 (0.8685) acc@5 0.9844 (0.9951)
[2022-06-11 11:12:36] __main__ INFO: Elapsed 15.59
[2022-06-11 11:12:36] __main__ INFO: Val 5
[2022-06-11 11:12:36] __main__ INFO: Epoch 5 loss 0.6748 acc@1 0.7864 acc@5 0.9885
[2022-06-11 11:12:36] __main__ INFO: Elapsed 0.94
[2022-06-11 11:12:36] __main__ INFO: Train 6 1950
[2022-06-11 11:12:41] __main__ INFO: Epoch 6 Step 100/390 lr 0.100000 loss 0.4149 (0.3637) acc@1 0.8750 (0.8734) acc@5 1.0000 (0.9965)
[2022-06-11 11:12:45] __main__ INFO: Epoch 6 Step 200/390 lr 0.100000 loss 0.2633 (0.3619) acc@1 0.9297 (0.8749) acc@5 1.0000 (0.9963)
[2022-06-11 11:12:49] __main__ INFO: Epoch 6 Step 300/390 lr 0.100000 loss 0.3190 (0.3587) acc@1 0.8750 (0.8758) acc@5 1.0000 (0.9960)
[2022-06-11 11:12:52] __main__ INFO: Epoch 6 Step 390/390 lr 0.100000 loss 0.2982 (0.3598) acc@1 0.8828 (0.8750) acc@5 1.0000 (0.9960)
[2022-06-11 11:12:52] __main__ INFO: Elapsed 15.81
[2022-06-11 11:12:52] __main__ INFO: Val 6
[2022-06-11 11:12:53] __main__ INFO: Epoch 6 loss 0.4413 acc@1 0.8534 acc@5 0.9935
[2022-06-11 11:12:53] __main__ INFO: Elapsed 0.95
[2022-06-11 11:12:53] __main__ INFO: Train 7 2340
[2022-06-11 11:12:58] __main__ INFO: Epoch 7 Step 100/390 lr 0.100000 loss 0.4156 (0.3303) acc@1 0.8594 (0.8850) acc@5 1.0000 (0.9962)
[2022-06-11 11:13:02] __main__ INFO: Epoch 7 Step 200/390 lr 0.100000 loss 0.2663 (0.3403) acc@1 0.9141 (0.8829) acc@5 0.9922 (0.9955)
[2022-06-11 11:13:06] __main__ INFO: Epoch 7 Step 300/390 lr 0.100000 loss 0.3548 (0.3376) acc@1 0.8906 (0.8838) acc@5 1.0000 (0.9959)
[2022-06-11 11:13:09] __main__ INFO: Epoch 7 Step 390/390 lr 0.100000 loss 0.2980 (0.3371) acc@1 0.8906 (0.8839) acc@5 0.9922 (0.9958)
[2022-06-11 11:13:09] __main__ INFO: Elapsed 15.85
[2022-06-11 11:13:09] __main__ INFO: Val 7
[2022-06-11 11:13:10] __main__ INFO: Epoch 7 loss 0.5129 acc@1 0.8314 acc@5 0.9914
[2022-06-11 11:13:10] __main__ INFO: Elapsed 0.99
[2022-06-11 11:13:10] __main__ INFO: Train 8 2730
[2022-06-11 11:13:14] __main__ INFO: Epoch 8 Step 100/390 lr 0.100000 loss 0.4732 (0.3177) acc@1 0.8359 (0.8915) acc@5 0.9922 (0.9962)
[2022-06-11 11:13:18] __main__ INFO: Epoch 8 Step 200/390 lr 0.100000 loss 0.3651 (0.3274) acc@1 0.8359 (0.8882) acc@5 1.0000 (0.9962)
[2022-06-11 11:13:22] __main__ INFO: Epoch 8 Step 300/390 lr 0.100000 loss 0.3525 (0.3278) acc@1 0.8828 (0.8876) acc@5 0.9922 (0.9965)
[2022-06-11 11:13:26] __main__ INFO: Epoch 8 Step 390/390 lr 0.100000 loss 0.3310 (0.3252) acc@1 0.8906 (0.8879) acc@5 0.9922 (0.9966)
[2022-06-11 11:13:26] __main__ INFO: Elapsed 15.58
[2022-06-11 11:13:26] __main__ INFO: Val 8
[2022-06-11 11:13:27] __main__ INFO: Epoch 8 loss 0.4486 acc@1 0.8552 acc@5 0.9947
[2022-06-11 11:13:27] __main__ INFO: Elapsed 0.98
[2022-06-11 11:13:27] __main__ INFO: Train 9 3120
[2022-06-11 11:13:31] __main__ INFO: Epoch 9 Step 100/390 lr 0.100000 loss 0.3911 (0.3088) acc@1 0.8672 (0.8948) acc@5 0.9844 (0.9970)
[2022-06-11 11:13:35] __main__ INFO: Epoch 9 Step 200/390 lr 0.100000 loss 0.2668 (0.3079) acc@1 0.9062 (0.8941) acc@5 1.0000 (0.9968)
[2022-06-11 11:13:39] __main__ INFO: Epoch 9 Step 300/390 lr 0.100000 loss 0.3345 (0.3118) acc@1 0.8984 (0.8935) acc@5 0.9922 (0.9967)
[2022-06-11 11:13:43] __main__ INFO: Epoch 9 Step 390/390 lr 0.100000 loss 0.2954 (0.3126) acc@1 0.9297 (0.8929) acc@5 1.0000 (0.9965)
[2022-06-11 11:13:43] __main__ INFO: Elapsed 16.03
[2022-06-11 11:13:43] __main__ INFO: Val 9
[2022-06-11 11:13:44] __main__ INFO: Epoch 9 loss 0.4958 acc@1 0.8402 acc@5 0.9917
[2022-06-11 11:13:44] __main__ INFO: Elapsed 1.00
[2022-06-11 11:13:44] __main__ INFO: Train 10 3510
[2022-06-11 11:13:48] __main__ INFO: Epoch 10 Step 100/390 lr 0.100000 loss 0.2592 (0.2875) acc@1 0.9297 (0.9017) acc@5 1.0000 (0.9969)
[2022-06-11 11:13:52] __main__ INFO: Epoch 10 Step 200/390 lr 0.100000 loss 0.3328 (0.2966) acc@1 0.8906 (0.8982) acc@5 1.0000 (0.9970)
[2022-06-11 11:13:56] __main__ INFO: Epoch 10 Step 300/390 lr 0.100000 loss 0.1916 (0.2963) acc@1 0.9375 (0.8990) acc@5 1.0000 (0.9973)
[2022-06-11 11:13:59] __main__ INFO: Epoch 10 Step 390/390 lr 0.100000 loss 0.2386 (0.3004) acc@1 0.8984 (0.8967) acc@5 1.0000 (0.9971)
[2022-06-11 11:13:59] __main__ INFO: Elapsed 15.75
[2022-06-11 11:13:59] __main__ INFO: Val 10
[2022-06-11 11:14:00] __main__ INFO: Epoch 10 loss 0.4900 acc@1 0.8455 acc@5 0.9924
[2022-06-11 11:14:00] __main__ INFO: Elapsed 0.93
[2022-06-11 11:14:00] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00010.pth
[2022-06-11 11:27:26] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: True
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 11:27:26] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 1
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 11:27:30] __main__ INFO: MACs   : 112.57M
[2022-06-11 11:27:30] __main__ INFO: #params: 758.55K
[2022-06-11 11:27:30] __main__ INFO: Val 0
[2022-06-11 11:27:31] __main__ INFO: Epoch 0 loss 2.9075 acc@1 0.1334 acc@5 0.5883
[2022-06-11 11:27:31] __main__ INFO: Elapsed 1.05
[2022-06-11 11:27:31] __main__ INFO: Train 1 0
[2022-06-11 11:27:35] __main__ INFO: Epoch 1 Step 100/390 lr 0.100000 loss 0.6703 (1.1210) acc@1 0.8047 (0.6047) acc@5 0.9844 (0.9472)
[2022-06-11 11:27:39] __main__ INFO: Epoch 1 Step 200/390 lr 0.100000 loss 0.7222 (0.9280) acc@1 0.7422 (0.6745) acc@5 0.9922 (0.9645)
[2022-06-11 11:27:43] __main__ INFO: Epoch 1 Step 300/390 lr 0.100000 loss 0.6236 (0.8361) acc@1 0.7344 (0.7081) acc@5 1.0000 (0.9722)
[2022-06-11 11:27:47] __main__ INFO: Epoch 1 Step 390/390 lr 0.100000 loss 0.5172 (0.7813) acc@1 0.8438 (0.7278) acc@5 0.9922 (0.9760)
[2022-06-11 11:27:47] __main__ INFO: Elapsed 15.92
[2022-06-11 11:27:47] __main__ INFO: Val 1
[2022-06-11 11:27:48] __main__ INFO: Epoch 1 loss 0.8147 acc@1 0.7362 acc@5 0.9789
[2022-06-11 11:27:48] __main__ INFO: Elapsed 0.89
[2022-06-11 11:27:48] __main__ INFO: Train 2 390
[2022-06-11 11:27:52] __main__ INFO: Epoch 2 Step 100/390 lr 0.100000 loss 0.6497 (0.5428) acc@1 0.7812 (0.8155) acc@5 0.9766 (0.9916)
[2022-06-11 11:27:56] __main__ INFO: Epoch 2 Step 200/390 lr 0.100000 loss 0.4312 (0.5311) acc@1 0.8516 (0.8182) acc@5 1.0000 (0.9912)
[2022-06-11 11:28:00] __main__ INFO: Epoch 2 Step 300/390 lr 0.100000 loss 0.3347 (0.5251) acc@1 0.8984 (0.8199) acc@5 0.9922 (0.9915)
[2022-06-11 11:28:03] __main__ INFO: Epoch 2 Step 390/390 lr 0.100000 loss 0.4081 (0.5213) acc@1 0.8438 (0.8206) acc@5 1.0000 (0.9917)
[2022-06-11 11:28:03] __main__ INFO: Elapsed 15.72
[2022-06-11 11:28:03] __main__ INFO: Val 2
[2022-06-11 11:28:04] __main__ INFO: Epoch 2 loss 0.6768 acc@1 0.7845 acc@5 0.9879
[2022-06-11 11:28:04] __main__ INFO: Elapsed 0.97
[2022-06-11 11:28:04] __main__ INFO: Train 3 780
[2022-06-11 11:28:08] __main__ INFO: Epoch 3 Step 100/390 lr 0.100000 loss 0.4239 (0.4556) acc@1 0.8594 (0.8435) acc@5 0.9844 (0.9933)
[2022-06-11 11:28:12] __main__ INFO: Epoch 3 Step 200/390 lr 0.100000 loss 0.4784 (0.4589) acc@1 0.8281 (0.8427) acc@5 0.9844 (0.9933)
[2022-06-11 11:28:16] __main__ INFO: Epoch 3 Step 300/390 lr 0.100000 loss 0.6112 (0.4546) acc@1 0.7891 (0.8441) acc@5 0.9844 (0.9933)
[2022-06-11 11:28:20] __main__ INFO: Epoch 3 Step 390/390 lr 0.100000 loss 0.4878 (0.4547) acc@1 0.8359 (0.8434) acc@5 1.0000 (0.9933)
[2022-06-11 11:28:20] __main__ INFO: Elapsed 15.48
[2022-06-11 11:28:20] __main__ INFO: Val 3
[2022-06-11 11:28:21] __main__ INFO: Epoch 3 loss 0.6242 acc@1 0.8021 acc@5 0.9880
[2022-06-11 11:28:21] __main__ INFO: Elapsed 0.91
[2022-06-11 11:28:21] __main__ INFO: Train 4 1170
[2022-06-11 11:28:25] __main__ INFO: Epoch 4 Step 100/390 lr 0.100000 loss 0.3409 (0.4014) acc@1 0.9141 (0.8616) acc@5 1.0000 (0.9958)
[2022-06-11 11:28:29] __main__ INFO: Epoch 4 Step 200/390 lr 0.100000 loss 0.3867 (0.4082) acc@1 0.8828 (0.8592) acc@5 1.0000 (0.9952)
[2022-06-11 11:28:33] __main__ INFO: Epoch 4 Step 300/390 lr 0.100000 loss 0.4383 (0.4112) acc@1 0.8906 (0.8586) acc@5 0.9922 (0.9948)
[2022-06-11 11:28:36] __main__ INFO: Epoch 4 Step 390/390 lr 0.100000 loss 0.3536 (0.4104) acc@1 0.8828 (0.8591) acc@5 0.9922 (0.9947)
[2022-06-11 11:28:36] __main__ INFO: Elapsed 15.52
[2022-06-11 11:28:36] __main__ INFO: Val 4
[2022-06-11 11:28:37] __main__ INFO: Epoch 4 loss 0.5941 acc@1 0.8077 acc@5 0.9905
[2022-06-11 11:28:37] __main__ INFO: Elapsed 0.95
[2022-06-11 11:28:37] __main__ INFO: Train 5 1560
[2022-06-11 11:28:41] __main__ INFO: Epoch 5 Step 100/390 lr 0.100000 loss 0.3732 (0.3712) acc@1 0.8516 (0.8715) acc@5 0.9922 (0.9959)
[2022-06-11 11:28:45] __main__ INFO: Epoch 5 Step 200/390 lr 0.100000 loss 0.3087 (0.3722) acc@1 0.9062 (0.8719) acc@5 0.9922 (0.9959)
[2022-06-11 11:28:49] __main__ INFO: Epoch 5 Step 300/390 lr 0.100000 loss 0.2765 (0.3759) acc@1 0.9219 (0.8697) acc@5 1.0000 (0.9958)
[2022-06-11 11:28:53] __main__ INFO: Epoch 5 Step 390/390 lr 0.100000 loss 0.4014 (0.3771) acc@1 0.8828 (0.8703) acc@5 0.9922 (0.9955)
[2022-06-11 11:28:53] __main__ INFO: Elapsed 16.21
[2022-06-11 11:28:53] __main__ INFO: Val 5
[2022-06-11 11:28:54] __main__ INFO: Epoch 5 loss 0.5608 acc@1 0.8170 acc@5 0.9902
[2022-06-11 11:28:54] __main__ INFO: Elapsed 0.92
[2022-06-11 11:28:54] __main__ INFO: Train 6 1950
[2022-06-11 11:28:59] __main__ INFO: Epoch 6 Step 100/390 lr 0.100000 loss 0.4154 (0.3587) acc@1 0.8828 (0.8772) acc@5 0.9844 (0.9955)
[2022-06-11 11:29:02] __main__ INFO: Epoch 6 Step 200/390 lr 0.100000 loss 0.2614 (0.3568) acc@1 0.8828 (0.8778) acc@5 1.0000 (0.9957)
[2022-06-11 11:29:06] __main__ INFO: Epoch 6 Step 300/390 lr 0.100000 loss 0.3376 (0.3562) acc@1 0.8828 (0.8774) acc@5 0.9922 (0.9958)
[2022-06-11 11:29:10] __main__ INFO: Epoch 6 Step 390/390 lr 0.100000 loss 0.3081 (0.3559) acc@1 0.8906 (0.8776) acc@5 0.9922 (0.9957)
[2022-06-11 11:29:10] __main__ INFO: Elapsed 15.84
[2022-06-11 11:29:10] __main__ INFO: Val 6
[2022-06-11 11:29:11] __main__ INFO: Epoch 6 loss 0.5310 acc@1 0.8204 acc@5 0.9908
[2022-06-11 11:29:11] __main__ INFO: Elapsed 0.92
[2022-06-11 11:29:11] __main__ INFO: Train 7 2340
[2022-06-11 11:29:15] __main__ INFO: Epoch 7 Step 100/390 lr 0.100000 loss 0.4543 (0.3390) acc@1 0.8203 (0.8825) acc@5 1.0000 (0.9962)
[2022-06-11 11:29:19] __main__ INFO: Epoch 7 Step 200/390 lr 0.100000 loss 0.2390 (0.3413) acc@1 0.9219 (0.8832) acc@5 0.9922 (0.9962)
[2022-06-11 11:29:23] __main__ INFO: Epoch 7 Step 300/390 lr 0.100000 loss 0.3725 (0.3419) acc@1 0.8750 (0.8827) acc@5 0.9922 (0.9964)
[2022-06-11 11:29:27] __main__ INFO: Epoch 7 Step 390/390 lr 0.100000 loss 0.2497 (0.3394) acc@1 0.9219 (0.8829) acc@5 0.9922 (0.9963)
[2022-06-11 11:29:27] __main__ INFO: Elapsed 15.82
[2022-06-11 11:29:27] __main__ INFO: Val 7
[2022-06-11 11:29:28] __main__ INFO: Epoch 7 loss 0.5983 acc@1 0.8205 acc@5 0.9866
[2022-06-11 11:29:28] __main__ INFO: Elapsed 0.91
[2022-06-11 11:29:28] __main__ INFO: Train 8 2730
[2022-06-11 11:29:32] __main__ INFO: Epoch 8 Step 100/390 lr 0.100000 loss 0.5077 (0.3133) acc@1 0.8203 (0.8886) acc@5 0.9922 (0.9966)
[2022-06-11 11:29:36] __main__ INFO: Epoch 8 Step 200/390 lr 0.100000 loss 0.3406 (0.3232) acc@1 0.8828 (0.8873) acc@5 0.9922 (0.9964)
[2022-06-11 11:29:40] __main__ INFO: Epoch 8 Step 300/390 lr 0.100000 loss 0.2572 (0.3243) acc@1 0.9219 (0.8881) acc@5 0.9922 (0.9964)
[2022-06-11 11:29:44] __main__ INFO: Epoch 8 Step 390/390 lr 0.100000 loss 0.3474 (0.3239) acc@1 0.9062 (0.8881) acc@5 0.9922 (0.9965)
[2022-06-11 11:29:44] __main__ INFO: Elapsed 15.80
[2022-06-11 11:29:44] __main__ INFO: Val 8
[2022-06-11 11:29:45] __main__ INFO: Epoch 8 loss 0.4387 acc@1 0.8590 acc@5 0.9939
[2022-06-11 11:29:45] __main__ INFO: Elapsed 0.93
[2022-06-11 11:29:45] __main__ INFO: Train 9 3120
[2022-06-11 11:29:49] __main__ INFO: Epoch 9 Step 100/390 lr 0.100000 loss 0.4169 (0.3055) acc@1 0.8516 (0.8938) acc@5 0.9844 (0.9975)
[2022-06-11 11:29:53] __main__ INFO: Epoch 9 Step 200/390 lr 0.100000 loss 0.2892 (0.3029) acc@1 0.9219 (0.8959) acc@5 1.0000 (0.9974)
[2022-06-11 11:29:57] __main__ INFO: Epoch 9 Step 300/390 lr 0.100000 loss 0.4639 (0.3071) acc@1 0.8359 (0.8943) acc@5 0.9922 (0.9972)
[2022-06-11 11:30:00] __main__ INFO: Epoch 9 Step 390/390 lr 0.100000 loss 0.2928 (0.3071) acc@1 0.9297 (0.8946) acc@5 1.0000 (0.9971)
[2022-06-11 11:30:00] __main__ INFO: Elapsed 15.70
[2022-06-11 11:30:00] __main__ INFO: Val 9
[2022-06-11 11:30:01] __main__ INFO: Epoch 9 loss 0.5721 acc@1 0.8158 acc@5 0.9908
[2022-06-11 11:30:01] __main__ INFO: Elapsed 0.95
[2022-06-11 11:30:01] __main__ INFO: Train 10 3510
[2022-06-11 11:30:05] __main__ INFO: Epoch 10 Step 100/390 lr 0.100000 loss 0.3427 (0.2947) acc@1 0.8906 (0.8985) acc@5 1.0000 (0.9972)
[2022-06-11 11:30:09] __main__ INFO: Epoch 10 Step 200/390 lr 0.100000 loss 0.3172 (0.2975) acc@1 0.8828 (0.8981) acc@5 1.0000 (0.9970)
[2022-06-11 11:30:13] __main__ INFO: Epoch 10 Step 300/390 lr 0.100000 loss 0.2174 (0.2962) acc@1 0.9375 (0.8975) acc@5 0.9922 (0.9971)
[2022-06-11 11:30:17] __main__ INFO: Epoch 10 Step 390/390 lr 0.100000 loss 0.2691 (0.3025) acc@1 0.9062 (0.8953) acc@5 1.0000 (0.9971)
[2022-06-11 11:30:17] __main__ INFO: Elapsed 15.39
[2022-06-11 11:30:17] __main__ INFO: Val 10
[2022-06-11 11:30:18] __main__ INFO: Epoch 10 loss 0.5443 acc@1 0.8378 acc@5 0.9919
[2022-06-11 11:30:18] __main__ INFO: Elapsed 0.92
[2022-06-11 11:30:18] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00010.pth
[2022-06-11 11:32:53] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: True
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 11:32:53] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 1
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 11:33:14] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: False
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 11:33:14] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 1
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 11:33:18] __main__ INFO: MACs   : 112.57M
[2022-06-11 11:33:18] __main__ INFO: #params: 758.55K
[2022-06-11 11:33:18] __main__ INFO: Val 0
[2022-06-11 11:33:19] __main__ INFO: Epoch 0 loss 2.9075 acc@1 0.1334 acc@5 0.5883
[2022-06-11 11:33:29] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: False
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 11:33:29] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 1
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 11:33:33] __main__ INFO: MACs   : 112.57M
[2022-06-11 11:33:33] __main__ INFO: #params: 758.55K
[2022-06-11 11:33:33] __main__ INFO: Val 0
[2022-06-11 11:33:34] __main__ INFO: Epoch 0 loss 2.9075 acc@1 0.1334 acc@5 0.5883
[2022-06-11 11:33:34] __main__ INFO: Elapsed 0.99
[2022-06-11 11:33:34] __main__ INFO: Train 1 0
[2022-06-11 11:33:39] __main__ INFO: Epoch 1 Step 100/390 lr 0.100000 loss 0.6751 (1.1220) acc@1 0.7812 (0.6027) acc@5 1.0000 (0.9478)
[2022-06-11 11:33:43] __main__ INFO: Epoch 1 Step 200/390 lr 0.100000 loss 0.7131 (0.9283) acc@1 0.7500 (0.6738) acc@5 0.9844 (0.9642)
[2022-06-11 11:33:46] __main__ INFO: Epoch 1 Step 300/390 lr 0.100000 loss 0.6397 (0.8345) acc@1 0.7422 (0.7076) acc@5 1.0000 (0.9721)
[2022-06-11 11:33:50] __main__ INFO: Epoch 1 Step 390/390 lr 0.100000 loss 0.5589 (0.7804) acc@1 0.8203 (0.7277) acc@5 0.9766 (0.9760)
[2022-06-11 11:33:50] __main__ INFO: Elapsed 15.75
[2022-06-11 11:33:50] __main__ INFO: Val 1
[2022-06-11 11:33:51] __main__ INFO: Epoch 1 loss 0.6942 acc@1 0.7607 acc@5 0.9840
[2022-06-11 11:33:51] __main__ INFO: Elapsed 0.90
[2022-06-11 11:33:51] __main__ INFO: Train 2 390
[2022-06-11 11:33:55] __main__ INFO: Epoch 2 Step 100/390 lr 0.100000 loss 0.6618 (0.5374) acc@1 0.7500 (0.8174) acc@5 0.9922 (0.9913)
[2022-06-11 11:33:59] __main__ INFO: Epoch 2 Step 200/390 lr 0.100000 loss 0.5240 (0.5284) acc@1 0.8203 (0.8186) acc@5 1.0000 (0.9910)
[2022-06-11 11:34:03] __main__ INFO: Epoch 2 Step 300/390 lr 0.100000 loss 0.3815 (0.5242) acc@1 0.8750 (0.8194) acc@5 0.9922 (0.9911)
[2022-06-11 11:34:06] __main__ INFO: Epoch 2 Step 390/390 lr 0.100000 loss 0.4191 (0.5212) acc@1 0.8359 (0.8203) acc@5 1.0000 (0.9912)
[2022-06-11 11:34:07] __main__ INFO: Elapsed 15.60
[2022-06-11 11:34:07] __main__ INFO: Val 2
[2022-06-11 11:34:07] __main__ INFO: Epoch 2 loss 0.7611 acc@1 0.7603 acc@5 0.9845
[2022-06-11 11:34:07] __main__ INFO: Elapsed 0.92
[2022-06-11 11:34:07] __main__ INFO: Train 3 780
[2022-06-11 11:34:11] __main__ INFO: Epoch 3 Step 100/390 lr 0.100000 loss 0.4705 (0.4533) acc@1 0.8281 (0.8444) acc@5 0.9922 (0.9933)
[2022-06-11 11:34:15] __main__ INFO: Epoch 3 Step 200/390 lr 0.100000 loss 0.5197 (0.4571) acc@1 0.7969 (0.8413) acc@5 0.9922 (0.9930)
[2022-06-11 11:34:19] __main__ INFO: Epoch 3 Step 300/390 lr 0.100000 loss 0.5835 (0.4506) acc@1 0.7969 (0.8434) acc@5 0.9922 (0.9934)
[2022-06-11 11:34:23] __main__ INFO: Epoch 3 Step 390/390 lr 0.100000 loss 0.4464 (0.4526) acc@1 0.8281 (0.8429) acc@5 1.0000 (0.9932)
[2022-06-11 11:34:23] __main__ INFO: Elapsed 15.44
[2022-06-11 11:34:23] __main__ INFO: Val 3
[2022-06-11 11:34:24] __main__ INFO: Epoch 3 loss 0.5718 acc@1 0.8130 acc@5 0.9891
[2022-06-11 11:34:24] __main__ INFO: Elapsed 0.92
[2022-06-11 11:34:24] __main__ INFO: Train 4 1170
[2022-06-11 11:34:28] __main__ INFO: Epoch 4 Step 100/390 lr 0.100000 loss 0.4051 (0.4032) acc@1 0.8750 (0.8591) acc@5 0.9844 (0.9955)
[2022-06-11 11:34:32] __main__ INFO: Epoch 4 Step 200/390 lr 0.100000 loss 0.4391 (0.4073) acc@1 0.8281 (0.8590) acc@5 1.0000 (0.9953)
[2022-06-11 11:34:36] __main__ INFO: Epoch 4 Step 300/390 lr 0.100000 loss 0.4881 (0.4133) acc@1 0.8203 (0.8578) acc@5 0.9922 (0.9946)
[2022-06-11 11:34:39] __main__ INFO: Epoch 4 Step 390/390 lr 0.100000 loss 0.3223 (0.4137) acc@1 0.8906 (0.8577) acc@5 1.0000 (0.9946)
[2022-06-11 11:34:39] __main__ INFO: Elapsed 15.49
[2022-06-11 11:34:39] __main__ INFO: Val 4
[2022-06-11 11:34:40] __main__ INFO: Epoch 4 loss 0.5389 acc@1 0.8208 acc@5 0.9917
[2022-06-11 11:34:40] __main__ INFO: Elapsed 0.90
[2022-06-11 11:34:40] __main__ INFO: Train 5 1560
[2022-06-11 11:34:44] __main__ INFO: Epoch 5 Step 100/390 lr 0.100000 loss 0.3861 (0.3711) acc@1 0.8594 (0.8691) acc@5 0.9922 (0.9951)
[2022-06-11 11:34:48] __main__ INFO: Epoch 5 Step 200/390 lr 0.100000 loss 0.2983 (0.3729) acc@1 0.9062 (0.8700) acc@5 0.9844 (0.9956)
[2022-06-11 11:34:52] __main__ INFO: Epoch 5 Step 300/390 lr 0.100000 loss 0.2727 (0.3737) acc@1 0.9062 (0.8699) acc@5 0.9922 (0.9954)
[2022-06-11 11:34:56] __main__ INFO: Epoch 5 Step 390/390 lr 0.100000 loss 0.3270 (0.3734) acc@1 0.8984 (0.8701) acc@5 0.9922 (0.9954)
[2022-06-11 11:34:56] __main__ INFO: Elapsed 15.50
[2022-06-11 11:34:56] __main__ INFO: Val 5
[2022-06-11 11:34:57] __main__ INFO: Epoch 5 loss 0.5079 acc@1 0.8316 acc@5 0.9918
[2022-06-11 11:34:57] __main__ INFO: Elapsed 0.92
[2022-06-11 11:34:57] __main__ INFO: Train 6 1950
[2022-06-11 11:35:01] __main__ INFO: Epoch 6 Step 100/390 lr 0.100000 loss 0.4298 (0.3571) acc@1 0.8594 (0.8766) acc@5 0.9922 (0.9960)
[2022-06-11 11:35:05] __main__ INFO: Epoch 6 Step 200/390 lr 0.100000 loss 0.2715 (0.3590) acc@1 0.9141 (0.8746) acc@5 1.0000 (0.9957)
[2022-06-11 11:35:09] __main__ INFO: Epoch 6 Step 300/390 lr 0.100000 loss 0.3308 (0.3558) acc@1 0.8594 (0.8752) acc@5 0.9922 (0.9957)
[2022-06-11 11:35:12] __main__ INFO: Epoch 6 Step 390/390 lr 0.100000 loss 0.4591 (0.3577) acc@1 0.8281 (0.8746) acc@5 0.9922 (0.9954)
[2022-06-11 11:35:12] __main__ INFO: Elapsed 15.70
[2022-06-11 11:35:12] __main__ INFO: Val 6
[2022-06-11 11:35:13] __main__ INFO: Epoch 6 loss 0.4822 acc@1 0.8396 acc@5 0.9937
[2022-06-11 11:35:13] __main__ INFO: Elapsed 0.96
[2022-06-11 11:35:13] __main__ INFO: Train 7 2340
[2022-06-11 11:35:17] __main__ INFO: Epoch 7 Step 100/390 lr 0.100000 loss 0.4135 (0.3379) acc@1 0.8594 (0.8837) acc@5 1.0000 (0.9966)
[2022-06-11 11:35:21] __main__ INFO: Epoch 7 Step 200/390 lr 0.100000 loss 0.2655 (0.3424) acc@1 0.8984 (0.8819) acc@5 0.9922 (0.9958)
[2022-06-11 11:35:25] __main__ INFO: Epoch 7 Step 300/390 lr 0.100000 loss 0.3765 (0.3425) acc@1 0.8828 (0.8811) acc@5 0.9922 (0.9959)
[2022-06-11 11:35:29] __main__ INFO: Epoch 7 Step 390/390 lr 0.100000 loss 0.2733 (0.3410) acc@1 0.8828 (0.8814) acc@5 1.0000 (0.9961)
[2022-06-11 11:35:29] __main__ INFO: Elapsed 15.47
[2022-06-11 11:35:29] __main__ INFO: Val 7
[2022-06-11 11:35:30] __main__ INFO: Epoch 7 loss 0.4863 acc@1 0.8432 acc@5 0.9916
[2022-06-11 11:35:30] __main__ INFO: Elapsed 0.95
[2022-06-11 11:35:30] __main__ INFO: Train 8 2730
[2022-06-11 11:35:34] __main__ INFO: Epoch 8 Step 100/390 lr 0.100000 loss 0.3991 (0.3068) acc@1 0.8125 (0.8939) acc@5 1.0000 (0.9972)
[2022-06-11 11:35:38] __main__ INFO: Epoch 8 Step 200/390 lr 0.100000 loss 0.3476 (0.3234) acc@1 0.8984 (0.8891) acc@5 1.0000 (0.9966)
[2022-06-11 11:35:42] __main__ INFO: Epoch 8 Step 300/390 lr 0.100000 loss 0.3212 (0.3239) acc@1 0.8906 (0.8890) acc@5 1.0000 (0.9967)
[2022-06-11 11:35:45] __main__ INFO: Epoch 8 Step 390/390 lr 0.100000 loss 0.2679 (0.3236) acc@1 0.9062 (0.8885) acc@5 0.9922 (0.9967)
[2022-06-11 11:35:45] __main__ INFO: Elapsed 15.67
[2022-06-11 11:35:45] __main__ INFO: Val 8
[2022-06-11 11:35:46] __main__ INFO: Epoch 8 loss 0.4820 acc@1 0.8479 acc@5 0.9936
[2022-06-11 11:35:46] __main__ INFO: Elapsed 0.97
[2022-06-11 11:35:46] __main__ INFO: Train 9 3120
[2022-06-11 11:35:50] __main__ INFO: Epoch 9 Step 100/390 lr 0.100000 loss 0.4427 (0.3077) acc@1 0.8828 (0.8927) acc@5 0.9844 (0.9970)
[2022-06-11 11:35:54] __main__ INFO: Epoch 9 Step 200/390 lr 0.100000 loss 0.2354 (0.3048) acc@1 0.9609 (0.8939) acc@5 1.0000 (0.9968)
[2022-06-11 11:35:58] __main__ INFO: Epoch 9 Step 300/390 lr 0.100000 loss 0.3996 (0.3074) acc@1 0.8438 (0.8931) acc@5 1.0000 (0.9967)
[2022-06-11 11:36:02] __main__ INFO: Epoch 9 Step 390/390 lr 0.100000 loss 0.3134 (0.3090) acc@1 0.9062 (0.8928) acc@5 1.0000 (0.9967)
[2022-06-11 11:36:02] __main__ INFO: Elapsed 15.58
[2022-06-11 11:36:02] __main__ INFO: Val 9
[2022-06-11 11:36:03] __main__ INFO: Epoch 9 loss 0.4614 acc@1 0.8524 acc@5 0.9929
[2022-06-11 11:36:03] __main__ INFO: Elapsed 0.91
[2022-06-11 11:36:03] __main__ INFO: Train 10 3510
[2022-06-11 11:36:07] __main__ INFO: Epoch 10 Step 100/390 lr 0.100000 loss 0.2864 (0.2892) acc@1 0.8984 (0.8998) acc@5 0.9922 (0.9968)
[2022-06-11 11:36:11] __main__ INFO: Epoch 10 Step 200/390 lr 0.100000 loss 0.3060 (0.3012) acc@1 0.8750 (0.8959) acc@5 1.0000 (0.9966)
[2022-06-11 11:36:15] __main__ INFO: Epoch 10 Step 300/390 lr 0.100000 loss 0.2197 (0.2992) acc@1 0.9062 (0.8962) acc@5 1.0000 (0.9969)
[2022-06-11 11:36:19] __main__ INFO: Epoch 10 Step 390/390 lr 0.100000 loss 0.2509 (0.3031) acc@1 0.8984 (0.8944) acc@5 1.0000 (0.9968)
[2022-06-11 11:36:19] __main__ INFO: Elapsed 15.77
[2022-06-11 11:36:19] __main__ INFO: Val 10
[2022-06-11 11:36:20] __main__ INFO: Epoch 10 loss 0.7019 acc@1 0.8042 acc@5 0.9815
[2022-06-11 11:36:20] __main__ INFO: Elapsed 0.97
[2022-06-11 11:36:20] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00010.pth
[2022-06-11 11:37:40] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: False
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 11:37:40] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 1
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 11:37:44] __main__ INFO: MACs   : 112.57M
[2022-06-11 11:37:44] __main__ INFO: #params: 758.55K
[2022-06-11 11:37:44] __main__ INFO: Val 0
[2022-06-11 11:37:45] __main__ INFO: Epoch 0 loss 2.9075 acc@1 0.1334 acc@5 0.5883
[2022-06-11 11:39:29] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: False
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 11:39:29] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 1
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 11:39:33] __main__ INFO: MACs   : 112.57M
[2022-06-11 11:39:33] __main__ INFO: #params: 758.55K
[2022-06-11 11:39:33] __main__ INFO: Val 0
[2022-06-11 11:39:34] __main__ INFO: Epoch 0 loss 2.9075 acc@1 0.1334 acc@5 0.5883
[2022-06-11 11:39:34] __main__ INFO: Elapsed 0.96
[2022-06-11 11:39:34] __main__ INFO: Train 1 0
[2022-06-11 11:39:38] __main__ INFO: Epoch 1 Step 100/390 lr 0.100000 loss 0.6560 (1.1208) acc@1 0.7969 (0.6029) acc@5 0.9922 (0.9484)
[2022-06-11 11:39:42] __main__ INFO: Epoch 1 Step 200/390 lr 0.100000 loss 0.6912 (0.9272) acc@1 0.7500 (0.6748) acc@5 0.9844 (0.9650)
[2022-06-11 11:39:47] __main__ INFO: Epoch 1 Step 300/390 lr 0.100000 loss 0.5721 (0.8329) acc@1 0.7969 (0.7089) acc@5 0.9922 (0.9728)
[2022-06-11 11:39:50] __main__ INFO: Epoch 1 Step 390/390 lr 0.100000 loss 0.5301 (0.7798) acc@1 0.8125 (0.7275) acc@5 0.9922 (0.9767)
[2022-06-11 11:39:50] __main__ INFO: Elapsed 16.30
[2022-06-11 11:39:50] __main__ INFO: Val 1
[2022-06-11 11:39:51] __main__ INFO: Epoch 1 loss 0.8266 acc@1 0.7217 acc@5 0.9845
[2022-06-11 11:39:51] __main__ INFO: Elapsed 0.87
[2022-06-11 11:39:51] __main__ INFO: Train 2 390
[2022-06-11 11:39:55] __main__ INFO: Epoch 2 Step 100/390 lr 0.100000 loss 0.6631 (0.5386) acc@1 0.7891 (0.8181) acc@5 0.9922 (0.9905)
[2022-06-11 11:39:59] __main__ INFO: Epoch 2 Step 200/390 lr 0.100000 loss 0.4213 (0.5260) acc@1 0.8516 (0.8202) acc@5 1.0000 (0.9910)
[2022-06-11 11:40:03] __main__ INFO: Epoch 2 Step 300/390 lr 0.100000 loss 0.4339 (0.5216) acc@1 0.8359 (0.8199) acc@5 0.9844 (0.9910)
[2022-06-11 11:40:06] __main__ INFO: Epoch 2 Step 390/390 lr 0.100000 loss 0.4238 (0.5171) acc@1 0.8594 (0.8215) acc@5 0.9922 (0.9908)
[2022-06-11 11:40:07] __main__ INFO: Elapsed 15.49
[2022-06-11 11:40:07] __main__ INFO: Val 2
[2022-06-11 11:40:07] __main__ INFO: Epoch 2 loss 0.7564 acc@1 0.7634 acc@5 0.9817
[2022-06-11 11:40:07] __main__ INFO: Elapsed 0.89
[2022-06-11 11:40:07] __main__ INFO: Train 3 780
[2022-06-11 11:40:12] __main__ INFO: Epoch 3 Step 100/390 lr 0.100000 loss 0.4271 (0.4522) acc@1 0.8438 (0.8474) acc@5 0.9922 (0.9947)
[2022-06-11 11:40:16] __main__ INFO: Epoch 3 Step 200/390 lr 0.100000 loss 0.4912 (0.4587) acc@1 0.8281 (0.8420) acc@5 1.0000 (0.9938)
[2022-06-11 11:40:19] __main__ INFO: Epoch 3 Step 300/390 lr 0.100000 loss 0.5944 (0.4537) acc@1 0.7812 (0.8430) acc@5 0.9922 (0.9939)
[2022-06-11 11:40:23] __main__ INFO: Epoch 3 Step 390/390 lr 0.100000 loss 0.4324 (0.4562) acc@1 0.8516 (0.8425) acc@5 0.9922 (0.9937)
[2022-06-11 11:40:23] __main__ INFO: Elapsed 15.68
[2022-06-11 11:40:23] __main__ INFO: Val 3
[2022-06-11 11:40:24] __main__ INFO: Epoch 3 loss 0.5576 acc@1 0.8108 acc@5 0.9906
[2022-06-11 11:40:24] __main__ INFO: Elapsed 0.92
[2022-06-11 11:40:24] __main__ INFO: Train 4 1170
[2022-06-11 11:40:28] __main__ INFO: Epoch 4 Step 100/390 lr 0.100000 loss 0.3417 (0.4003) acc@1 0.8750 (0.8628) acc@5 1.0000 (0.9959)
[2022-06-11 11:40:32] __main__ INFO: Epoch 4 Step 200/390 lr 0.100000 loss 0.4416 (0.4058) acc@1 0.8359 (0.8599) acc@5 1.0000 (0.9952)
[2022-06-11 11:40:36] __main__ INFO: Epoch 4 Step 300/390 lr 0.100000 loss 0.4186 (0.4105) acc@1 0.8516 (0.8592) acc@5 1.0000 (0.9947)
[2022-06-11 11:40:40] __main__ INFO: Epoch 4 Step 390/390 lr 0.100000 loss 0.3404 (0.4104) acc@1 0.8672 (0.8595) acc@5 0.9922 (0.9945)
[2022-06-11 11:40:40] __main__ INFO: Elapsed 15.61
[2022-06-11 11:40:40] __main__ INFO: Val 4
[2022-06-11 11:40:41] __main__ INFO: Epoch 4 loss 0.6213 acc@1 0.8007 acc@5 0.9880
[2022-06-11 11:40:41] __main__ INFO: Elapsed 0.94
[2022-06-11 11:40:41] __main__ INFO: Train 5 1560
[2022-06-11 11:40:45] __main__ INFO: Epoch 5 Step 100/390 lr 0.100000 loss 0.4079 (0.3747) acc@1 0.8359 (0.8704) acc@5 1.0000 (0.9960)
[2022-06-11 11:40:49] __main__ INFO: Epoch 5 Step 200/390 lr 0.100000 loss 0.2575 (0.3763) acc@1 0.9219 (0.8705) acc@5 0.9922 (0.9960)
[2022-06-11 11:40:53] __main__ INFO: Epoch 5 Step 300/390 lr 0.100000 loss 0.2716 (0.3793) acc@1 0.9219 (0.8702) acc@5 0.9922 (0.9957)
[2022-06-11 11:40:56] __main__ INFO: Epoch 5 Step 390/390 lr 0.100000 loss 0.3944 (0.3779) acc@1 0.8984 (0.8708) acc@5 0.9922 (0.9954)
[2022-06-11 11:40:56] __main__ INFO: Elapsed 15.65
[2022-06-11 11:40:56] __main__ INFO: Val 5
[2022-06-11 11:40:57] __main__ INFO: Epoch 5 loss 0.6340 acc@1 0.7983 acc@5 0.9882
[2022-06-11 11:40:57] __main__ INFO: Elapsed 0.89
[2022-06-11 11:40:57] __main__ INFO: Train 6 1950
[2022-06-11 11:41:01] __main__ INFO: Epoch 6 Step 100/390 lr 0.100000 loss 0.3681 (0.3611) acc@1 0.8984 (0.8734) acc@5 1.0000 (0.9956)
[2022-06-11 11:41:05] __main__ INFO: Epoch 6 Step 200/390 lr 0.100000 loss 0.2942 (0.3556) acc@1 0.8828 (0.8760) acc@5 1.0000 (0.9954)
[2022-06-11 11:41:09] __main__ INFO: Epoch 6 Step 300/390 lr 0.100000 loss 0.2841 (0.3535) acc@1 0.8828 (0.8767) acc@5 1.0000 (0.9957)
[2022-06-11 11:41:13] __main__ INFO: Epoch 6 Step 390/390 lr 0.100000 loss 0.3543 (0.3569) acc@1 0.8984 (0.8756) acc@5 1.0000 (0.9956)
[2022-06-11 11:41:13] __main__ INFO: Elapsed 15.90
[2022-06-11 11:41:13] __main__ INFO: Val 6
[2022-06-11 11:41:14] __main__ INFO: Epoch 6 loss 0.4887 acc@1 0.8397 acc@5 0.9926
[2022-06-11 11:41:14] __main__ INFO: Elapsed 0.98
[2022-06-11 11:41:14] __main__ INFO: Train 7 2340
[2022-06-11 11:41:18] __main__ INFO: Epoch 7 Step 100/390 lr 0.100000 loss 0.3626 (0.3323) acc@1 0.8750 (0.8843) acc@5 0.9922 (0.9970)
[2022-06-11 11:41:22] __main__ INFO: Epoch 7 Step 200/390 lr 0.100000 loss 0.2886 (0.3423) acc@1 0.8906 (0.8816) acc@5 0.9922 (0.9966)
[2022-06-11 11:41:26] __main__ INFO: Epoch 7 Step 300/390 lr 0.100000 loss 0.4444 (0.3420) acc@1 0.8516 (0.8821) acc@5 0.9766 (0.9964)
[2022-06-11 11:41:30] __main__ INFO: Epoch 7 Step 390/390 lr 0.100000 loss 0.2477 (0.3417) acc@1 0.9141 (0.8825) acc@5 1.0000 (0.9964)
[2022-06-11 11:41:30] __main__ INFO: Elapsed 15.95
[2022-06-11 11:41:30] __main__ INFO: Val 7
[2022-06-11 11:41:31] __main__ INFO: Epoch 7 loss 0.5324 acc@1 0.8294 acc@5 0.9917
[2022-06-11 11:41:31] __main__ INFO: Elapsed 0.95
[2022-06-11 11:41:31] __main__ INFO: Train 8 2730
[2022-06-11 11:41:35] __main__ INFO: Epoch 8 Step 100/390 lr 0.100000 loss 0.4255 (0.3164) acc@1 0.8672 (0.8939) acc@5 1.0000 (0.9970)
[2022-06-11 11:41:39] __main__ INFO: Epoch 8 Step 200/390 lr 0.100000 loss 0.3388 (0.3273) acc@1 0.8984 (0.8889) acc@5 1.0000 (0.9966)
[2022-06-11 11:41:43] __main__ INFO: Epoch 8 Step 300/390 lr 0.100000 loss 0.3674 (0.3291) acc@1 0.8516 (0.8886) acc@5 1.0000 (0.9966)
[2022-06-11 11:41:47] __main__ INFO: Epoch 8 Step 390/390 lr 0.100000 loss 0.2771 (0.3262) acc@1 0.9297 (0.8883) acc@5 0.9922 (0.9968)
[2022-06-11 11:41:47] __main__ INFO: Elapsed 15.87
[2022-06-11 11:41:47] __main__ INFO: Val 8
[2022-06-11 11:41:48] __main__ INFO: Epoch 8 loss 0.4212 acc@1 0.8588 acc@5 0.9951
[2022-06-11 11:41:48] __main__ INFO: Elapsed 0.91
[2022-06-11 11:41:48] __main__ INFO: Train 9 3120
[2022-06-11 11:41:52] __main__ INFO: Epoch 9 Step 100/390 lr 0.100000 loss 0.3391 (0.3046) acc@1 0.8906 (0.8966) acc@5 1.0000 (0.9978)
[2022-06-11 11:41:56] __main__ INFO: Epoch 9 Step 200/390 lr 0.100000 loss 0.2622 (0.3039) acc@1 0.9375 (0.8954) acc@5 1.0000 (0.9977)
[2022-06-11 11:42:00] __main__ INFO: Epoch 9 Step 300/390 lr 0.100000 loss 0.4118 (0.3096) acc@1 0.8750 (0.8928) acc@5 1.0000 (0.9972)
[2022-06-11 11:42:03] __main__ INFO: Epoch 9 Step 390/390 lr 0.100000 loss 0.2930 (0.3094) acc@1 0.9219 (0.8930) acc@5 1.0000 (0.9970)
[2022-06-11 11:42:03] __main__ INFO: Elapsed 15.57
[2022-06-11 11:42:03] __main__ INFO: Val 9
[2022-06-11 11:42:04] __main__ INFO: Epoch 9 loss 0.5741 acc@1 0.8195 acc@5 0.9928
[2022-06-11 11:42:04] __main__ INFO: Elapsed 0.97
[2022-06-11 11:42:04] __main__ INFO: Train 10 3510
[2022-06-11 11:42:08] __main__ INFO: Epoch 10 Step 100/390 lr 0.100000 loss 0.3077 (0.2956) acc@1 0.8594 (0.8976) acc@5 1.0000 (0.9969)
[2022-06-11 11:42:13] __main__ INFO: Epoch 10 Step 200/390 lr 0.100000 loss 0.3786 (0.3034) acc@1 0.8750 (0.8950) acc@5 1.0000 (0.9964)
[2022-06-11 11:42:16] __main__ INFO: Epoch 10 Step 300/390 lr 0.100000 loss 0.1840 (0.3018) acc@1 0.9375 (0.8956) acc@5 1.0000 (0.9967)
[2022-06-11 11:42:20] __main__ INFO: Epoch 10 Step 390/390 lr 0.100000 loss 0.2419 (0.3055) acc@1 0.8906 (0.8938) acc@5 1.0000 (0.9967)
[2022-06-11 11:42:20] __main__ INFO: Elapsed 15.69
[2022-06-11 11:42:20] __main__ INFO: Val 10
[2022-06-11 11:42:21] __main__ INFO: Epoch 10 loss 0.6033 acc@1 0.8143 acc@5 0.9891
[2022-06-11 11:42:21] __main__ INFO: Elapsed 0.95
[2022-06-11 11:42:21] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00010.pth
[2022-06-11 11:43:25] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: True
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 11:43:25] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 1
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 11:45:19] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: True
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 11:45:19] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 1
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 11:45:47] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: True
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 11:45:47] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 1
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 11:46:25] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: True
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 11:46:25] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 1
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 11:47:09] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: /home/esoc/jihee/Image_Classification/experiments/cifar10/resnet/exp00/checkpoint_00160.pth
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: True
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-11 11:47:09] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 1
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-11 11:47:13] __main__ INFO: MACs   : 112.57M
[2022-06-11 11:47:13] __main__ INFO: #params: 758.55K
[2022-06-11 11:47:13] __main__ INFO: Val 0
[2022-06-11 11:47:14] __main__ INFO: Epoch 0 loss 2.9075 acc@1 0.1334 acc@5 0.5883
[2022-06-11 11:47:14] __main__ INFO: Elapsed 1.02
[2022-06-11 11:47:14] __main__ INFO: Train 1 0
[2022-06-11 11:47:16] __main__ INFO: Epoch 1 Step 39/39 lr 0.100000 loss 1.1085 (1.4677) acc@1 0.6094 (0.4816) acc@5 0.9766 (0.9099)
[2022-06-11 11:47:16] __main__ INFO: Elapsed 2.00
[2022-06-11 11:47:16] __main__ INFO: Val 1
[2022-06-11 11:47:17] __main__ INFO: Epoch 1 loss 1.9991 acc@1 0.4416 acc@5 0.8211
[2022-06-11 11:47:17] __main__ INFO: Elapsed 0.98
[2022-06-11 11:47:17] __main__ INFO: Train 2 39
[2022-06-11 11:47:19] __main__ INFO: Epoch 2 Step 39/39 lr 0.100000 loss 0.8281 (0.8931) acc@1 0.6641 (0.6855) acc@5 0.9844 (0.9764)
[2022-06-11 11:47:19] __main__ INFO: Elapsed 1.82
[2022-06-11 11:47:19] __main__ INFO: Val 2
[2022-06-11 11:47:20] __main__ INFO: Epoch 2 loss 1.1504 acc@1 0.6234 acc@5 0.9625
[2022-06-11 11:47:20] __main__ INFO: Elapsed 0.98
[2022-06-11 11:47:20] __main__ INFO: Train 3 78
[2022-06-11 11:47:22] __main__ INFO: Epoch 3 Step 39/39 lr 0.100000 loss 0.7418 (0.7268) acc@1 0.7188 (0.7442) acc@5 0.9844 (0.9874)
[2022-06-11 11:47:22] __main__ INFO: Elapsed 1.88
[2022-06-11 11:47:22] __main__ INFO: Val 3
[2022-06-11 11:47:23] __main__ INFO: Epoch 3 loss 1.0344 acc@1 0.6665 acc@5 0.9723
[2022-06-11 11:47:23] __main__ INFO: Elapsed 0.95
[2022-06-11 11:47:23] __main__ INFO: Train 4 117
[2022-06-11 11:47:25] __main__ INFO: Epoch 4 Step 39/39 lr 0.100000 loss 0.7325 (0.6317) acc@1 0.7266 (0.7788) acc@5 0.9922 (0.9878)
[2022-06-11 11:47:25] __main__ INFO: Elapsed 1.92
[2022-06-11 11:47:25] __main__ INFO: Val 4
[2022-06-11 11:47:26] __main__ INFO: Epoch 4 loss 1.1381 acc@1 0.6523 acc@5 0.9468
[2022-06-11 11:47:26] __main__ INFO: Elapsed 0.93
[2022-06-11 11:47:26] __main__ INFO: Train 5 156
[2022-06-11 11:47:28] __main__ INFO: Epoch 5 Step 39/39 lr 0.100000 loss 0.4686 (0.5484) acc@1 0.8594 (0.8089) acc@5 0.9844 (0.9878)
[2022-06-11 11:47:28] __main__ INFO: Elapsed 1.90
[2022-06-11 11:47:28] __main__ INFO: Val 5
[2022-06-11 11:47:29] __main__ INFO: Epoch 5 loss 1.1331 acc@1 0.6604 acc@5 0.9684
[2022-06-11 11:47:29] __main__ INFO: Elapsed 1.00
[2022-06-11 11:47:29] __main__ INFO: Train 6 195
[2022-06-11 11:47:31] __main__ INFO: Epoch 6 Step 39/39 lr 0.100000 loss 0.5048 (0.5052) acc@1 0.8281 (0.8247) acc@5 0.9922 (0.9914)
[2022-06-11 11:47:31] __main__ INFO: Elapsed 1.89
[2022-06-11 11:47:31] __main__ INFO: Val 6
[2022-06-11 11:47:32] __main__ INFO: Epoch 6 loss 1.1472 acc@1 0.6510 acc@5 0.9723
[2022-06-11 11:47:32] __main__ INFO: Elapsed 0.90
[2022-06-11 11:47:32] __main__ INFO: Train 7 234
[2022-06-11 11:47:33] __main__ INFO: Epoch 7 Step 39/39 lr 0.100000 loss 0.5296 (0.4943) acc@1 0.8125 (0.8309) acc@5 0.9922 (0.9924)
[2022-06-11 11:47:33] __main__ INFO: Elapsed 1.86
[2022-06-11 11:47:33] __main__ INFO: Val 7
[2022-06-11 11:47:34] __main__ INFO: Epoch 7 loss 1.0596 acc@1 0.6989 acc@5 0.9757
[2022-06-11 11:47:34] __main__ INFO: Elapsed 0.90
[2022-06-11 11:47:34] __main__ INFO: Train 8 273
[2022-06-11 11:47:36] __main__ INFO: Epoch 8 Step 39/39 lr 0.100000 loss 0.5585 (0.4186) acc@1 0.7969 (0.8544) acc@5 1.0000 (0.9952)
[2022-06-11 11:47:36] __main__ INFO: Elapsed 1.82
[2022-06-11 11:47:36] __main__ INFO: Val 8
[2022-06-11 11:47:37] __main__ INFO: Epoch 8 loss 0.8895 acc@1 0.7261 acc@5 0.9779
[2022-06-11 11:47:37] __main__ INFO: Elapsed 0.99
[2022-06-11 11:47:37] __main__ INFO: Train 9 312
[2022-06-11 11:47:39] __main__ INFO: Epoch 9 Step 39/39 lr 0.100000 loss 0.3663 (0.3945) acc@1 0.8594 (0.8620) acc@5 0.9922 (0.9968)
[2022-06-11 11:47:39] __main__ INFO: Elapsed 1.86
[2022-06-11 11:47:39] __main__ INFO: Val 9
[2022-06-11 11:47:40] __main__ INFO: Epoch 9 loss 1.0373 acc@1 0.7009 acc@5 0.9689
[2022-06-11 11:47:40] __main__ INFO: Elapsed 0.94
[2022-06-11 11:47:40] __main__ INFO: Train 10 351
[2022-06-11 11:47:42] __main__ INFO: Epoch 10 Step 39/39 lr 0.100000 loss 0.4399 (0.3806) acc@1 0.8594 (0.8652) acc@5 1.0000 (0.9972)
[2022-06-11 11:47:42] __main__ INFO: Elapsed 1.86
[2022-06-11 11:47:42] __main__ INFO: Val 10
[2022-06-11 11:47:43] __main__ INFO: Epoch 10 loss 1.0641 acc@1 0.7062 acc@5 0.9784
[2022-06-11 11:47:43] __main__ INFO: Elapsed 0.90
[2022-06-11 11:47:43] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00010.pth
[2022-06-16 11:03:12] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: ''
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: True
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-16 11:03:12] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 1
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-16 11:04:04] __main__ INFO: MACs   : 112.57M
[2022-06-16 11:04:04] __main__ INFO: #params: 758.55K
[2022-06-16 11:04:50] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: ''
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: True
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-16 11:04:50] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 1
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-16 11:04:53] __main__ INFO: MACs   : 112.57M
[2022-06-16 11:04:53] __main__ INFO: #params: 758.55K
[2022-06-16 11:04:55] __main__ INFO: Val 0
[2022-06-16 11:05:19] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: ''
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: False
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-16 11:05:19] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 1
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-16 11:05:23] __main__ INFO: MACs   : 112.57M
[2022-06-16 11:05:23] __main__ INFO: #params: 758.55K
[2022-06-16 11:05:24] __main__ INFO: Val 0
[2022-06-16 11:33:11] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: ''
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: True
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-16 11:33:11] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-16 11:33:16] __main__ INFO: MACs   : 112.57M
[2022-06-16 11:33:16] __main__ INFO: #params: 758.55K
[2022-06-16 11:33:16] __main__ INFO: Val 0
[2022-06-16 11:33:17] __main__ INFO: Epoch 0 loss 27477.4460 acc@1 0.1002 acc@5 0.5020
[2022-06-16 11:33:17] __main__ INFO: Elapsed 0.82
[2022-06-16 11:33:17] __main__ INFO: Train 1 0
[2022-06-16 11:33:19] __main__ INFO: Epoch 1 Step 39/39 lr 0.100000 loss 2.5557 (4.3958) acc@1 0.0938 (0.1084) acc@5 0.5625 (0.5134)
[2022-06-16 11:33:19] __main__ INFO: Elapsed 2.04
[2022-06-16 11:33:19] __main__ INFO: Val 1
[2022-06-16 11:33:20] __main__ INFO: Epoch 1 loss 8.0965 acc@1 0.1058 acc@5 0.5285
[2022-06-16 11:33:20] __main__ INFO: Elapsed 0.70
[2022-06-16 11:33:20] __main__ INFO: Train 2 39
[2022-06-16 11:33:21] __main__ INFO: Epoch 2 Step 39/39 lr 0.100000 loss 2.2933 (2.3830) acc@1 0.1094 (0.1030) acc@5 0.5469 (0.5154)
[2022-06-16 11:33:21] __main__ INFO: Elapsed 1.70
[2022-06-16 11:33:21] __main__ INFO: Val 2
[2022-06-16 11:33:22] __main__ INFO: Epoch 2 loss 2.3345 acc@1 0.1069 acc@5 0.5106
[2022-06-16 11:33:22] __main__ INFO: Elapsed 0.69
[2022-06-16 11:33:22] __main__ INFO: Train 3 78
[2022-06-16 11:33:24] __main__ INFO: Epoch 3 Step 39/39 lr 0.100000 loss 2.3020 (2.3441) acc@1 0.1250 (0.1004) acc@5 0.5469 (0.5116)
[2022-06-16 11:33:24] __main__ INFO: Elapsed 1.68
[2022-06-16 11:33:24] __main__ INFO: Val 3
[2022-06-16 11:33:24] __main__ INFO: Epoch 3 loss 2.3178 acc@1 0.1056 acc@5 0.5081
[2022-06-16 11:33:24] __main__ INFO: Elapsed 0.71
[2022-06-16 11:33:24] __main__ INFO: Train 4 117
[2022-06-16 11:33:26] __main__ INFO: Epoch 4 Step 39/39 lr 0.100000 loss 2.3065 (2.3293) acc@1 0.1016 (0.1064) acc@5 0.4766 (0.5260)
[2022-06-16 11:33:26] __main__ INFO: Elapsed 1.67
[2022-06-16 11:33:26] __main__ INFO: Val 4
[2022-06-16 11:33:27] __main__ INFO: Epoch 4 loss 2.3127 acc@1 0.1077 acc@5 0.5138
[2022-06-16 11:33:27] __main__ INFO: Elapsed 0.77
[2022-06-16 11:33:27] __main__ INFO: Train 5 156
[2022-06-16 11:33:29] __main__ INFO: Epoch 5 Step 39/39 lr 0.100000 loss 2.3142 (2.3242) acc@1 0.1094 (0.0970) acc@5 0.4688 (0.5150)
[2022-06-16 11:33:29] __main__ INFO: Elapsed 1.70
[2022-06-16 11:33:29] __main__ INFO: Val 5
[2022-06-16 11:33:29] __main__ INFO: Epoch 5 loss 2.3142 acc@1 0.1059 acc@5 0.5064
[2022-06-16 11:33:29] __main__ INFO: Elapsed 0.71
[2022-06-16 11:33:29] __main__ INFO: Train 6 195
[2022-06-16 11:33:31] __main__ INFO: Epoch 6 Step 39/39 lr 0.100000 loss 2.3112 (2.3170) acc@1 0.1094 (0.1040) acc@5 0.4922 (0.5054)
[2022-06-16 11:33:31] __main__ INFO: Elapsed 1.73
[2022-06-16 11:33:31] __main__ INFO: Val 6
[2022-06-16 11:33:32] __main__ INFO: Epoch 6 loss 2.3140 acc@1 0.1057 acc@5 0.5091
[2022-06-16 11:33:32] __main__ INFO: Elapsed 0.76
[2022-06-16 11:33:32] __main__ INFO: Train 7 234
[2022-06-16 11:33:33] __main__ INFO: Epoch 7 Step 39/39 lr 0.100000 loss 2.2988 (2.3093) acc@1 0.1094 (0.1040) acc@5 0.5781 (0.5166)
[2022-06-16 11:33:33] __main__ INFO: Elapsed 1.67
[2022-06-16 11:33:33] __main__ INFO: Val 7
[2022-06-16 11:33:34] __main__ INFO: Epoch 7 loss 2.3050 acc@1 0.1051 acc@5 0.5089
[2022-06-16 11:33:34] __main__ INFO: Elapsed 0.73
[2022-06-16 11:33:34] __main__ INFO: Train 8 273
[2022-06-16 11:33:36] __main__ INFO: Epoch 8 Step 39/39 lr 0.100000 loss 2.3509 (2.3081) acc@1 0.0938 (0.0996) acc@5 0.4766 (0.5152)
[2022-06-16 11:33:36] __main__ INFO: Elapsed 1.65
[2022-06-16 11:33:36] __main__ INFO: Val 8
[2022-06-16 11:33:37] __main__ INFO: Epoch 8 loss 2.3020 acc@1 0.1057 acc@5 0.5065
[2022-06-16 11:33:37] __main__ INFO: Elapsed 0.69
[2022-06-16 11:33:37] __main__ INFO: Train 9 312
[2022-06-16 11:33:38] __main__ INFO: Epoch 9 Step 39/39 lr 0.100000 loss 2.3283 (2.3066) acc@1 0.0625 (0.1044) acc@5 0.4844 (0.5108)
[2022-06-16 11:33:38] __main__ INFO: Elapsed 1.65
[2022-06-16 11:33:38] __main__ INFO: Val 9
[2022-06-16 11:33:39] __main__ INFO: Epoch 9 loss 2.3324 acc@1 0.1026 acc@5 0.5095
[2022-06-16 11:33:39] __main__ INFO: Elapsed 0.70
[2022-06-16 11:33:39] __main__ INFO: Train 10 351
[2022-06-16 11:33:41] __main__ INFO: Epoch 10 Step 39/39 lr 0.100000 loss 2.3122 (2.3051) acc@1 0.0703 (0.1056) acc@5 0.5547 (0.5056)
[2022-06-16 11:33:41] __main__ INFO: Elapsed 1.66
[2022-06-16 11:33:41] __main__ INFO: Val 10
[2022-06-16 11:33:41] __main__ INFO: Epoch 10 loss 2.3193 acc@1 0.1071 acc@5 0.5082
[2022-06-16 11:33:41] __main__ INFO: Elapsed 0.75
[2022-06-16 11:33:41] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00010.pth
[2022-06-16 11:34:01] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: ''
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: False
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-16 11:34:01] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-16 11:34:05] __main__ INFO: MACs   : 112.57M
[2022-06-16 11:34:05] __main__ INFO: #params: 758.55K
[2022-06-16 11:34:05] __main__ INFO: Val 0
[2022-06-16 11:34:06] __main__ INFO: Epoch 0 loss 16761.8818 acc@1 0.1000 acc@5 0.5000
[2022-06-16 11:34:06] __main__ INFO: Elapsed 0.77
[2022-06-16 11:34:06] __main__ INFO: Train 1 0
[2022-06-16 11:34:10] __main__ INFO: Epoch 1 Step 100/390 lr 0.100000 loss 2.3763 (3.0748) acc@1 0.1484 (0.1088) acc@5 0.5781 (0.5452)
[2022-06-16 11:34:14] __main__ INFO: Epoch 1 Step 200/390 lr 0.100000 loss 2.2905 (2.6777) acc@1 0.1562 (0.1248) acc@5 0.6797 (0.5755)
[2022-06-16 11:34:18] __main__ INFO: Epoch 1 Step 300/390 lr 0.100000 loss 2.1006 (2.5043) acc@1 0.1875 (0.1440) acc@5 0.7812 (0.6162)
[2022-06-16 11:34:21] __main__ INFO: Epoch 1 Step 390/390 lr 0.100000 loss 1.9764 (2.3939) acc@1 0.2578 (0.1625) acc@5 0.8438 (0.6540)
[2022-06-16 11:34:21] __main__ INFO: Elapsed 15.59
[2022-06-16 11:34:21] __main__ INFO: Val 1
[2022-06-16 11:34:22] __main__ INFO: Epoch 1 loss 1.9449 acc@1 0.2482 acc@5 0.8131
[2022-06-16 11:34:22] __main__ INFO: Elapsed 0.78
[2022-06-16 11:34:22] __main__ INFO: Train 2 390
[2022-06-16 11:34:26] __main__ INFO: Epoch 2 Step 100/390 lr 0.100000 loss 1.8968 (1.9380) acc@1 0.2109 (0.2491) acc@5 0.8203 (0.8190)
[2022-06-16 11:34:30] __main__ INFO: Epoch 2 Step 200/390 lr 0.100000 loss 1.6616 (1.8778) acc@1 0.3828 (0.2758) acc@5 0.8828 (0.8350)
[2022-06-16 11:34:34] __main__ INFO: Epoch 2 Step 300/390 lr 0.100000 loss 1.6512 (1.8239) acc@1 0.3594 (0.3015) acc@5 0.8906 (0.8492)
[2022-06-16 11:34:38] __main__ INFO: Epoch 2 Step 390/390 lr 0.100000 loss 1.4952 (1.7783) acc@1 0.4531 (0.3233) acc@5 0.8984 (0.8595)
[2022-06-16 11:34:38] __main__ INFO: Elapsed 15.79
[2022-06-16 11:34:38] __main__ INFO: Val 2
[2022-06-16 11:34:39] __main__ INFO: Epoch 2 loss 1.5750 acc@1 0.4263 acc@5 0.9087
[2022-06-16 11:34:39] __main__ INFO: Elapsed 0.74
[2022-06-16 11:34:39] __main__ INFO: Train 3 780
[2022-06-16 11:34:43] __main__ INFO: Epoch 3 Step 100/390 lr 0.100000 loss 1.4499 (1.5385) acc@1 0.4844 (0.4384) acc@5 0.8672 (0.9067)
[2022-06-16 11:34:47] __main__ INFO: Epoch 3 Step 200/390 lr 0.100000 loss 1.3488 (1.4895) acc@1 0.5312 (0.4552) acc@5 0.9531 (0.9152)
[2022-06-16 11:35:04] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: ''
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: True
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 500
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: False
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-16 11:35:04] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-16 11:35:08] __main__ INFO: MACs   : 112.57M
[2022-06-16 11:35:08] __main__ INFO: #params: 758.55K
[2022-06-16 11:35:08] __main__ INFO: Val 0
[2022-06-16 11:35:09] __main__ INFO: Epoch 0 loss 27477.4463 acc@1 0.1002 acc@5 0.5020
[2022-06-16 11:35:09] __main__ INFO: Elapsed 0.78
[2022-06-16 11:35:09] __main__ INFO: Train 1 0
[2022-06-16 11:35:10] __main__ INFO: Epoch 1 Step 39/39 lr 0.100000 loss 2.2672 (4.3523) acc@1 0.1094 (0.1092) acc@5 0.5938 (0.5154)
[2022-06-16 11:35:11] __main__ INFO: Elapsed 1.89
[2022-06-16 11:35:11] __main__ INFO: Val 1
[2022-06-16 11:35:11] __main__ INFO: Epoch 1 loss 14.9878 acc@1 0.1087 acc@5 0.5310
[2022-06-16 11:35:11] __main__ INFO: Elapsed 0.73
[2022-06-16 11:35:11] __main__ INFO: Train 2 39
[2022-06-16 11:35:13] __main__ INFO: Epoch 2 Step 39/39 lr 0.100000 loss 2.2800 (2.4397) acc@1 0.1172 (0.1060) acc@5 0.5391 (0.5126)
[2022-06-16 11:35:13] __main__ INFO: Elapsed 1.60
[2022-06-16 11:35:13] __main__ INFO: Val 2
[2022-06-16 11:35:14] __main__ INFO: Epoch 2 loss 2.5837 acc@1 0.1131 acc@5 0.5089
[2022-06-16 11:35:14] __main__ INFO: Elapsed 0.73
[2022-06-16 11:35:14] __main__ INFO: Train 3 78
[2022-06-16 11:35:15] __main__ INFO: Epoch 3 Step 39/39 lr 0.100000 loss 2.2854 (2.3240) acc@1 0.1484 (0.1010) acc@5 0.5547 (0.5172)
[2022-06-16 11:35:15] __main__ INFO: Elapsed 1.63
[2022-06-16 11:35:15] __main__ INFO: Val 3
[2022-06-16 11:35:16] __main__ INFO: Epoch 3 loss 2.3020 acc@1 0.1105 acc@5 0.5179
[2022-06-16 11:35:16] __main__ INFO: Elapsed 0.67
[2022-06-16 11:35:16] __main__ INFO: Train 4 117
[2022-06-16 11:35:17] __main__ INFO: Epoch 4 Step 39/39 lr 0.100000 loss 2.3363 (2.3162) acc@1 0.0938 (0.1086) acc@5 0.4609 (0.5323)
[2022-06-16 11:35:18] __main__ INFO: Elapsed 1.62
[2022-06-16 11:35:18] __main__ INFO: Val 4
[2022-06-16 11:35:18] __main__ INFO: Epoch 4 loss 2.3066 acc@1 0.1100 acc@5 0.5188
[2022-06-16 11:35:18] __main__ INFO: Elapsed 0.76
[2022-06-16 11:35:18] __main__ INFO: Train 5 156
[2022-06-16 11:35:20] __main__ INFO: Epoch 5 Step 39/39 lr 0.100000 loss 2.3025 (2.3147) acc@1 0.1016 (0.1040) acc@5 0.5234 (0.5236)
[2022-06-16 11:35:20] __main__ INFO: Elapsed 1.70
[2022-06-16 11:35:20] __main__ INFO: Val 5
[2022-06-16 11:35:21] __main__ INFO: Epoch 5 loss 2.3371 acc@1 0.1089 acc@5 0.5207
[2022-06-16 11:35:21] __main__ INFO: Elapsed 0.75
[2022-06-16 11:35:21] __main__ INFO: Train 6 195
[2022-06-16 11:35:22] __main__ INFO: Epoch 6 Step 39/39 lr 0.100000 loss 2.2979 (2.3011) acc@1 0.1172 (0.1096) acc@5 0.5234 (0.5284)
[2022-06-16 11:35:22] __main__ INFO: Elapsed 1.61
[2022-06-16 11:35:22] __main__ INFO: Val 6
[2022-06-16 11:35:23] __main__ INFO: Epoch 6 loss 2.3055 acc@1 0.1121 acc@5 0.5196
[2022-06-16 11:35:23] __main__ INFO: Elapsed 0.77
[2022-06-16 11:35:23] __main__ INFO: Train 7 234
[2022-06-16 11:35:25] __main__ INFO: Epoch 7 Step 39/39 lr 0.100000 loss 2.2930 (2.3044) acc@1 0.1016 (0.1114) acc@5 0.5859 (0.5268)
[2022-06-16 11:35:25] __main__ INFO: Elapsed 1.67
[2022-06-16 11:35:25] __main__ INFO: Val 7
[2022-06-16 11:35:26] __main__ INFO: Epoch 7 loss 2.3103 acc@1 0.1109 acc@5 0.5199
[2022-06-16 11:35:26] __main__ INFO: Elapsed 0.76
[2022-06-16 11:35:26] __main__ INFO: Train 8 273
[2022-06-16 11:35:27] __main__ INFO: Epoch 8 Step 39/39 lr 0.100000 loss 2.2964 (2.3009) acc@1 0.1484 (0.1090) acc@5 0.4922 (0.5359)
[2022-06-16 11:35:27] __main__ INFO: Elapsed 1.65
[2022-06-16 11:35:27] __main__ INFO: Val 8
[2022-06-16 11:35:28] __main__ INFO: Epoch 8 loss 2.3151 acc@1 0.1241 acc@5 0.5236
[2022-06-16 11:35:28] __main__ INFO: Elapsed 0.68
[2022-06-16 11:35:28] __main__ INFO: Train 9 312
[2022-06-16 11:35:30] __main__ INFO: Epoch 9 Step 39/39 lr 0.100000 loss 2.3284 (2.2972) acc@1 0.0703 (0.1152) acc@5 0.4922 (0.5481)
[2022-06-16 11:35:30] __main__ INFO: Elapsed 1.68
[2022-06-16 11:35:30] __main__ INFO: Val 9
[2022-06-16 11:35:30] __main__ INFO: Epoch 9 loss 2.2984 acc@1 0.1164 acc@5 0.5232
[2022-06-16 11:35:30] __main__ INFO: Elapsed 0.67
[2022-06-16 11:35:30] __main__ INFO: Train 10 351
[2022-06-16 11:35:32] __main__ INFO: Epoch 10 Step 39/39 lr 0.100000 loss 2.3025 (2.2965) acc@1 0.0781 (0.1222) acc@5 0.5625 (0.5489)
[2022-06-16 11:35:32] __main__ INFO: Elapsed 1.75
[2022-06-16 11:35:32] __main__ INFO: Val 10
[2022-06-16 11:35:33] __main__ INFO: Epoch 10 loss 2.3022 acc@1 0.1119 acc@5 0.5457
[2022-06-16 11:35:33] __main__ INFO: Elapsed 0.69
[2022-06-16 11:35:33] __main__ INFO: Train 11 390
[2022-06-16 11:35:34] __main__ INFO: Epoch 11 Step 39/39 lr 0.100000 loss 2.3096 (2.2937) acc@1 0.0781 (0.1204) acc@5 0.4531 (0.5505)
[2022-06-16 11:35:34] __main__ INFO: Elapsed 1.59
[2022-06-16 11:35:34] __main__ INFO: Val 11
[2022-06-16 11:35:35] __main__ INFO: Epoch 11 loss 2.2991 acc@1 0.1163 acc@5 0.5379
[2022-06-16 11:35:35] __main__ INFO: Elapsed 0.81
[2022-06-16 11:35:35] __main__ INFO: Train 12 429
[2022-06-16 11:35:37] __main__ INFO: Epoch 12 Step 39/39 lr 0.100000 loss 2.3136 (2.2933) acc@1 0.0938 (0.1292) acc@5 0.5000 (0.5577)
[2022-06-16 11:35:37] __main__ INFO: Elapsed 1.64
[2022-06-16 11:35:37] __main__ INFO: Val 12
[2022-06-16 11:35:37] __main__ INFO: Epoch 12 loss 2.3110 acc@1 0.1304 acc@5 0.5488
[2022-06-16 11:35:37] __main__ INFO: Elapsed 0.71
[2022-06-16 11:35:37] __main__ INFO: Train 13 468
[2022-06-16 11:35:39] __main__ INFO: Epoch 13 Step 39/39 lr 0.100000 loss 2.3048 (2.2887) acc@1 0.1484 (0.1154) acc@5 0.5312 (0.5627)
[2022-06-16 11:35:39] __main__ INFO: Elapsed 1.69
[2022-06-16 11:35:39] __main__ INFO: Val 13
[2022-06-16 11:35:40] __main__ INFO: Epoch 13 loss 2.2953 acc@1 0.1327 acc@5 0.5447
[2022-06-16 11:35:40] __main__ INFO: Elapsed 0.69
[2022-06-16 11:35:40] __main__ INFO: Train 14 507
[2022-06-16 11:35:41] __main__ INFO: Epoch 14 Step 39/39 lr 0.100000 loss 2.2760 (2.2876) acc@1 0.1484 (0.1244) acc@5 0.6094 (0.5671)
[2022-06-16 11:35:41] __main__ INFO: Elapsed 1.64
[2022-06-16 11:35:41] __main__ INFO: Val 14
[2022-06-16 11:35:42] __main__ INFO: Epoch 14 loss 2.3034 acc@1 0.1277 acc@5 0.5588
[2022-06-16 11:35:42] __main__ INFO: Elapsed 0.71
[2022-06-16 11:35:42] __main__ INFO: Train 15 546
[2022-06-16 11:35:44] __main__ INFO: Epoch 15 Step 39/39 lr 0.100000 loss 2.3169 (2.2852) acc@1 0.1094 (0.1342) acc@5 0.5547 (0.5671)
[2022-06-16 11:35:44] __main__ INFO: Elapsed 1.60
[2022-06-16 11:35:44] __main__ INFO: Val 15
[2022-06-16 11:35:45] __main__ INFO: Epoch 15 loss 2.2867 acc@1 0.1332 acc@5 0.5534
[2022-06-16 11:35:45] __main__ INFO: Elapsed 0.76
[2022-06-16 11:35:45] __main__ INFO: Train 16 585
[2022-06-16 11:35:46] __main__ INFO: Epoch 16 Step 39/39 lr 0.100000 loss 2.1684 (2.2658) acc@1 0.2031 (0.1394) acc@5 0.7031 (0.5921)
[2022-06-16 11:35:46] __main__ INFO: Elapsed 1.65
[2022-06-16 11:35:46] __main__ INFO: Val 16
[2022-06-16 11:35:47] __main__ INFO: Epoch 16 loss 2.2085 acc@1 0.1664 acc@5 0.6456
[2022-06-16 11:35:47] __main__ INFO: Elapsed 0.75
[2022-06-16 11:35:47] __main__ INFO: Train 17 624
[2022-06-16 11:35:49] __main__ INFO: Epoch 17 Step 39/39 lr 0.100000 loss 2.1018 (2.1356) acc@1 0.1797 (0.1809) acc@5 0.7031 (0.7284)
[2022-06-16 11:35:49] __main__ INFO: Elapsed 1.81
[2022-06-16 11:35:49] __main__ INFO: Val 17
[2022-06-16 11:35:50] __main__ INFO: Epoch 17 loss 2.0913 acc@1 0.1766 acc@5 0.7510
[2022-06-16 11:35:50] __main__ INFO: Elapsed 0.74
[2022-06-16 11:35:50] __main__ INFO: Train 18 663
[2022-06-16 11:35:51] __main__ INFO: Epoch 18 Step 39/39 lr 0.100000 loss 1.9292 (2.0709) acc@1 0.3203 (0.1871) acc@5 0.8359 (0.7668)
[2022-06-16 11:35:51] __main__ INFO: Elapsed 1.70
[2022-06-16 11:35:51] __main__ INFO: Val 18
[2022-06-16 11:35:52] __main__ INFO: Epoch 18 loss 2.5405 acc@1 0.1659 acc@5 0.6787
[2022-06-16 11:35:52] __main__ INFO: Elapsed 0.77
[2022-06-16 11:35:52] __main__ INFO: Train 19 702
[2022-06-16 11:35:54] __main__ INFO: Epoch 19 Step 39/39 lr 0.100000 loss 1.9299 (2.0221) acc@1 0.2812 (0.1931) acc@5 0.8203 (0.7855)
[2022-06-16 11:35:54] __main__ INFO: Elapsed 1.65
[2022-06-16 11:35:54] __main__ INFO: Val 19
[2022-06-16 11:35:54] __main__ INFO: Epoch 19 loss 2.1422 acc@1 0.1678 acc@5 0.7075
[2022-06-16 11:35:54] __main__ INFO: Elapsed 0.66
[2022-06-16 11:35:54] __main__ INFO: Train 20 741
[2022-06-16 11:35:56] __main__ INFO: Epoch 20 Step 39/39 lr 0.100000 loss 1.9470 (1.9851) acc@1 0.2422 (0.2089) acc@5 0.7734 (0.7903)
[2022-06-16 11:35:56] __main__ INFO: Elapsed 1.76
[2022-06-16 11:35:56] __main__ INFO: Val 20
[2022-06-16 11:35:57] __main__ INFO: Epoch 20 loss 2.0145 acc@1 0.2216 acc@5 0.7878
[2022-06-16 11:35:57] __main__ INFO: Elapsed 0.74
[2022-06-16 11:35:57] __main__ INFO: Train 21 780
[2022-06-16 11:35:58] __main__ INFO: Epoch 21 Step 39/39 lr 0.100000 loss 1.9126 (1.9522) acc@1 0.2188 (0.2334) acc@5 0.7812 (0.8059)
[2022-06-16 11:35:59] __main__ INFO: Elapsed 1.72
[2022-06-16 11:35:59] __main__ INFO: Val 21
[2022-06-16 11:35:59] __main__ INFO: Epoch 21 loss 1.9732 acc@1 0.2296 acc@5 0.8256
[2022-06-16 11:35:59] __main__ INFO: Elapsed 0.76
[2022-06-16 11:35:59] __main__ INFO: Train 22 819
[2022-06-16 11:36:01] __main__ INFO: Epoch 22 Step 39/39 lr 0.100000 loss 1.9435 (1.9354) acc@1 0.2422 (0.2454) acc@5 0.8125 (0.8125)
[2022-06-16 11:36:01] __main__ INFO: Elapsed 1.69
[2022-06-16 11:36:01] __main__ INFO: Val 22
[2022-06-16 11:36:02] __main__ INFO: Epoch 22 loss 1.9139 acc@1 0.2702 acc@5 0.8347
[2022-06-16 11:36:02] __main__ INFO: Elapsed 0.74
[2022-06-16 11:36:02] __main__ INFO: Train 23 858
[2022-06-16 11:36:03] __main__ INFO: Epoch 23 Step 39/39 lr 0.100000 loss 1.9179 (1.8952) acc@1 0.2969 (0.2728) acc@5 0.8281 (0.8245)
[2022-06-16 11:36:03] __main__ INFO: Elapsed 1.67
[2022-06-16 11:36:03] __main__ INFO: Val 23
[2022-06-16 11:36:04] __main__ INFO: Epoch 23 loss 1.9667 acc@1 0.2756 acc@5 0.8144
[2022-06-16 11:36:04] __main__ INFO: Elapsed 0.71
[2022-06-16 11:36:04] __main__ INFO: Train 24 897
[2022-06-16 11:36:06] __main__ INFO: Epoch 24 Step 39/39 lr 0.100000 loss 1.9453 (1.8582) acc@1 0.2656 (0.2827) acc@5 0.7891 (0.8438)
[2022-06-16 11:36:06] __main__ INFO: Elapsed 1.71
[2022-06-16 11:36:06] __main__ INFO: Val 24
[2022-06-16 11:36:07] __main__ INFO: Epoch 24 loss 1.8079 acc@1 0.3174 acc@5 0.8547
[2022-06-16 11:36:07] __main__ INFO: Elapsed 0.73
[2022-06-16 11:36:07] __main__ INFO: Train 25 936
[2022-06-16 11:36:08] __main__ INFO: Epoch 25 Step 39/39 lr 0.100000 loss 1.8371 (1.8299) acc@1 0.3828 (0.3009) acc@5 0.8594 (0.8494)
[2022-06-16 11:36:08] __main__ INFO: Elapsed 1.67
[2022-06-16 11:36:08] __main__ INFO: Val 25
[2022-06-16 11:36:09] __main__ INFO: Epoch 25 loss 1.8347 acc@1 0.3144 acc@5 0.8423
[2022-06-16 11:36:09] __main__ INFO: Elapsed 0.76
[2022-06-16 11:36:09] __main__ INFO: Train 26 975
[2022-06-16 11:36:11] __main__ INFO: Epoch 26 Step 39/39 lr 0.100000 loss 1.8043 (1.8118) acc@1 0.2969 (0.3113) acc@5 0.8828 (0.8530)
[2022-06-16 11:36:11] __main__ INFO: Elapsed 1.71
[2022-06-16 11:36:11] __main__ INFO: Val 26
[2022-06-16 11:36:11] __main__ INFO: Epoch 26 loss 1.7871 acc@1 0.3333 acc@5 0.8615
[2022-06-16 11:36:11] __main__ INFO: Elapsed 0.74
[2022-06-16 11:36:11] __main__ INFO: Train 27 1014
[2022-06-16 11:36:13] __main__ INFO: Epoch 27 Step 39/39 lr 0.100000 loss 1.7804 (1.7769) acc@1 0.3594 (0.3307) acc@5 0.8828 (0.8598)
[2022-06-16 11:36:13] __main__ INFO: Elapsed 1.69
[2022-06-16 11:36:13] __main__ INFO: Val 27
[2022-06-16 11:36:14] __main__ INFO: Epoch 27 loss 1.7587 acc@1 0.3465 acc@5 0.8633
[2022-06-16 11:36:14] __main__ INFO: Elapsed 0.73
[2022-06-16 11:36:14] __main__ INFO: Train 28 1053
[2022-06-16 11:36:16] __main__ INFO: Epoch 28 Step 39/39 lr 0.100000 loss 1.5926 (1.7500) acc@1 0.4141 (0.3405) acc@5 0.8984 (0.8684)
[2022-06-16 11:36:16] __main__ INFO: Elapsed 1.68
[2022-06-16 11:36:16] __main__ INFO: Val 28
[2022-06-16 11:36:16] __main__ INFO: Epoch 28 loss 1.9361 acc@1 0.3095 acc@5 0.8338
[2022-06-16 11:36:16] __main__ INFO: Elapsed 0.74
[2022-06-16 11:36:16] __main__ INFO: Train 29 1092
[2022-06-16 11:36:18] __main__ INFO: Epoch 29 Step 39/39 lr 0.100000 loss 1.6202 (1.7206) acc@1 0.3516 (0.3638) acc@5 0.9297 (0.8722)
[2022-06-16 11:36:18] __main__ INFO: Elapsed 1.77
[2022-06-16 11:36:18] __main__ INFO: Val 29
[2022-06-16 11:36:19] __main__ INFO: Epoch 29 loss 1.7134 acc@1 0.3725 acc@5 0.8706
[2022-06-16 11:36:19] __main__ INFO: Elapsed 0.73
[2022-06-16 11:36:19] __main__ INFO: Train 30 1131
[2022-06-16 11:36:20] __main__ INFO: Epoch 30 Step 39/39 lr 0.100000 loss 1.6510 (1.6951) acc@1 0.4297 (0.3616) acc@5 0.8984 (0.8838)
[2022-06-16 11:36:21] __main__ INFO: Elapsed 1.67
[2022-06-16 11:36:21] __main__ INFO: Val 30
[2022-06-16 11:36:21] __main__ INFO: Epoch 30 loss 1.7140 acc@1 0.3731 acc@5 0.8693
[2022-06-16 11:36:21] __main__ INFO: Elapsed 0.71
[2022-06-16 11:36:21] __main__ INFO: Train 31 1170
[2022-06-16 11:36:23] __main__ INFO: Epoch 31 Step 39/39 lr 0.100000 loss 1.5881 (1.6772) acc@1 0.4453 (0.3722) acc@5 0.9297 (0.8842)
[2022-06-16 11:36:23] __main__ INFO: Elapsed 1.68
[2022-06-16 11:36:23] __main__ INFO: Val 31
[2022-06-16 11:36:24] __main__ INFO: Epoch 31 loss 1.9123 acc@1 0.3358 acc@5 0.8453
[2022-06-16 11:36:24] __main__ INFO: Elapsed 0.80
[2022-06-16 11:36:24] __main__ INFO: Train 32 1209
[2022-06-16 11:36:25] __main__ INFO: Epoch 32 Step 39/39 lr 0.100000 loss 1.7301 (1.6385) acc@1 0.4062 (0.3824) acc@5 0.8359 (0.8896)
[2022-06-16 11:36:25] __main__ INFO: Elapsed 1.66
[2022-06-16 11:36:25] __main__ INFO: Val 32
[2022-06-16 11:36:26] __main__ INFO: Epoch 32 loss 1.7300 acc@1 0.3817 acc@5 0.8824
[2022-06-16 11:36:26] __main__ INFO: Elapsed 0.73
[2022-06-16 11:36:26] __main__ INFO: Train 33 1248
[2022-06-16 11:36:28] __main__ INFO: Epoch 33 Step 39/39 lr 0.100000 loss 1.5046 (1.6077) acc@1 0.3672 (0.3976) acc@5 0.9062 (0.9020)
[2022-06-16 11:36:28] __main__ INFO: Elapsed 1.70
[2022-06-16 11:36:28] __main__ INFO: Val 33
[2022-06-16 11:36:28] __main__ INFO: Epoch 33 loss 1.6551 acc@1 0.3860 acc@5 0.8739
[2022-06-16 11:36:28] __main__ INFO: Elapsed 0.66
[2022-06-16 11:36:28] __main__ INFO: Train 34 1287
[2022-06-16 11:36:30] __main__ INFO: Epoch 34 Step 39/39 lr 0.100000 loss 1.5436 (1.5791) acc@1 0.4219 (0.4101) acc@5 0.9453 (0.9071)
[2022-06-16 11:36:30] __main__ INFO: Elapsed 1.68
[2022-06-16 11:36:30] __main__ INFO: Val 34
[2022-06-16 11:36:31] __main__ INFO: Epoch 34 loss 1.6046 acc@1 0.3998 acc@5 0.8954
[2022-06-16 11:36:31] __main__ INFO: Elapsed 0.76
[2022-06-16 11:36:31] __main__ INFO: Train 35 1326
[2022-06-16 11:36:33] __main__ INFO: Epoch 35 Step 39/39 lr 0.100000 loss 1.5049 (1.5486) acc@1 0.4766 (0.4187) acc@5 0.9297 (0.9121)
[2022-06-16 11:36:33] __main__ INFO: Elapsed 1.66
[2022-06-16 11:36:33] __main__ INFO: Val 35
[2022-06-16 11:36:33] __main__ INFO: Epoch 35 loss 1.5941 acc@1 0.4134 acc@5 0.8997
[2022-06-16 11:36:33] __main__ INFO: Elapsed 0.80
[2022-06-16 11:36:33] __main__ INFO: Train 36 1365
[2022-06-16 11:36:35] __main__ INFO: Epoch 36 Step 39/39 lr 0.100000 loss 1.3997 (1.5450) acc@1 0.4844 (0.4199) acc@5 0.9141 (0.9119)
[2022-06-16 11:36:35] __main__ INFO: Elapsed 1.70
[2022-06-16 11:36:35] __main__ INFO: Val 36
[2022-06-16 11:36:36] __main__ INFO: Epoch 36 loss 1.6755 acc@1 0.3987 acc@5 0.8856
[2022-06-16 11:36:36] __main__ INFO: Elapsed 0.73
[2022-06-16 11:36:36] __main__ INFO: Train 37 1404
[2022-06-16 11:36:38] __main__ INFO: Epoch 37 Step 39/39 lr 0.100000 loss 1.5457 (1.4937) acc@1 0.4219 (0.4481) acc@5 0.9453 (0.9201)
[2022-06-16 11:36:38] __main__ INFO: Elapsed 1.76
[2022-06-16 11:36:38] __main__ INFO: Val 37
[2022-06-16 11:36:38] __main__ INFO: Epoch 37 loss 1.5326 acc@1 0.4393 acc@5 0.9148
[2022-06-16 11:36:38] __main__ INFO: Elapsed 0.75
[2022-06-16 11:36:38] __main__ INFO: Train 38 1443
[2022-06-16 11:36:40] __main__ INFO: Epoch 38 Step 39/39 lr 0.100000 loss 1.5741 (1.4991) acc@1 0.3750 (0.4393) acc@5 0.9219 (0.9191)
[2022-06-16 11:36:40] __main__ INFO: Elapsed 1.66
[2022-06-16 11:36:40] __main__ INFO: Val 38
[2022-06-16 11:36:41] __main__ INFO: Epoch 38 loss 1.5839 acc@1 0.4225 acc@5 0.9040
[2022-06-16 11:36:41] __main__ INFO: Elapsed 0.72
[2022-06-16 11:36:41] __main__ INFO: Train 39 1482
[2022-06-16 11:36:42] __main__ INFO: Epoch 39 Step 39/39 lr 0.100000 loss 1.4062 (1.4582) acc@1 0.4375 (0.4531) acc@5 0.9531 (0.9233)
[2022-06-16 11:36:42] __main__ INFO: Elapsed 1.62
[2022-06-16 11:36:42] __main__ INFO: Val 39
[2022-06-16 11:36:43] __main__ INFO: Epoch 39 loss 1.5653 acc@1 0.4323 acc@5 0.9037
[2022-06-16 11:36:43] __main__ INFO: Elapsed 0.76
[2022-06-16 11:36:43] __main__ INFO: Train 40 1521
[2022-06-16 11:36:45] __main__ INFO: Epoch 40 Step 39/39 lr 0.100000 loss 1.5002 (1.4220) acc@1 0.4688 (0.4742) acc@5 0.8984 (0.9311)
[2022-06-16 11:36:45] __main__ INFO: Elapsed 1.72
[2022-06-16 11:36:45] __main__ INFO: Val 40
[2022-06-16 11:36:46] __main__ INFO: Epoch 40 loss 1.6099 acc@1 0.4397 acc@5 0.9039
[2022-06-16 11:36:46] __main__ INFO: Elapsed 0.75
[2022-06-16 11:36:46] __main__ INFO: Train 41 1560
[2022-06-16 11:36:47] __main__ INFO: Epoch 41 Step 39/39 lr 0.100000 loss 1.2619 (1.3877) acc@1 0.5391 (0.4878) acc@5 0.9531 (0.9335)
[2022-06-16 11:36:47] __main__ INFO: Elapsed 1.69
[2022-06-16 11:36:47] __main__ INFO: Val 41
[2022-06-16 11:36:48] __main__ INFO: Epoch 41 loss 1.5032 acc@1 0.4513 acc@5 0.9168
[2022-06-16 11:36:48] __main__ INFO: Elapsed 0.72
[2022-06-16 11:36:48] __main__ INFO: Train 42 1599
[2022-06-16 11:36:50] __main__ INFO: Epoch 42 Step 39/39 lr 0.100000 loss 1.2566 (1.3652) acc@1 0.5312 (0.4952) acc@5 0.9375 (0.9391)
[2022-06-16 11:36:50] __main__ INFO: Elapsed 1.59
[2022-06-16 11:36:50] __main__ INFO: Val 42
[2022-06-16 11:36:50] __main__ INFO: Epoch 42 loss 1.6255 acc@1 0.4388 acc@5 0.8973
[2022-06-16 11:36:50] __main__ INFO: Elapsed 0.72
[2022-06-16 11:36:50] __main__ INFO: Train 43 1638
[2022-06-16 11:36:52] __main__ INFO: Epoch 43 Step 39/39 lr 0.100000 loss 1.2804 (1.3318) acc@1 0.5469 (0.5078) acc@5 0.9375 (0.9457)
[2022-06-16 11:36:52] __main__ INFO: Elapsed 1.64
[2022-06-16 11:36:52] __main__ INFO: Val 43
[2022-06-16 11:36:53] __main__ INFO: Epoch 43 loss 1.4142 acc@1 0.4945 acc@5 0.9325
[2022-06-16 11:36:53] __main__ INFO: Elapsed 0.77
[2022-06-16 11:36:53] __main__ INFO: Train 44 1677
[2022-06-16 11:36:54] __main__ INFO: Epoch 44 Step 39/39 lr 0.100000 loss 1.3935 (1.3156) acc@1 0.5078 (0.5122) acc@5 0.9219 (0.9473)
[2022-06-16 11:36:54] __main__ INFO: Elapsed 1.70
[2022-06-16 11:36:54] __main__ INFO: Val 44
[2022-06-16 11:36:55] __main__ INFO: Epoch 44 loss 1.4485 acc@1 0.4875 acc@5 0.9302
[2022-06-16 11:36:55] __main__ INFO: Elapsed 0.79
[2022-06-16 11:36:55] __main__ INFO: Train 45 1716
[2022-06-16 11:36:57] __main__ INFO: Epoch 45 Step 39/39 lr 0.100000 loss 1.3507 (1.2877) acc@1 0.5156 (0.5238) acc@5 0.9609 (0.9529)
[2022-06-16 11:36:57] __main__ INFO: Elapsed 1.61
[2022-06-16 11:36:57] __main__ INFO: Val 45
[2022-06-16 11:36:58] __main__ INFO: Epoch 45 loss 1.4495 acc@1 0.4791 acc@5 0.9262
[2022-06-16 11:36:58] __main__ INFO: Elapsed 0.78
[2022-06-16 11:36:58] __main__ INFO: Train 46 1755
[2022-06-16 11:36:59] __main__ INFO: Epoch 46 Step 39/39 lr 0.100000 loss 1.2861 (1.2572) acc@1 0.5312 (0.5377) acc@5 0.9531 (0.9501)
[2022-06-16 11:36:59] __main__ INFO: Elapsed 1.71
[2022-06-16 11:36:59] __main__ INFO: Val 46
[2022-06-16 11:37:00] __main__ INFO: Epoch 46 loss 1.3964 acc@1 0.4998 acc@5 0.9307
[2022-06-16 11:37:00] __main__ INFO: Elapsed 0.79
[2022-06-16 11:37:00] __main__ INFO: Train 47 1794
[2022-06-16 11:37:02] __main__ INFO: Epoch 47 Step 39/39 lr 0.100000 loss 1.1345 (1.2460) acc@1 0.5547 (0.5399) acc@5 0.9688 (0.9565)
[2022-06-16 11:37:02] __main__ INFO: Elapsed 1.64
[2022-06-16 11:37:02] __main__ INFO: Val 47
[2022-06-16 11:37:03] __main__ INFO: Epoch 47 loss 1.8098 acc@1 0.4507 acc@5 0.8780
[2022-06-16 11:37:03] __main__ INFO: Elapsed 0.78
[2022-06-16 11:37:03] __main__ INFO: Train 48 1833
[2022-06-16 11:37:04] __main__ INFO: Epoch 48 Step 39/39 lr 0.100000 loss 1.2792 (1.2030) acc@1 0.5469 (0.5593) acc@5 0.9453 (0.9585)
[2022-06-16 11:37:04] __main__ INFO: Elapsed 1.72
[2022-06-16 11:37:04] __main__ INFO: Val 48
[2022-06-16 11:37:05] __main__ INFO: Epoch 48 loss 1.3419 acc@1 0.5250 acc@5 0.9396
[2022-06-16 11:37:05] __main__ INFO: Elapsed 0.73
[2022-06-16 11:37:05] __main__ INFO: Train 49 1872
[2022-06-16 11:37:07] __main__ INFO: Epoch 49 Step 39/39 lr 0.100000 loss 1.3011 (1.1632) acc@1 0.4766 (0.5745) acc@5 0.9609 (0.9631)
[2022-06-16 11:37:07] __main__ INFO: Elapsed 1.70
[2022-06-16 11:37:07] __main__ INFO: Val 49
[2022-06-16 11:37:08] __main__ INFO: Epoch 49 loss 1.3387 acc@1 0.5234 acc@5 0.9362
[2022-06-16 11:37:08] __main__ INFO: Elapsed 0.80
[2022-06-16 11:37:08] __main__ INFO: Train 50 1911
[2022-06-16 11:37:09] __main__ INFO: Epoch 50 Step 39/39 lr 0.100000 loss 1.2499 (1.1469) acc@1 0.5781 (0.5841) acc@5 0.9453 (0.9629)
[2022-06-16 11:37:09] __main__ INFO: Elapsed 1.61
[2022-06-16 11:37:09] __main__ INFO: Val 50
[2022-06-16 11:37:10] __main__ INFO: Epoch 50 loss 1.4296 acc@1 0.5052 acc@5 0.9313
[2022-06-16 11:37:10] __main__ INFO: Elapsed 0.78
[2022-06-16 11:37:10] __main__ INFO: Train 51 1950
[2022-06-16 11:37:11] __main__ INFO: Epoch 51 Step 39/39 lr 0.100000 loss 1.1953 (1.1075) acc@1 0.5781 (0.6002) acc@5 0.9219 (0.9657)
[2022-06-16 11:37:12] __main__ INFO: Elapsed 1.61
[2022-06-16 11:37:12] __main__ INFO: Val 51
[2022-06-16 11:37:12] __main__ INFO: Epoch 51 loss 1.4752 acc@1 0.5016 acc@5 0.9236
[2022-06-16 11:37:12] __main__ INFO: Elapsed 0.68
[2022-06-16 11:37:12] __main__ INFO: Train 52 1989
[2022-06-16 11:37:14] __main__ INFO: Epoch 52 Step 39/39 lr 0.100000 loss 1.0124 (1.0759) acc@1 0.6016 (0.6088) acc@5 0.9844 (0.9690)
[2022-06-16 11:37:14] __main__ INFO: Elapsed 1.64
[2022-06-16 11:37:14] __main__ INFO: Val 52
[2022-06-16 11:37:15] __main__ INFO: Epoch 52 loss 1.3263 acc@1 0.5437 acc@5 0.9404
[2022-06-16 11:37:15] __main__ INFO: Elapsed 0.72
[2022-06-16 11:37:15] __main__ INFO: Train 53 2028
[2022-06-16 11:37:16] __main__ INFO: Epoch 53 Step 39/39 lr 0.100000 loss 1.1334 (1.0465) acc@1 0.5781 (0.6272) acc@5 0.9609 (0.9679)
[2022-06-16 11:37:16] __main__ INFO: Elapsed 1.60
[2022-06-16 11:37:16] __main__ INFO: Val 53
[2022-06-16 11:37:17] __main__ INFO: Epoch 53 loss 1.4032 acc@1 0.5321 acc@5 0.9410
[2022-06-16 11:37:17] __main__ INFO: Elapsed 0.80
[2022-06-16 11:37:17] __main__ INFO: Train 54 2067
[2022-06-16 11:37:19] __main__ INFO: Epoch 54 Step 39/39 lr 0.100000 loss 1.1296 (1.0275) acc@1 0.5391 (0.6288) acc@5 0.9766 (0.9722)
[2022-06-16 11:37:19] __main__ INFO: Elapsed 1.69
[2022-06-16 11:37:19] __main__ INFO: Val 54
[2022-06-16 11:37:19] __main__ INFO: Epoch 54 loss 1.3635 acc@1 0.5452 acc@5 0.9419
[2022-06-16 11:37:19] __main__ INFO: Elapsed 0.75
[2022-06-16 11:37:19] __main__ INFO: Train 55 2106
[2022-06-16 11:37:21] __main__ INFO: Epoch 55 Step 39/39 lr 0.100000 loss 0.9240 (1.0107) acc@1 0.6406 (0.6352) acc@5 0.9766 (0.9708)
[2022-06-16 11:37:21] __main__ INFO: Elapsed 1.76
[2022-06-16 11:37:21] __main__ INFO: Val 55
[2022-06-16 11:37:22] __main__ INFO: Epoch 55 loss 1.2438 acc@1 0.5806 acc@5 0.9506
[2022-06-16 11:37:22] __main__ INFO: Elapsed 0.69
[2022-06-16 11:37:22] __main__ INFO: Train 56 2145
[2022-06-16 11:37:23] __main__ INFO: Epoch 56 Step 39/39 lr 0.100000 loss 1.0028 (0.9836) acc@1 0.6406 (0.6466) acc@5 0.9922 (0.9724)
[2022-06-16 11:37:23] __main__ INFO: Elapsed 1.58
[2022-06-16 11:37:23] __main__ INFO: Val 56
[2022-06-16 11:37:24] __main__ INFO: Epoch 56 loss 1.3644 acc@1 0.5500 acc@5 0.9419
[2022-06-16 11:37:24] __main__ INFO: Elapsed 0.76
[2022-06-16 11:37:24] __main__ INFO: Train 57 2184
[2022-06-16 11:37:26] __main__ INFO: Epoch 57 Step 39/39 lr 0.100000 loss 1.1710 (0.9598) acc@1 0.5938 (0.6536) acc@5 0.9453 (0.9740)
[2022-06-16 11:37:26] __main__ INFO: Elapsed 1.71
[2022-06-16 11:37:26] __main__ INFO: Val 57
[2022-06-16 11:37:27] __main__ INFO: Epoch 57 loss 1.3037 acc@1 0.5608 acc@5 0.9466
[2022-06-16 11:37:27] __main__ INFO: Elapsed 0.66
[2022-06-16 11:37:27] __main__ INFO: Train 58 2223
[2022-06-16 11:37:28] __main__ INFO: Epoch 58 Step 39/39 lr 0.100000 loss 0.9098 (0.9462) acc@1 0.6875 (0.6607) acc@5 0.9766 (0.9764)
[2022-06-16 11:37:28] __main__ INFO: Elapsed 1.71
[2022-06-16 11:37:28] __main__ INFO: Val 58
[2022-06-16 11:37:29] __main__ INFO: Epoch 58 loss 1.5470 acc@1 0.5318 acc@5 0.9360
[2022-06-16 11:37:29] __main__ INFO: Elapsed 0.80
[2022-06-16 11:37:29] __main__ INFO: Train 59 2262
[2022-06-16 11:37:31] __main__ INFO: Epoch 59 Step 39/39 lr 0.100000 loss 1.0146 (0.9143) acc@1 0.5859 (0.6665) acc@5 0.9766 (0.9764)
[2022-06-16 11:37:31] __main__ INFO: Elapsed 1.64
[2022-06-16 11:37:31] __main__ INFO: Val 59
[2022-06-16 11:37:31] __main__ INFO: Epoch 59 loss 1.2876 acc@1 0.5617 acc@5 0.9485
[2022-06-16 11:37:31] __main__ INFO: Elapsed 0.74
[2022-06-16 11:37:31] __main__ INFO: Train 60 2301
[2022-06-16 11:37:33] __main__ INFO: Epoch 60 Step 39/39 lr 0.100000 loss 0.9098 (0.8880) acc@1 0.6797 (0.6819) acc@5 0.9922 (0.9776)
[2022-06-16 11:37:33] __main__ INFO: Elapsed 1.72
[2022-06-16 11:37:33] __main__ INFO: Val 60
[2022-06-16 11:37:34] __main__ INFO: Epoch 60 loss 1.5272 acc@1 0.5417 acc@5 0.9352
[2022-06-16 11:37:34] __main__ INFO: Elapsed 0.71
[2022-06-16 11:37:34] __main__ INFO: Train 61 2340
[2022-06-16 11:37:36] __main__ INFO: Epoch 61 Step 39/39 lr 0.100000 loss 0.8847 (0.8735) acc@1 0.6641 (0.6899) acc@5 0.9766 (0.9798)
[2022-06-16 11:37:36] __main__ INFO: Elapsed 1.69
[2022-06-16 11:37:36] __main__ INFO: Val 61
[2022-06-16 11:37:36] __main__ INFO: Epoch 61 loss 1.3913 acc@1 0.5581 acc@5 0.9447
[2022-06-16 11:37:36] __main__ INFO: Elapsed 0.69
[2022-06-16 11:37:36] __main__ INFO: Train 62 2379
[2022-06-16 11:37:38] __main__ INFO: Epoch 62 Step 39/39 lr 0.100000 loss 0.8202 (0.8590) acc@1 0.6953 (0.6951) acc@5 1.0000 (0.9788)
[2022-06-16 11:37:38] __main__ INFO: Elapsed 1.63
[2022-06-16 11:37:38] __main__ INFO: Val 62
[2022-06-16 11:37:39] __main__ INFO: Epoch 62 loss 1.3083 acc@1 0.5762 acc@5 0.9524
[2022-06-16 11:37:39] __main__ INFO: Elapsed 0.77
[2022-06-16 11:37:39] __main__ INFO: Train 63 2418
[2022-06-16 11:37:40] __main__ INFO: Epoch 63 Step 39/39 lr 0.100000 loss 0.7913 (0.8362) acc@1 0.7500 (0.6963) acc@5 0.9844 (0.9814)
[2022-06-16 11:37:40] __main__ INFO: Elapsed 1.73
[2022-06-16 11:37:40] __main__ INFO: Val 63
[2022-06-16 11:37:41] __main__ INFO: Epoch 63 loss 1.4095 acc@1 0.5470 acc@5 0.9477
[2022-06-16 11:37:41] __main__ INFO: Elapsed 0.72
[2022-06-16 11:37:41] __main__ INFO: Train 64 2457
[2022-06-16 11:37:43] __main__ INFO: Epoch 64 Step 39/39 lr 0.100000 loss 0.8182 (0.8076) acc@1 0.6875 (0.7101) acc@5 0.9922 (0.9820)
[2022-06-16 11:37:43] __main__ INFO: Elapsed 1.65
[2022-06-16 11:37:43] __main__ INFO: Val 64
[2022-06-16 11:37:44] __main__ INFO: Epoch 64 loss 1.3006 acc@1 0.5827 acc@5 0.9536
[2022-06-16 11:37:44] __main__ INFO: Elapsed 0.71
[2022-06-16 11:37:44] __main__ INFO: Train 65 2496
[2022-06-16 11:37:45] __main__ INFO: Epoch 65 Step 39/39 lr 0.100000 loss 0.6934 (0.7944) acc@1 0.7891 (0.7194) acc@5 0.9766 (0.9836)
[2022-06-16 11:37:45] __main__ INFO: Elapsed 1.64
[2022-06-16 11:37:45] __main__ INFO: Val 65
[2022-06-16 11:37:46] __main__ INFO: Epoch 65 loss 1.6110 acc@1 0.5320 acc@5 0.9272
[2022-06-16 11:37:46] __main__ INFO: Elapsed 0.79
[2022-06-16 11:37:46] __main__ INFO: Train 66 2535
[2022-06-16 11:37:48] __main__ INFO: Epoch 66 Step 39/39 lr 0.100000 loss 0.5765 (0.7586) acc@1 0.7812 (0.7266) acc@5 0.9922 (0.9848)
[2022-06-16 11:37:48] __main__ INFO: Elapsed 1.66
[2022-06-16 11:37:48] __main__ INFO: Val 66
[2022-06-16 11:37:48] __main__ INFO: Epoch 66 loss 1.3768 acc@1 0.5880 acc@5 0.9503
[2022-06-16 11:37:48] __main__ INFO: Elapsed 0.80
[2022-06-16 11:37:48] __main__ INFO: Train 67 2574
[2022-06-16 11:37:50] __main__ INFO: Epoch 67 Step 39/39 lr 0.100000 loss 0.7164 (0.7501) acc@1 0.7266 (0.7210) acc@5 0.9844 (0.9880)
[2022-06-16 11:37:50] __main__ INFO: Elapsed 1.69
[2022-06-16 11:37:50] __main__ INFO: Val 67
[2022-06-16 11:37:51] __main__ INFO: Epoch 67 loss 1.2747 acc@1 0.5918 acc@5 0.9593
[2022-06-16 11:37:51] __main__ INFO: Elapsed 0.80
[2022-06-16 11:37:51] __main__ INFO: Train 68 2613
[2022-06-16 11:37:53] __main__ INFO: Epoch 68 Step 39/39 lr 0.100000 loss 0.8606 (0.7477) acc@1 0.7422 (0.7330) acc@5 0.9844 (0.9854)
[2022-06-16 11:37:53] __main__ INFO: Elapsed 1.63
[2022-06-16 11:37:53] __main__ INFO: Val 68
[2022-06-16 11:37:53] __main__ INFO: Epoch 68 loss 1.5259 acc@1 0.5686 acc@5 0.9471
[2022-06-16 11:37:53] __main__ INFO: Elapsed 0.77
[2022-06-16 11:37:53] __main__ INFO: Train 69 2652
[2022-06-16 11:37:55] __main__ INFO: Epoch 69 Step 39/39 lr 0.100000 loss 0.7814 (0.7151) acc@1 0.7188 (0.7422) acc@5 0.9844 (0.9862)
[2022-06-16 11:37:55] __main__ INFO: Elapsed 1.69
[2022-06-16 11:37:55] __main__ INFO: Val 69
[2022-06-16 11:37:56] __main__ INFO: Epoch 69 loss 1.3429 acc@1 0.5856 acc@5 0.9489
[2022-06-16 11:37:56] __main__ INFO: Elapsed 0.71
[2022-06-16 11:37:56] __main__ INFO: Train 70 2691
[2022-06-16 11:37:57] __main__ INFO: Epoch 70 Step 39/39 lr 0.100000 loss 0.6295 (0.7125) acc@1 0.7344 (0.7454) acc@5 1.0000 (0.9858)
[2022-06-16 11:37:57] __main__ INFO: Elapsed 1.65
[2022-06-16 11:37:57] __main__ INFO: Val 70
[2022-06-16 11:37:58] __main__ INFO: Epoch 70 loss 1.2900 acc@1 0.5980 acc@5 0.9558
[2022-06-16 11:37:58] __main__ INFO: Elapsed 0.72
[2022-06-16 11:37:58] __main__ INFO: Train 71 2730
[2022-06-16 11:38:00] __main__ INFO: Epoch 71 Step 39/39 lr 0.100000 loss 0.8257 (0.6938) acc@1 0.7266 (0.7508) acc@5 1.0000 (0.9878)
[2022-06-16 11:38:00] __main__ INFO: Elapsed 1.65
[2022-06-16 11:38:00] __main__ INFO: Val 71
[2022-06-16 11:38:01] __main__ INFO: Epoch 71 loss 1.2331 acc@1 0.6176 acc@5 0.9575
[2022-06-16 11:38:01] __main__ INFO: Elapsed 0.84
[2022-06-16 11:38:01] __main__ INFO: Train 72 2769
[2022-06-16 11:38:02] __main__ INFO: Epoch 72 Step 39/39 lr 0.100000 loss 0.7164 (0.6874) acc@1 0.7422 (0.7530) acc@5 0.9844 (0.9882)
[2022-06-16 11:38:02] __main__ INFO: Elapsed 1.75
[2022-06-16 11:38:02] __main__ INFO: Val 72
[2022-06-16 11:38:03] __main__ INFO: Epoch 72 loss 1.2880 acc@1 0.6076 acc@5 0.9542
[2022-06-16 11:38:03] __main__ INFO: Elapsed 0.81
[2022-06-16 11:38:03] __main__ INFO: Train 73 2808
[2022-06-16 11:38:05] __main__ INFO: Epoch 73 Step 39/39 lr 0.100000 loss 0.8354 (0.6446) acc@1 0.6953 (0.7686) acc@5 0.9766 (0.9876)
[2022-06-16 11:38:05] __main__ INFO: Elapsed 1.66
[2022-06-16 11:38:05] __main__ INFO: Val 73
[2022-06-16 11:38:06] __main__ INFO: Epoch 73 loss 1.6240 acc@1 0.5496 acc@5 0.9445
[2022-06-16 11:38:06] __main__ INFO: Elapsed 0.78
[2022-06-16 11:38:06] __main__ INFO: Train 74 2847
[2022-06-16 11:38:07] __main__ INFO: Epoch 74 Step 39/39 lr 0.100000 loss 0.6354 (0.6461) acc@1 0.7656 (0.7670) acc@5 0.9922 (0.9898)
[2022-06-16 11:38:07] __main__ INFO: Elapsed 1.67
[2022-06-16 11:38:07] __main__ INFO: Val 74
[2022-06-16 11:38:08] __main__ INFO: Epoch 74 loss 1.3034 acc@1 0.6035 acc@5 0.9581
[2022-06-16 11:38:08] __main__ INFO: Elapsed 0.70
[2022-06-16 11:38:08] __main__ INFO: Train 75 2886
[2022-06-16 11:38:10] __main__ INFO: Epoch 75 Step 39/39 lr 0.100000 loss 0.5871 (0.6145) acc@1 0.7422 (0.7732) acc@5 1.0000 (0.9918)
[2022-06-16 11:38:10] __main__ INFO: Elapsed 1.62
[2022-06-16 11:38:10] __main__ INFO: Val 75
[2022-06-16 11:38:10] __main__ INFO: Epoch 75 loss 1.3189 acc@1 0.6196 acc@5 0.9593
[2022-06-16 11:38:10] __main__ INFO: Elapsed 0.75
[2022-06-16 11:38:10] __main__ INFO: Train 76 2925
[2022-06-16 11:38:12] __main__ INFO: Epoch 76 Step 39/39 lr 0.100000 loss 0.7832 (0.6174) acc@1 0.7344 (0.7780) acc@5 0.9922 (0.9930)
[2022-06-16 11:38:12] __main__ INFO: Elapsed 1.65
[2022-06-16 11:38:12] __main__ INFO: Val 76
[2022-06-16 11:38:13] __main__ INFO: Epoch 76 loss 1.4064 acc@1 0.5961 acc@5 0.9559
[2022-06-16 11:38:13] __main__ INFO: Elapsed 0.73
[2022-06-16 11:38:13] __main__ INFO: Train 77 2964
[2022-06-16 11:38:14] __main__ INFO: Epoch 77 Step 39/39 lr 0.100000 loss 0.5921 (0.6143) acc@1 0.7578 (0.7857) acc@5 1.0000 (0.9906)
[2022-06-16 11:38:14] __main__ INFO: Elapsed 1.62
[2022-06-16 11:38:14] __main__ INFO: Val 77
[2022-06-16 11:38:15] __main__ INFO: Epoch 77 loss 1.3390 acc@1 0.6068 acc@5 0.9611
[2022-06-16 11:38:15] __main__ INFO: Elapsed 0.78
[2022-06-16 11:38:15] __main__ INFO: Train 78 3003
[2022-06-16 11:38:17] __main__ INFO: Epoch 78 Step 39/39 lr 0.100000 loss 0.5971 (0.5585) acc@1 0.7656 (0.7999) acc@5 0.9922 (0.9926)
[2022-06-16 11:38:17] __main__ INFO: Elapsed 1.66
[2022-06-16 11:38:17] __main__ INFO: Val 78
[2022-06-16 11:38:18] __main__ INFO: Epoch 78 loss 1.4221 acc@1 0.6073 acc@5 0.9536
[2022-06-16 11:38:18] __main__ INFO: Elapsed 0.74
[2022-06-16 11:38:18] __main__ INFO: Train 79 3042
[2022-06-16 11:38:19] __main__ INFO: Epoch 79 Step 39/39 lr 0.100000 loss 0.6637 (0.5337) acc@1 0.7578 (0.8037) acc@5 0.9844 (0.9930)
[2022-06-16 11:38:19] __main__ INFO: Elapsed 1.73
[2022-06-16 11:38:19] __main__ INFO: Val 79
[2022-06-16 11:38:20] __main__ INFO: Epoch 79 loss 1.2696 acc@1 0.6323 acc@5 0.9605
[2022-06-16 11:38:20] __main__ INFO: Elapsed 0.84
[2022-06-16 11:38:20] __main__ INFO: Train 80 3081
[2022-06-16 11:38:22] __main__ INFO: Epoch 80 Step 39/39 lr 0.100000 loss 0.5369 (0.5473) acc@1 0.8281 (0.8037) acc@5 0.9922 (0.9938)
[2022-06-16 11:38:22] __main__ INFO: Elapsed 1.65
[2022-06-16 11:38:22] __main__ INFO: Val 80
[2022-06-16 11:38:22] __main__ INFO: Epoch 80 loss 1.5722 acc@1 0.5771 acc@5 0.9443
[2022-06-16 11:38:22] __main__ INFO: Elapsed 0.70
[2022-06-16 11:38:22] __main__ INFO: Train 81 3120
[2022-06-16 11:38:24] __main__ INFO: Epoch 81 Step 39/39 lr 0.010000 loss 0.4722 (0.4518) acc@1 0.8047 (0.8431) acc@5 0.9922 (0.9948)
[2022-06-16 11:38:24] __main__ INFO: Elapsed 1.67
[2022-06-16 11:38:24] __main__ INFO: Val 81
[2022-06-16 11:38:25] __main__ INFO: Epoch 81 loss 1.1671 acc@1 0.6661 acc@5 0.9686
[2022-06-16 11:38:25] __main__ INFO: Elapsed 0.68
[2022-06-16 11:38:25] __main__ INFO: Train 82 3159
[2022-06-16 11:38:26] __main__ INFO: Epoch 82 Step 39/39 lr 0.010000 loss 0.3196 (0.3508) acc@1 0.8750 (0.8814) acc@5 1.0000 (0.9966)
[2022-06-16 11:38:27] __main__ INFO: Elapsed 1.67
[2022-06-16 11:38:27] __main__ INFO: Val 82
[2022-06-16 11:38:27] __main__ INFO: Epoch 82 loss 1.1749 acc@1 0.6666 acc@5 0.9691
[2022-06-16 11:38:27] __main__ INFO: Elapsed 0.79
[2022-06-16 11:38:27] __main__ INFO: Train 83 3198
[2022-06-16 11:38:29] __main__ INFO: Epoch 83 Step 39/39 lr 0.010000 loss 0.2929 (0.3249) acc@1 0.9062 (0.8886) acc@5 1.0000 (0.9988)
[2022-06-16 11:38:29] __main__ INFO: Elapsed 1.66
[2022-06-16 11:38:29] __main__ INFO: Val 83
[2022-06-16 11:38:30] __main__ INFO: Epoch 83 loss 1.1958 acc@1 0.6649 acc@5 0.9679
[2022-06-16 11:38:30] __main__ INFO: Elapsed 0.82
[2022-06-16 11:38:30] __main__ INFO: Train 84 3237
[2022-06-16 11:38:31] __main__ INFO: Epoch 84 Step 39/39 lr 0.010000 loss 0.3361 (0.3074) acc@1 0.8984 (0.8954) acc@5 1.0000 (0.9986)
[2022-06-16 11:38:31] __main__ INFO: Elapsed 1.69
[2022-06-16 11:38:31] __main__ INFO: Val 84
[2022-06-16 11:38:32] __main__ INFO: Epoch 84 loss 1.2115 acc@1 0.6674 acc@5 0.9676
[2022-06-16 11:38:32] __main__ INFO: Elapsed 0.75
[2022-06-16 11:38:32] __main__ INFO: Train 85 3276
[2022-06-16 11:38:34] __main__ INFO: Epoch 85 Step 39/39 lr 0.010000 loss 0.2346 (0.2880) acc@1 0.8984 (0.9026) acc@5 1.0000 (0.9986)
[2022-06-16 11:38:34] __main__ INFO: Elapsed 1.64
[2022-06-16 11:38:34] __main__ INFO: Val 85
[2022-06-16 11:38:35] __main__ INFO: Epoch 85 loss 1.2412 acc@1 0.6623 acc@5 0.9667
[2022-06-16 11:38:35] __main__ INFO: Elapsed 0.79
[2022-06-16 11:38:35] __main__ INFO: Train 86 3315
[2022-06-16 11:38:36] __main__ INFO: Epoch 86 Step 39/39 lr 0.010000 loss 0.2856 (0.2709) acc@1 0.9062 (0.9087) acc@5 0.9922 (0.9986)
[2022-06-16 11:38:36] __main__ INFO: Elapsed 1.65
[2022-06-16 11:38:36] __main__ INFO: Val 86
[2022-06-16 11:38:37] __main__ INFO: Epoch 86 loss 1.2519 acc@1 0.6617 acc@5 0.9674
[2022-06-16 11:38:37] __main__ INFO: Elapsed 0.79
[2022-06-16 11:38:37] __main__ INFO: Train 87 3354
[2022-06-16 11:38:39] __main__ INFO: Epoch 87 Step 39/39 lr 0.010000 loss 0.2271 (0.2592) acc@1 0.9141 (0.9079) acc@5 1.0000 (0.9984)
[2022-06-16 11:38:39] __main__ INFO: Elapsed 1.63
[2022-06-16 11:38:39] __main__ INFO: Val 87
[2022-06-16 11:38:40] __main__ INFO: Epoch 87 loss 1.2806 acc@1 0.6629 acc@5 0.9688
[2022-06-16 11:38:40] __main__ INFO: Elapsed 0.78
[2022-06-16 11:38:40] __main__ INFO: Train 88 3393
[2022-06-16 11:38:41] __main__ INFO: Epoch 88 Step 39/39 lr 0.010000 loss 0.2282 (0.2573) acc@1 0.9297 (0.9111) acc@5 1.0000 (0.9988)
[2022-06-16 11:38:41] __main__ INFO: Elapsed 1.68
[2022-06-16 11:38:41] __main__ INFO: Val 88
[2022-06-16 11:38:42] __main__ INFO: Epoch 88 loss 1.2834 acc@1 0.6624 acc@5 0.9676
[2022-06-16 11:38:42] __main__ INFO: Elapsed 0.76
[2022-06-16 11:38:42] __main__ INFO: Train 89 3432
[2022-06-16 11:38:43] __main__ INFO: Epoch 89 Step 39/39 lr 0.010000 loss 0.2683 (0.2532) acc@1 0.9219 (0.9141) acc@5 0.9922 (0.9984)
[2022-06-16 11:38:44] __main__ INFO: Elapsed 1.52
[2022-06-16 11:38:44] __main__ INFO: Val 89
[2022-06-16 11:38:44] __main__ INFO: Epoch 89 loss 1.2930 acc@1 0.6652 acc@5 0.9677
[2022-06-16 11:38:44] __main__ INFO: Elapsed 0.77
[2022-06-16 11:38:44] __main__ INFO: Train 90 3471
[2022-06-16 11:38:46] __main__ INFO: Epoch 90 Step 39/39 lr 0.010000 loss 0.2990 (0.2342) acc@1 0.9219 (0.9239) acc@5 0.9922 (0.9990)
[2022-06-16 11:38:46] __main__ INFO: Elapsed 1.72
[2022-06-16 11:38:46] __main__ INFO: Val 90
[2022-06-16 11:38:47] __main__ INFO: Epoch 90 loss 1.3379 acc@1 0.6637 acc@5 0.9665
[2022-06-16 11:38:47] __main__ INFO: Elapsed 0.76
[2022-06-16 11:38:47] __main__ INFO: Train 91 3510
[2022-06-16 11:38:48] __main__ INFO: Epoch 91 Step 39/39 lr 0.010000 loss 0.1627 (0.2291) acc@1 0.9531 (0.9219) acc@5 1.0000 (0.9992)
[2022-06-16 11:38:48] __main__ INFO: Elapsed 1.61
[2022-06-16 11:38:48] __main__ INFO: Val 91
[2022-06-16 11:38:49] __main__ INFO: Epoch 91 loss 1.3242 acc@1 0.6655 acc@5 0.9651
[2022-06-16 11:38:49] __main__ INFO: Elapsed 0.80
[2022-06-16 11:38:49] __main__ INFO: Train 92 3549
[2022-06-16 11:38:51] __main__ INFO: Epoch 92 Step 39/39 lr 0.010000 loss 0.1924 (0.2338) acc@1 0.9375 (0.9217) acc@5 1.0000 (0.9994)
[2022-06-16 11:38:51] __main__ INFO: Elapsed 1.66
[2022-06-16 11:38:51] __main__ INFO: Val 92
[2022-06-16 11:38:52] __main__ INFO: Epoch 92 loss 1.3628 acc@1 0.6619 acc@5 0.9655
[2022-06-16 11:38:52] __main__ INFO: Elapsed 0.76
[2022-06-16 11:38:52] __main__ INFO: Train 93 3588
[2022-06-16 11:38:53] __main__ INFO: Epoch 93 Step 39/39 lr 0.010000 loss 0.2654 (0.2180) acc@1 0.8672 (0.9277) acc@5 1.0000 (0.9996)
[2022-06-16 11:38:53] __main__ INFO: Elapsed 1.66
[2022-06-16 11:38:53] __main__ INFO: Val 93
[2022-06-16 11:38:54] __main__ INFO: Epoch 93 loss 1.3606 acc@1 0.6624 acc@5 0.9674
[2022-06-16 11:38:54] __main__ INFO: Elapsed 0.80
[2022-06-16 11:38:54] __main__ INFO: Train 94 3627
[2022-06-16 11:38:56] __main__ INFO: Epoch 94 Step 39/39 lr 0.010000 loss 0.2002 (0.2140) acc@1 0.9453 (0.9285) acc@5 1.0000 (0.9988)
[2022-06-16 11:38:56] __main__ INFO: Elapsed 1.67
[2022-06-16 11:38:56] __main__ INFO: Val 94
[2022-06-16 11:38:56] __main__ INFO: Epoch 94 loss 1.3732 acc@1 0.6652 acc@5 0.9650
[2022-06-16 11:38:56] __main__ INFO: Elapsed 0.75
[2022-06-16 11:38:57] __main__ INFO: Train 95 3666
[2022-06-16 11:38:58] __main__ INFO: Epoch 95 Step 39/39 lr 0.010000 loss 0.2367 (0.2039) acc@1 0.9219 (0.9299) acc@5 1.0000 (0.9994)
[2022-06-16 11:38:58] __main__ INFO: Elapsed 1.72
[2022-06-16 11:38:58] __main__ INFO: Val 95
[2022-06-16 11:38:59] __main__ INFO: Epoch 95 loss 1.3953 acc@1 0.6607 acc@5 0.9652
[2022-06-16 11:38:59] __main__ INFO: Elapsed 0.76
[2022-06-16 11:38:59] __main__ INFO: Train 96 3705
[2022-06-16 11:39:01] __main__ INFO: Epoch 96 Step 39/39 lr 0.010000 loss 0.1537 (0.2030) acc@1 0.9453 (0.9265) acc@5 1.0000 (0.9994)
[2022-06-16 11:39:01] __main__ INFO: Elapsed 1.73
[2022-06-16 11:39:01] __main__ INFO: Val 96
[2022-06-16 11:39:02] __main__ INFO: Epoch 96 loss 1.4260 acc@1 0.6568 acc@5 0.9658
[2022-06-16 11:39:02] __main__ INFO: Elapsed 0.82
[2022-06-16 11:39:02] __main__ INFO: Train 97 3744
[2022-06-16 11:39:03] __main__ INFO: Epoch 97 Step 39/39 lr 0.010000 loss 0.2042 (0.1990) acc@1 0.9375 (0.9331) acc@5 1.0000 (0.9992)
[2022-06-16 11:39:03] __main__ INFO: Elapsed 1.66
[2022-06-16 11:39:03] __main__ INFO: Val 97
[2022-06-16 11:39:04] __main__ INFO: Epoch 97 loss 1.4113 acc@1 0.6611 acc@5 0.9660
[2022-06-16 11:39:04] __main__ INFO: Elapsed 0.68
[2022-06-16 11:39:04] __main__ INFO: Train 98 3783
[2022-06-16 11:39:06] __main__ INFO: Epoch 98 Step 39/39 lr 0.010000 loss 0.1835 (0.1898) acc@1 0.9297 (0.9329) acc@5 1.0000 (0.9996)
[2022-06-16 11:39:06] __main__ INFO: Elapsed 1.70
[2022-06-16 11:39:06] __main__ INFO: Val 98
[2022-06-16 11:39:06] __main__ INFO: Epoch 98 loss 1.4561 acc@1 0.6595 acc@5 0.9656
[2022-06-16 11:39:06] __main__ INFO: Elapsed 0.77
[2022-06-16 11:39:06] __main__ INFO: Train 99 3822
[2022-06-16 11:39:08] __main__ INFO: Epoch 99 Step 39/39 lr 0.010000 loss 0.2236 (0.1792) acc@1 0.9219 (0.9401) acc@5 1.0000 (0.9994)
[2022-06-16 11:39:08] __main__ INFO: Elapsed 1.59
[2022-06-16 11:39:08] __main__ INFO: Val 99
[2022-06-16 11:39:09] __main__ INFO: Epoch 99 loss 1.4901 acc@1 0.6564 acc@5 0.9646
[2022-06-16 11:39:09] __main__ INFO: Elapsed 0.77
[2022-06-16 11:39:09] __main__ INFO: Train 100 3861
[2022-06-16 11:39:10] __main__ INFO: Epoch 100 Step 39/39 lr 0.010000 loss 0.1820 (0.1814) acc@1 0.9609 (0.9379) acc@5 1.0000 (1.0000)
[2022-06-16 11:39:10] __main__ INFO: Elapsed 1.58
[2022-06-16 11:39:10] __main__ INFO: Val 100
[2022-06-16 11:39:11] __main__ INFO: Epoch 100 loss 1.4952 acc@1 0.6579 acc@5 0.9651
[2022-06-16 11:39:11] __main__ INFO: Elapsed 0.75
[2022-06-16 11:39:11] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00100.pth
[2022-06-16 11:39:11] __main__ INFO: Train 101 3900
[2022-06-16 11:39:13] __main__ INFO: Epoch 101 Step 39/39 lr 0.010000 loss 0.2718 (0.1858) acc@1 0.9141 (0.9333) acc@5 1.0000 (1.0000)
[2022-06-16 11:39:13] __main__ INFO: Elapsed 1.72
[2022-06-16 11:39:13] __main__ INFO: Val 101
[2022-06-16 11:39:14] __main__ INFO: Epoch 101 loss 1.4956 acc@1 0.6582 acc@5 0.9646
[2022-06-16 11:39:14] __main__ INFO: Elapsed 0.84
[2022-06-16 11:39:14] __main__ INFO: Train 102 3939
[2022-06-16 11:39:15] __main__ INFO: Epoch 102 Step 39/39 lr 0.010000 loss 0.2194 (0.1723) acc@1 0.9141 (0.9379) acc@5 1.0000 (0.9994)
[2022-06-16 11:39:15] __main__ INFO: Elapsed 1.67
[2022-06-16 11:39:15] __main__ INFO: Val 102
[2022-06-16 11:39:16] __main__ INFO: Epoch 102 loss 1.5142 acc@1 0.6570 acc@5 0.9665
[2022-06-16 11:39:16] __main__ INFO: Elapsed 0.74
[2022-06-16 11:39:16] __main__ INFO: Train 103 3978
[2022-06-16 11:39:18] __main__ INFO: Epoch 103 Step 39/39 lr 0.010000 loss 0.2575 (0.1648) acc@1 0.8906 (0.9473) acc@5 1.0000 (1.0000)
[2022-06-16 11:39:18] __main__ INFO: Elapsed 1.65
[2022-06-16 11:39:18] __main__ INFO: Val 103
[2022-06-16 11:39:19] __main__ INFO: Epoch 103 loss 1.5183 acc@1 0.6563 acc@5 0.9657
[2022-06-16 11:39:19] __main__ INFO: Elapsed 0.74
[2022-06-16 11:39:19] __main__ INFO: Train 104 4017
[2022-06-16 11:39:20] __main__ INFO: Epoch 104 Step 39/39 lr 0.010000 loss 0.1093 (0.1555) acc@1 0.9844 (0.9505) acc@5 1.0000 (0.9996)
[2022-06-16 11:39:20] __main__ INFO: Elapsed 1.64
[2022-06-16 11:39:20] __main__ INFO: Val 104
[2022-06-16 11:39:21] __main__ INFO: Epoch 104 loss 1.5485 acc@1 0.6524 acc@5 0.9643
[2022-06-16 11:39:21] __main__ INFO: Elapsed 0.74
[2022-06-16 11:39:21] __main__ INFO: Train 105 4056
[2022-06-16 11:39:23] __main__ INFO: Epoch 105 Step 39/39 lr 0.010000 loss 0.0965 (0.1536) acc@1 0.9609 (0.9497) acc@5 1.0000 (0.9998)
[2022-06-16 11:39:23] __main__ INFO: Elapsed 1.67
[2022-06-16 11:39:23] __main__ INFO: Val 105
[2022-06-16 11:39:23] __main__ INFO: Epoch 105 loss 1.5531 acc@1 0.6587 acc@5 0.9657
[2022-06-16 11:39:23] __main__ INFO: Elapsed 0.79
[2022-06-16 11:39:23] __main__ INFO: Train 106 4095
[2022-06-16 11:39:25] __main__ INFO: Epoch 106 Step 39/39 lr 0.010000 loss 0.1863 (0.1436) acc@1 0.9219 (0.9517) acc@5 1.0000 (0.9998)
[2022-06-16 11:39:25] __main__ INFO: Elapsed 1.70
[2022-06-16 11:39:25] __main__ INFO: Val 106
[2022-06-16 11:39:26] __main__ INFO: Epoch 106 loss 1.5713 acc@1 0.6527 acc@5 0.9649
[2022-06-16 11:39:26] __main__ INFO: Elapsed 0.72
[2022-06-16 11:39:26] __main__ INFO: Train 107 4134
[2022-06-16 11:39:27] __main__ INFO: Epoch 107 Step 39/39 lr 0.010000 loss 0.1604 (0.1468) acc@1 0.9297 (0.9539) acc@5 1.0000 (0.9996)
[2022-06-16 11:39:27] __main__ INFO: Elapsed 1.62
[2022-06-16 11:39:27] __main__ INFO: Val 107
[2022-06-16 11:39:28] __main__ INFO: Epoch 107 loss 1.5850 acc@1 0.6551 acc@5 0.9656
[2022-06-16 11:39:28] __main__ INFO: Elapsed 0.74
[2022-06-16 11:39:28] __main__ INFO: Train 108 4173
[2022-06-16 11:39:30] __main__ INFO: Epoch 108 Step 39/39 lr 0.010000 loss 0.1439 (0.1437) acc@1 0.9609 (0.9549) acc@5 1.0000 (0.9996)
[2022-06-16 11:39:30] __main__ INFO: Elapsed 1.65
[2022-06-16 11:39:30] __main__ INFO: Val 108
[2022-06-16 11:39:31] __main__ INFO: Epoch 108 loss 1.6021 acc@1 0.6577 acc@5 0.9657
[2022-06-16 11:39:31] __main__ INFO: Elapsed 0.72
[2022-06-16 11:39:31] __main__ INFO: Train 109 4212
[2022-06-16 11:39:32] __main__ INFO: Epoch 109 Step 39/39 lr 0.010000 loss 0.1607 (0.1394) acc@1 0.9297 (0.9545) acc@5 1.0000 (0.9998)
[2022-06-16 11:39:32] __main__ INFO: Elapsed 1.63
[2022-06-16 11:39:32] __main__ INFO: Val 109
[2022-06-16 11:39:33] __main__ INFO: Epoch 109 loss 1.6284 acc@1 0.6565 acc@5 0.9634
[2022-06-16 11:39:33] __main__ INFO: Elapsed 0.75
[2022-06-16 11:39:33] __main__ INFO: Train 110 4251
[2022-06-16 11:39:34] __main__ INFO: Epoch 110 Step 39/39 lr 0.010000 loss 0.1373 (0.1391) acc@1 0.9453 (0.9489) acc@5 1.0000 (1.0000)
[2022-06-16 11:39:35] __main__ INFO: Elapsed 1.62
[2022-06-16 11:39:35] __main__ INFO: Val 110
[2022-06-16 11:39:35] __main__ INFO: Epoch 110 loss 1.6310 acc@1 0.6536 acc@5 0.9636
[2022-06-16 11:39:35] __main__ INFO: Elapsed 0.79
[2022-06-16 11:39:35] __main__ INFO: Train 111 4290
[2022-06-16 11:39:37] __main__ INFO: Epoch 111 Step 39/39 lr 0.010000 loss 0.0954 (0.1394) acc@1 0.9609 (0.9509) acc@5 1.0000 (0.9996)
[2022-06-16 11:39:37] __main__ INFO: Elapsed 1.71
[2022-06-16 11:39:37] __main__ INFO: Val 111
[2022-06-16 11:39:38] __main__ INFO: Epoch 111 loss 1.6144 acc@1 0.6526 acc@5 0.9653
[2022-06-16 11:39:38] __main__ INFO: Elapsed 0.73
[2022-06-16 11:39:38] __main__ INFO: Train 112 4329
[2022-06-16 11:39:39] __main__ INFO: Epoch 112 Step 39/39 lr 0.010000 loss 0.1707 (0.1193) acc@1 0.9375 (0.9607) acc@5 1.0000 (0.9998)
[2022-06-16 11:39:39] __main__ INFO: Elapsed 1.64
[2022-06-16 11:39:39] __main__ INFO: Val 112
[2022-06-16 11:39:40] __main__ INFO: Epoch 112 loss 1.6728 acc@1 0.6551 acc@5 0.9659
[2022-06-16 11:39:40] __main__ INFO: Elapsed 0.76
[2022-06-16 11:39:40] __main__ INFO: Train 113 4368
[2022-06-16 11:39:42] __main__ INFO: Epoch 113 Step 39/39 lr 0.010000 loss 0.1429 (0.1233) acc@1 0.9297 (0.9595) acc@5 1.0000 (0.9998)
[2022-06-16 11:39:42] __main__ INFO: Elapsed 1.66
[2022-06-16 11:39:42] __main__ INFO: Val 113
[2022-06-16 11:39:43] __main__ INFO: Epoch 113 loss 1.6784 acc@1 0.6496 acc@5 0.9651
[2022-06-16 11:39:43] __main__ INFO: Elapsed 0.75
[2022-06-16 11:39:43] __main__ INFO: Train 114 4407
[2022-06-16 11:39:44] __main__ INFO: Epoch 114 Step 39/39 lr 0.010000 loss 0.1541 (0.1202) acc@1 0.9531 (0.9603) acc@5 1.0000 (1.0000)
[2022-06-16 11:39:44] __main__ INFO: Elapsed 1.65
[2022-06-16 11:39:44] __main__ INFO: Val 114
[2022-06-16 11:39:45] __main__ INFO: Epoch 114 loss 1.6956 acc@1 0.6539 acc@5 0.9637
[2022-06-16 11:39:45] __main__ INFO: Elapsed 0.75
[2022-06-16 11:39:45] __main__ INFO: Train 115 4446
[2022-06-16 11:39:47] __main__ INFO: Epoch 115 Step 39/39 lr 0.010000 loss 0.1677 (0.1158) acc@1 0.9375 (0.9635) acc@5 1.0000 (1.0000)
[2022-06-16 11:39:47] __main__ INFO: Elapsed 1.69
[2022-06-16 11:39:47] __main__ INFO: Val 115
[2022-06-16 11:39:48] __main__ INFO: Epoch 115 loss 1.6785 acc@1 0.6529 acc@5 0.9659
[2022-06-16 11:39:48] __main__ INFO: Elapsed 0.82
[2022-06-16 11:39:48] __main__ INFO: Train 116 4485
[2022-06-16 11:39:49] __main__ INFO: Epoch 116 Step 39/39 lr 0.010000 loss 0.1350 (0.1231) acc@1 0.9688 (0.9607) acc@5 1.0000 (0.9998)
[2022-06-16 11:39:49] __main__ INFO: Elapsed 1.66
[2022-06-16 11:39:49] __main__ INFO: Val 116
[2022-06-16 11:39:50] __main__ INFO: Epoch 116 loss 1.7096 acc@1 0.6551 acc@5 0.9640
[2022-06-16 11:39:50] __main__ INFO: Elapsed 0.76
[2022-06-16 11:39:50] __main__ INFO: Train 117 4524
[2022-06-16 11:39:52] __main__ INFO: Epoch 117 Step 39/39 lr 0.010000 loss 0.2274 (0.1222) acc@1 0.9062 (0.9593) acc@5 1.0000 (0.9998)
[2022-06-16 11:39:52] __main__ INFO: Elapsed 1.66
[2022-06-16 11:39:52] __main__ INFO: Val 117
[2022-06-16 11:39:52] __main__ INFO: Epoch 117 loss 1.6931 acc@1 0.6531 acc@5 0.9659
[2022-06-16 11:39:52] __main__ INFO: Elapsed 0.73
[2022-06-16 11:39:52] __main__ INFO: Train 118 4563
[2022-06-16 11:39:54] __main__ INFO: Epoch 118 Step 39/39 lr 0.010000 loss 0.1064 (0.1100) acc@1 0.9688 (0.9621) acc@5 1.0000 (1.0000)
[2022-06-16 11:39:54] __main__ INFO: Elapsed 1.64
[2022-06-16 11:39:54] __main__ INFO: Val 118
[2022-06-16 11:39:55] __main__ INFO: Epoch 118 loss 1.7889 acc@1 0.6448 acc@5 0.9637
[2022-06-16 11:39:55] __main__ INFO: Elapsed 0.77
[2022-06-16 11:39:55] __main__ INFO: Train 119 4602
[2022-06-16 11:39:56] __main__ INFO: Epoch 119 Step 39/39 lr 0.010000 loss 0.1607 (0.1090) acc@1 0.9297 (0.9677) acc@5 1.0000 (1.0000)
[2022-06-16 11:39:56] __main__ INFO: Elapsed 1.69
[2022-06-16 11:39:56] __main__ INFO: Val 119
[2022-06-16 11:39:57] __main__ INFO: Epoch 119 loss 1.7058 acc@1 0.6552 acc@5 0.9651
[2022-06-16 11:39:57] __main__ INFO: Elapsed 0.68
[2022-06-16 11:39:57] __main__ INFO: Train 120 4641
[2022-06-16 11:39:59] __main__ INFO: Epoch 120 Step 39/39 lr 0.010000 loss 0.1166 (0.1089) acc@1 0.9609 (0.9653) acc@5 1.0000 (0.9998)
[2022-06-16 11:39:59] __main__ INFO: Elapsed 1.69
[2022-06-16 11:39:59] __main__ INFO: Val 120
[2022-06-16 11:40:00] __main__ INFO: Epoch 120 loss 1.7577 acc@1 0.6556 acc@5 0.9639
[2022-06-16 11:40:00] __main__ INFO: Elapsed 0.71
[2022-06-16 11:40:00] __main__ INFO: Train 121 4680
[2022-06-16 11:40:01] __main__ INFO: Epoch 121 Step 39/39 lr 0.001000 loss 0.1367 (0.0988) acc@1 0.9531 (0.9667) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:01] __main__ INFO: Elapsed 1.66
[2022-06-16 11:40:01] __main__ INFO: Val 121
[2022-06-16 11:40:02] __main__ INFO: Epoch 121 loss 1.7203 acc@1 0.6555 acc@5 0.9648
[2022-06-16 11:40:02] __main__ INFO: Elapsed 0.73
[2022-06-16 11:40:02] __main__ INFO: Train 122 4719
[2022-06-16 11:40:04] __main__ INFO: Epoch 122 Step 39/39 lr 0.001000 loss 0.0834 (0.0934) acc@1 0.9766 (0.9718) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:04] __main__ INFO: Elapsed 1.72
[2022-06-16 11:40:04] __main__ INFO: Val 122
[2022-06-16 11:40:04] __main__ INFO: Epoch 122 loss 1.7062 acc@1 0.6603 acc@5 0.9649
[2022-06-16 11:40:04] __main__ INFO: Elapsed 0.75
[2022-06-16 11:40:04] __main__ INFO: Train 123 4758
[2022-06-16 11:40:06] __main__ INFO: Epoch 123 Step 39/39 lr 0.001000 loss 0.1257 (0.0794) acc@1 0.9531 (0.9788) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:06] __main__ INFO: Elapsed 1.67
[2022-06-16 11:40:06] __main__ INFO: Val 123
[2022-06-16 11:40:07] __main__ INFO: Epoch 123 loss 1.7081 acc@1 0.6592 acc@5 0.9660
[2022-06-16 11:40:07] __main__ INFO: Elapsed 0.80
[2022-06-16 11:40:07] __main__ INFO: Train 124 4797
[2022-06-16 11:40:09] __main__ INFO: Epoch 124 Step 39/39 lr 0.001000 loss 0.1016 (0.0951) acc@1 0.9844 (0.9716) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:09] __main__ INFO: Elapsed 1.72
[2022-06-16 11:40:09] __main__ INFO: Val 124
[2022-06-16 11:40:09] __main__ INFO: Epoch 124 loss 1.7210 acc@1 0.6585 acc@5 0.9645
[2022-06-16 11:40:09] __main__ INFO: Elapsed 0.78
[2022-06-16 11:40:09] __main__ INFO: Train 125 4836
[2022-06-16 11:40:11] __main__ INFO: Epoch 125 Step 39/39 lr 0.001000 loss 0.0713 (0.0808) acc@1 0.9844 (0.9784) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:11] __main__ INFO: Elapsed 1.69
[2022-06-16 11:40:11] __main__ INFO: Val 125
[2022-06-16 11:40:12] __main__ INFO: Epoch 125 loss 1.7163 acc@1 0.6578 acc@5 0.9654
[2022-06-16 11:40:12] __main__ INFO: Elapsed 0.75
[2022-06-16 11:40:12] __main__ INFO: Train 126 4875
[2022-06-16 11:40:13] __main__ INFO: Epoch 126 Step 39/39 lr 0.001000 loss 0.1357 (0.0885) acc@1 0.9609 (0.9736) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:14] __main__ INFO: Elapsed 1.71
[2022-06-16 11:40:14] __main__ INFO: Val 126
[2022-06-16 11:40:14] __main__ INFO: Epoch 126 loss 1.7219 acc@1 0.6588 acc@5 0.9648
[2022-06-16 11:40:14] __main__ INFO: Elapsed 0.66
[2022-06-16 11:40:14] __main__ INFO: Train 127 4914
[2022-06-16 11:40:16] __main__ INFO: Epoch 127 Step 39/39 lr 0.001000 loss 0.0767 (0.0889) acc@1 0.9766 (0.9738) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:16] __main__ INFO: Elapsed 1.64
[2022-06-16 11:40:16] __main__ INFO: Val 127
[2022-06-16 11:40:17] __main__ INFO: Epoch 127 loss 1.7240 acc@1 0.6604 acc@5 0.9652
[2022-06-16 11:40:17] __main__ INFO: Elapsed 0.77
[2022-06-16 11:40:17] __main__ INFO: Train 128 4953
[2022-06-16 11:40:18] __main__ INFO: Epoch 128 Step 39/39 lr 0.001000 loss 0.0508 (0.0816) acc@1 0.9922 (0.9748) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:18] __main__ INFO: Elapsed 1.67
[2022-06-16 11:40:18] __main__ INFO: Val 128
[2022-06-16 11:40:19] __main__ INFO: Epoch 128 loss 1.7173 acc@1 0.6599 acc@5 0.9648
[2022-06-16 11:40:19] __main__ INFO: Elapsed 0.72
[2022-06-16 11:40:19] __main__ INFO: Train 129 4992
[2022-06-16 11:40:21] __main__ INFO: Epoch 129 Step 39/39 lr 0.001000 loss 0.0501 (0.0869) acc@1 0.9922 (0.9732) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:21] __main__ INFO: Elapsed 1.63
[2022-06-16 11:40:21] __main__ INFO: Val 129
[2022-06-16 11:40:21] __main__ INFO: Epoch 129 loss 1.7247 acc@1 0.6602 acc@5 0.9642
[2022-06-16 11:40:21] __main__ INFO: Elapsed 0.68
[2022-06-16 11:40:21] __main__ INFO: Train 130 5031
[2022-06-16 11:40:23] __main__ INFO: Epoch 130 Step 39/39 lr 0.001000 loss 0.1269 (0.0799) acc@1 0.9531 (0.9748) acc@5 1.0000 (0.9998)
[2022-06-16 11:40:23] __main__ INFO: Elapsed 1.68
[2022-06-16 11:40:23] __main__ INFO: Val 130
[2022-06-16 11:40:24] __main__ INFO: Epoch 130 loss 1.7289 acc@1 0.6581 acc@5 0.9641
[2022-06-16 11:40:24] __main__ INFO: Elapsed 0.76
[2022-06-16 11:40:24] __main__ INFO: Train 131 5070
[2022-06-16 11:40:25] __main__ INFO: Epoch 131 Step 39/39 lr 0.001000 loss 0.0580 (0.0786) acc@1 1.0000 (0.9778) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:25] __main__ INFO: Elapsed 1.61
[2022-06-16 11:40:25] __main__ INFO: Val 131
[2022-06-16 11:40:26] __main__ INFO: Epoch 131 loss 1.7341 acc@1 0.6598 acc@5 0.9632
[2022-06-16 11:40:26] __main__ INFO: Elapsed 0.69
[2022-06-16 11:40:26] __main__ INFO: Train 132 5109
[2022-06-16 11:40:28] __main__ INFO: Epoch 132 Step 39/39 lr 0.001000 loss 0.0454 (0.0728) acc@1 0.9922 (0.9804) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:28] __main__ INFO: Elapsed 1.67
[2022-06-16 11:40:28] __main__ INFO: Val 132
[2022-06-16 11:40:28] __main__ INFO: Epoch 132 loss 1.7362 acc@1 0.6584 acc@5 0.9631
[2022-06-16 11:40:28] __main__ INFO: Elapsed 0.71
[2022-06-16 11:40:28] __main__ INFO: Train 133 5148
[2022-06-16 11:40:30] __main__ INFO: Epoch 133 Step 39/39 lr 0.001000 loss 0.0668 (0.0829) acc@1 0.9844 (0.9758) acc@5 1.0000 (0.9998)
[2022-06-16 11:40:30] __main__ INFO: Elapsed 1.66
[2022-06-16 11:40:30] __main__ INFO: Val 133
[2022-06-16 11:40:31] __main__ INFO: Epoch 133 loss 1.7340 acc@1 0.6593 acc@5 0.9635
[2022-06-16 11:40:31] __main__ INFO: Elapsed 0.82
[2022-06-16 11:40:31] __main__ INFO: Train 134 5187
[2022-06-16 11:40:32] __main__ INFO: Epoch 134 Step 39/39 lr 0.001000 loss 0.1175 (0.0870) acc@1 0.9688 (0.9720) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:33] __main__ INFO: Elapsed 1.60
[2022-06-16 11:40:33] __main__ INFO: Val 134
[2022-06-16 11:40:33] __main__ INFO: Epoch 134 loss 1.7402 acc@1 0.6587 acc@5 0.9642
[2022-06-16 11:40:33] __main__ INFO: Elapsed 0.77
[2022-06-16 11:40:33] __main__ INFO: Train 135 5226
[2022-06-16 11:40:35] __main__ INFO: Epoch 135 Step 39/39 lr 0.001000 loss 0.0572 (0.0755) acc@1 0.9766 (0.9776) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:35] __main__ INFO: Elapsed 1.65
[2022-06-16 11:40:35] __main__ INFO: Val 135
[2022-06-16 11:40:36] __main__ INFO: Epoch 135 loss 1.7356 acc@1 0.6592 acc@5 0.9633
[2022-06-16 11:40:36] __main__ INFO: Elapsed 0.79
[2022-06-16 11:40:36] __main__ INFO: Train 136 5265
[2022-06-16 11:40:37] __main__ INFO: Epoch 136 Step 39/39 lr 0.001000 loss 0.0447 (0.0796) acc@1 1.0000 (0.9788) acc@5 1.0000 (0.9998)
[2022-06-16 11:40:37] __main__ INFO: Elapsed 1.73
[2022-06-16 11:40:37] __main__ INFO: Val 136
[2022-06-16 11:40:38] __main__ INFO: Epoch 136 loss 1.7411 acc@1 0.6573 acc@5 0.9629
[2022-06-16 11:40:38] __main__ INFO: Elapsed 0.69
[2022-06-16 11:40:38] __main__ INFO: Train 137 5304
[2022-06-16 11:40:40] __main__ INFO: Epoch 137 Step 39/39 lr 0.001000 loss 0.0932 (0.0821) acc@1 0.9766 (0.9758) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:40] __main__ INFO: Elapsed 1.66
[2022-06-16 11:40:40] __main__ INFO: Val 137
[2022-06-16 11:40:41] __main__ INFO: Epoch 137 loss 1.7490 acc@1 0.6574 acc@5 0.9638
[2022-06-16 11:40:41] __main__ INFO: Elapsed 0.80
[2022-06-16 11:40:41] __main__ INFO: Train 138 5343
[2022-06-16 11:40:42] __main__ INFO: Epoch 138 Step 39/39 lr 0.001000 loss 0.0644 (0.0824) acc@1 0.9844 (0.9768) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:42] __main__ INFO: Elapsed 1.67
[2022-06-16 11:40:42] __main__ INFO: Val 138
[2022-06-16 11:40:43] __main__ INFO: Epoch 138 loss 1.7452 acc@1 0.6580 acc@5 0.9637
[2022-06-16 11:40:43] __main__ INFO: Elapsed 0.71
[2022-06-16 11:40:43] __main__ INFO: Train 139 5382
[2022-06-16 11:40:45] __main__ INFO: Epoch 139 Step 39/39 lr 0.001000 loss 0.0861 (0.0748) acc@1 0.9766 (0.9780) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:45] __main__ INFO: Elapsed 1.69
[2022-06-16 11:40:45] __main__ INFO: Val 139
[2022-06-16 11:40:45] __main__ INFO: Epoch 139 loss 1.7504 acc@1 0.6583 acc@5 0.9634
[2022-06-16 11:40:45] __main__ INFO: Elapsed 0.74
[2022-06-16 11:40:45] __main__ INFO: Train 140 5421
[2022-06-16 11:40:47] __main__ INFO: Epoch 140 Step 39/39 lr 0.001000 loss 0.0482 (0.0768) acc@1 1.0000 (0.9766) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:47] __main__ INFO: Elapsed 1.67
[2022-06-16 11:40:47] __main__ INFO: Val 140
[2022-06-16 11:40:48] __main__ INFO: Epoch 140 loss 1.7507 acc@1 0.6577 acc@5 0.9632
[2022-06-16 11:40:48] __main__ INFO: Elapsed 0.67
[2022-06-16 11:40:48] __main__ INFO: Train 141 5460
[2022-06-16 11:40:49] __main__ INFO: Epoch 141 Step 39/39 lr 0.001000 loss 0.0463 (0.0837) acc@1 0.9922 (0.9762) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:49] __main__ INFO: Elapsed 1.64
[2022-06-16 11:40:49] __main__ INFO: Val 141
[2022-06-16 11:40:50] __main__ INFO: Epoch 141 loss 1.7597 acc@1 0.6590 acc@5 0.9648
[2022-06-16 11:40:50] __main__ INFO: Elapsed 0.76
[2022-06-16 11:40:50] __main__ INFO: Train 142 5499
[2022-06-16 11:40:52] __main__ INFO: Epoch 142 Step 39/39 lr 0.001000 loss 0.1224 (0.0691) acc@1 0.9609 (0.9822) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:52] __main__ INFO: Elapsed 1.61
[2022-06-16 11:40:52] __main__ INFO: Val 142
[2022-06-16 11:40:52] __main__ INFO: Epoch 142 loss 1.7488 acc@1 0.6574 acc@5 0.9643
[2022-06-16 11:40:52] __main__ INFO: Elapsed 0.68
[2022-06-16 11:40:52] __main__ INFO: Train 143 5538
[2022-06-16 11:40:54] __main__ INFO: Epoch 143 Step 39/39 lr 0.001000 loss 0.1163 (0.0798) acc@1 0.9531 (0.9770) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:54] __main__ INFO: Elapsed 1.59
[2022-06-16 11:40:54] __main__ INFO: Val 143
[2022-06-16 11:40:55] __main__ INFO: Epoch 143 loss 1.7570 acc@1 0.6569 acc@5 0.9642
[2022-06-16 11:40:55] __main__ INFO: Elapsed 0.82
[2022-06-16 11:40:55] __main__ INFO: Train 144 5577
[2022-06-16 11:40:57] __main__ INFO: Epoch 144 Step 39/39 lr 0.001000 loss 0.0766 (0.0756) acc@1 0.9688 (0.9796) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:57] __main__ INFO: Elapsed 1.72
[2022-06-16 11:40:57] __main__ INFO: Val 144
[2022-06-16 11:40:57] __main__ INFO: Epoch 144 loss 1.7613 acc@1 0.6571 acc@5 0.9636
[2022-06-16 11:40:57] __main__ INFO: Elapsed 0.69
[2022-06-16 11:40:57] __main__ INFO: Train 145 5616
[2022-06-16 11:40:59] __main__ INFO: Epoch 145 Step 39/39 lr 0.001000 loss 0.0455 (0.0725) acc@1 1.0000 (0.9800) acc@5 1.0000 (1.0000)
[2022-06-16 11:40:59] __main__ INFO: Elapsed 1.67
[2022-06-16 11:40:59] __main__ INFO: Val 145
[2022-06-16 11:41:00] __main__ INFO: Epoch 145 loss 1.7622 acc@1 0.6555 acc@5 0.9632
[2022-06-16 11:41:00] __main__ INFO: Elapsed 0.79
[2022-06-16 11:41:00] __main__ INFO: Train 146 5655
[2022-06-16 11:41:01] __main__ INFO: Epoch 146 Step 39/39 lr 0.001000 loss 0.0556 (0.0747) acc@1 0.9922 (0.9778) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:01] __main__ INFO: Elapsed 1.71
[2022-06-16 11:41:01] __main__ INFO: Val 146
[2022-06-16 11:41:02] __main__ INFO: Epoch 146 loss 1.7613 acc@1 0.6602 acc@5 0.9641
[2022-06-16 11:41:02] __main__ INFO: Elapsed 0.84
[2022-06-16 11:41:02] __main__ INFO: Train 147 5694
[2022-06-16 11:41:04] __main__ INFO: Epoch 147 Step 39/39 lr 0.001000 loss 0.0748 (0.0702) acc@1 0.9688 (0.9794) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:04] __main__ INFO: Elapsed 1.69
[2022-06-16 11:41:04] __main__ INFO: Val 147
[2022-06-16 11:41:05] __main__ INFO: Epoch 147 loss 1.7667 acc@1 0.6558 acc@5 0.9634
[2022-06-16 11:41:05] __main__ INFO: Elapsed 0.74
[2022-06-16 11:41:05] __main__ INFO: Train 148 5733
[2022-06-16 11:41:06] __main__ INFO: Epoch 148 Step 39/39 lr 0.001000 loss 0.0620 (0.0674) acc@1 0.9766 (0.9812) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:06] __main__ INFO: Elapsed 1.69
[2022-06-16 11:41:06] __main__ INFO: Val 148
[2022-06-16 11:41:07] __main__ INFO: Epoch 148 loss 1.7611 acc@1 0.6586 acc@5 0.9629
[2022-06-16 11:41:07] __main__ INFO: Elapsed 0.75
[2022-06-16 11:41:07] __main__ INFO: Train 149 5772
[2022-06-16 11:41:09] __main__ INFO: Epoch 149 Step 39/39 lr 0.001000 loss 0.1020 (0.0744) acc@1 0.9688 (0.9772) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:09] __main__ INFO: Elapsed 1.67
[2022-06-16 11:41:09] __main__ INFO: Val 149
[2022-06-16 11:41:10] __main__ INFO: Epoch 149 loss 1.7762 acc@1 0.6584 acc@5 0.9640
[2022-06-16 11:41:10] __main__ INFO: Elapsed 0.71
[2022-06-16 11:41:10] __main__ INFO: Train 150 5811
[2022-06-16 11:41:11] __main__ INFO: Epoch 150 Step 39/39 lr 0.001000 loss 0.0975 (0.0684) acc@1 0.9688 (0.9814) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:11] __main__ INFO: Elapsed 1.73
[2022-06-16 11:41:11] __main__ INFO: Val 150
[2022-06-16 11:41:12] __main__ INFO: Epoch 150 loss 1.7773 acc@1 0.6570 acc@5 0.9633
[2022-06-16 11:41:12] __main__ INFO: Elapsed 0.78
[2022-06-16 11:41:12] __main__ INFO: Train 151 5850
[2022-06-16 11:41:14] __main__ INFO: Epoch 151 Step 39/39 lr 0.001000 loss 0.0733 (0.0734) acc@1 0.9766 (0.9784) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:14] __main__ INFO: Elapsed 1.63
[2022-06-16 11:41:14] __main__ INFO: Val 151
[2022-06-16 11:41:15] __main__ INFO: Epoch 151 loss 1.7711 acc@1 0.6600 acc@5 0.9622
[2022-06-16 11:41:15] __main__ INFO: Elapsed 0.79
[2022-06-16 11:41:15] __main__ INFO: Train 152 5889
[2022-06-16 11:41:16] __main__ INFO: Epoch 152 Step 39/39 lr 0.001000 loss 0.0720 (0.0727) acc@1 0.9844 (0.9796) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:16] __main__ INFO: Elapsed 1.67
[2022-06-16 11:41:16] __main__ INFO: Val 152
[2022-06-16 11:41:17] __main__ INFO: Epoch 152 loss 1.7784 acc@1 0.6586 acc@5 0.9635
[2022-06-16 11:41:17] __main__ INFO: Elapsed 0.72
[2022-06-16 11:41:17] __main__ INFO: Train 153 5928
[2022-06-16 11:41:19] __main__ INFO: Epoch 153 Step 39/39 lr 0.001000 loss 0.0702 (0.0691) acc@1 0.9844 (0.9792) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:19] __main__ INFO: Elapsed 1.67
[2022-06-16 11:41:19] __main__ INFO: Val 153
[2022-06-16 11:41:19] __main__ INFO: Epoch 153 loss 1.7837 acc@1 0.6591 acc@5 0.9633
[2022-06-16 11:41:19] __main__ INFO: Elapsed 0.79
[2022-06-16 11:41:19] __main__ INFO: Train 154 5967
[2022-06-16 11:41:21] __main__ INFO: Epoch 154 Step 39/39 lr 0.001000 loss 0.0768 (0.0651) acc@1 0.9688 (0.9834) acc@5 1.0000 (0.9998)
[2022-06-16 11:41:21] __main__ INFO: Elapsed 1.73
[2022-06-16 11:41:21] __main__ INFO: Val 154
[2022-06-16 11:41:22] __main__ INFO: Epoch 154 loss 1.7845 acc@1 0.6592 acc@5 0.9644
[2022-06-16 11:41:22] __main__ INFO: Elapsed 0.68
[2022-06-16 11:41:22] __main__ INFO: Train 155 6006
[2022-06-16 11:41:23] __main__ INFO: Epoch 155 Step 39/39 lr 0.001000 loss 0.0759 (0.0733) acc@1 0.9844 (0.9786) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:23] __main__ INFO: Elapsed 1.64
[2022-06-16 11:41:23] __main__ INFO: Val 155
[2022-06-16 11:41:24] __main__ INFO: Epoch 155 loss 1.7770 acc@1 0.6573 acc@5 0.9643
[2022-06-16 11:41:24] __main__ INFO: Elapsed 0.78
[2022-06-16 11:41:24] __main__ INFO: Train 156 6045
[2022-06-16 11:41:26] __main__ INFO: Epoch 156 Step 39/39 lr 0.001000 loss 0.0743 (0.0666) acc@1 0.9844 (0.9800) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:26] __main__ INFO: Elapsed 1.66
[2022-06-16 11:41:26] __main__ INFO: Val 156
[2022-06-16 11:41:27] __main__ INFO: Epoch 156 loss 1.7700 acc@1 0.6604 acc@5 0.9636
[2022-06-16 11:41:27] __main__ INFO: Elapsed 0.71
[2022-06-16 11:41:27] __main__ INFO: Train 157 6084
[2022-06-16 11:41:28] __main__ INFO: Epoch 157 Step 39/39 lr 0.001000 loss 0.0720 (0.0647) acc@1 0.9688 (0.9828) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:28] __main__ INFO: Elapsed 1.73
[2022-06-16 11:41:28] __main__ INFO: Val 157
[2022-06-16 11:41:29] __main__ INFO: Epoch 157 loss 1.7807 acc@1 0.6588 acc@5 0.9639
[2022-06-16 11:41:29] __main__ INFO: Elapsed 0.72
[2022-06-16 11:41:29] __main__ INFO: Train 158 6123
[2022-06-16 11:41:31] __main__ INFO: Epoch 158 Step 39/39 lr 0.001000 loss 0.1171 (0.0752) acc@1 0.9375 (0.9782) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:31] __main__ INFO: Elapsed 1.70
[2022-06-16 11:41:31] __main__ INFO: Val 158
[2022-06-16 11:41:32] __main__ INFO: Epoch 158 loss 1.7821 acc@1 0.6575 acc@5 0.9647
[2022-06-16 11:41:32] __main__ INFO: Elapsed 0.79
[2022-06-16 11:41:32] __main__ INFO: Train 159 6162
[2022-06-16 11:41:33] __main__ INFO: Epoch 159 Step 39/39 lr 0.001000 loss 0.0737 (0.0720) acc@1 0.9766 (0.9804) acc@5 1.0000 (0.9998)
[2022-06-16 11:41:33] __main__ INFO: Elapsed 1.65
[2022-06-16 11:41:33] __main__ INFO: Val 159
[2022-06-16 11:41:34] __main__ INFO: Epoch 159 loss 1.7836 acc@1 0.6595 acc@5 0.9641
[2022-06-16 11:41:34] __main__ INFO: Elapsed 0.78
[2022-06-16 11:41:34] __main__ INFO: Train 160 6201
[2022-06-16 11:41:36] __main__ INFO: Epoch 160 Step 39/39 lr 0.001000 loss 0.0530 (0.0652) acc@1 0.9844 (0.9830) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:36] __main__ INFO: Elapsed 1.72
[2022-06-16 11:41:36] __main__ INFO: Val 160
[2022-06-16 11:41:36] __main__ INFO: Epoch 160 loss 1.7881 acc@1 0.6600 acc@5 0.9642
[2022-06-16 11:41:36] __main__ INFO: Elapsed 0.77
[2022-06-16 11:41:36] __main__ INFO: Train 161 6240
[2022-06-16 11:41:38] __main__ INFO: Epoch 161 Step 39/39 lr 0.001000 loss 0.0233 (0.0681) acc@1 1.0000 (0.9800) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:38] __main__ INFO: Elapsed 1.66
[2022-06-16 11:41:38] __main__ INFO: Val 161
[2022-06-16 11:41:39] __main__ INFO: Epoch 161 loss 1.7966 acc@1 0.6589 acc@5 0.9646
[2022-06-16 11:41:39] __main__ INFO: Elapsed 0.73
[2022-06-16 11:41:39] __main__ INFO: Train 162 6279
[2022-06-16 11:41:40] __main__ INFO: Epoch 162 Step 39/39 lr 0.001000 loss 0.0595 (0.0689) acc@1 0.9844 (0.9812) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:41] __main__ INFO: Elapsed 1.66
[2022-06-16 11:41:41] __main__ INFO: Val 162
[2022-06-16 11:41:41] __main__ INFO: Epoch 162 loss 1.8007 acc@1 0.6593 acc@5 0.9641
[2022-06-16 11:41:41] __main__ INFO: Elapsed 0.83
[2022-06-16 11:41:41] __main__ INFO: Train 163 6318
[2022-06-16 11:41:43] __main__ INFO: Epoch 163 Step 39/39 lr 0.001000 loss 0.0553 (0.0645) acc@1 0.9766 (0.9818) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:43] __main__ INFO: Elapsed 1.69
[2022-06-16 11:41:43] __main__ INFO: Val 163
[2022-06-16 11:41:44] __main__ INFO: Epoch 163 loss 1.7959 acc@1 0.6584 acc@5 0.9637
[2022-06-16 11:41:44] __main__ INFO: Elapsed 0.82
[2022-06-16 11:41:44] __main__ INFO: Train 164 6357
[2022-06-16 11:41:45] __main__ INFO: Epoch 164 Step 39/39 lr 0.001000 loss 0.0721 (0.0693) acc@1 0.9922 (0.9800) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:45] __main__ INFO: Elapsed 1.59
[2022-06-16 11:41:45] __main__ INFO: Val 164
[2022-06-16 11:41:46] __main__ INFO: Epoch 164 loss 1.8013 acc@1 0.6568 acc@5 0.9644
[2022-06-16 11:41:46] __main__ INFO: Elapsed 0.78
[2022-06-16 11:41:46] __main__ INFO: Train 165 6396
[2022-06-16 11:41:48] __main__ INFO: Epoch 165 Step 39/39 lr 0.001000 loss 0.0692 (0.0622) acc@1 0.9688 (0.9832) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:48] __main__ INFO: Elapsed 1.63
[2022-06-16 11:41:48] __main__ INFO: Val 165
[2022-06-16 11:41:49] __main__ INFO: Epoch 165 loss 1.8006 acc@1 0.6569 acc@5 0.9638
[2022-06-16 11:41:49] __main__ INFO: Elapsed 0.68
[2022-06-16 11:41:49] __main__ INFO: Train 166 6435
[2022-06-16 11:41:50] __main__ INFO: Epoch 166 Step 39/39 lr 0.001000 loss 0.0738 (0.0598) acc@1 0.9688 (0.9834) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:50] __main__ INFO: Elapsed 1.71
[2022-06-16 11:41:50] __main__ INFO: Val 166
[2022-06-16 11:41:51] __main__ INFO: Epoch 166 loss 1.8051 acc@1 0.6562 acc@5 0.9643
[2022-06-16 11:41:51] __main__ INFO: Elapsed 0.79
[2022-06-16 11:41:51] __main__ INFO: Train 167 6474
[2022-06-16 11:41:53] __main__ INFO: Epoch 167 Step 39/39 lr 0.001000 loss 0.0717 (0.0710) acc@1 0.9844 (0.9792) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:53] __main__ INFO: Elapsed 1.69
[2022-06-16 11:41:53] __main__ INFO: Val 167
[2022-06-16 11:41:54] __main__ INFO: Epoch 167 loss 1.7954 acc@1 0.6584 acc@5 0.9628
[2022-06-16 11:41:54] __main__ INFO: Elapsed 0.78
[2022-06-16 11:41:54] __main__ INFO: Train 168 6513
[2022-06-16 11:41:55] __main__ INFO: Epoch 168 Step 39/39 lr 0.001000 loss 0.0579 (0.0622) acc@1 0.9844 (0.9846) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:55] __main__ INFO: Elapsed 1.62
[2022-06-16 11:41:55] __main__ INFO: Val 168
[2022-06-16 11:41:56] __main__ INFO: Epoch 168 loss 1.8013 acc@1 0.6587 acc@5 0.9636
[2022-06-16 11:41:56] __main__ INFO: Elapsed 0.79
[2022-06-16 11:41:56] __main__ INFO: Train 169 6552
[2022-06-16 11:41:58] __main__ INFO: Epoch 169 Step 39/39 lr 0.001000 loss 0.0641 (0.0617) acc@1 0.9844 (0.9826) acc@5 1.0000 (1.0000)
[2022-06-16 11:41:58] __main__ INFO: Elapsed 1.64
[2022-06-16 11:41:58] __main__ INFO: Val 169
[2022-06-16 11:41:58] __main__ INFO: Epoch 169 loss 1.8065 acc@1 0.6575 acc@5 0.9623
[2022-06-16 11:41:58] __main__ INFO: Elapsed 0.74
[2022-06-16 11:41:58] __main__ INFO: Train 170 6591
[2022-06-16 11:42:00] __main__ INFO: Epoch 170 Step 39/39 lr 0.001000 loss 0.0497 (0.0625) acc@1 0.9922 (0.9808) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:00] __main__ INFO: Elapsed 1.61
[2022-06-16 11:42:00] __main__ INFO: Val 170
[2022-06-16 11:42:01] __main__ INFO: Epoch 170 loss 1.8150 acc@1 0.6571 acc@5 0.9625
[2022-06-16 11:42:01] __main__ INFO: Elapsed 0.74
[2022-06-16 11:42:01] __main__ INFO: Train 171 6630
[2022-06-16 11:42:02] __main__ INFO: Epoch 171 Step 39/39 lr 0.001000 loss 0.0513 (0.0618) acc@1 0.9922 (0.9828) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:02] __main__ INFO: Elapsed 1.71
[2022-06-16 11:42:02] __main__ INFO: Val 171
[2022-06-16 11:42:03] __main__ INFO: Epoch 171 loss 1.8057 acc@1 0.6574 acc@5 0.9627
[2022-06-16 11:42:03] __main__ INFO: Elapsed 0.77
[2022-06-16 11:42:03] __main__ INFO: Train 172 6669
[2022-06-16 11:42:05] __main__ INFO: Epoch 172 Step 39/39 lr 0.001000 loss 0.0875 (0.0601) acc@1 0.9688 (0.9842) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:05] __main__ INFO: Elapsed 1.79
[2022-06-16 11:42:05] __main__ INFO: Val 172
[2022-06-16 11:42:06] __main__ INFO: Epoch 172 loss 1.8120 acc@1 0.6574 acc@5 0.9637
[2022-06-16 11:42:06] __main__ INFO: Elapsed 0.76
[2022-06-16 11:42:06] __main__ INFO: Train 173 6708
[2022-06-16 11:42:07] __main__ INFO: Epoch 173 Step 39/39 lr 0.001000 loss 0.0587 (0.0675) acc@1 0.9766 (0.9788) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:07] __main__ INFO: Elapsed 1.64
[2022-06-16 11:42:07] __main__ INFO: Val 173
[2022-06-16 11:42:08] __main__ INFO: Epoch 173 loss 1.8110 acc@1 0.6573 acc@5 0.9636
[2022-06-16 11:42:08] __main__ INFO: Elapsed 0.75
[2022-06-16 11:42:08] __main__ INFO: Train 174 6747
[2022-06-16 11:42:10] __main__ INFO: Epoch 174 Step 39/39 lr 0.001000 loss 0.0580 (0.0634) acc@1 1.0000 (0.9822) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:10] __main__ INFO: Elapsed 1.73
[2022-06-16 11:42:10] __main__ INFO: Val 174
[2022-06-16 11:42:11] __main__ INFO: Epoch 174 loss 1.8154 acc@1 0.6582 acc@5 0.9629
[2022-06-16 11:42:11] __main__ INFO: Elapsed 0.74
[2022-06-16 11:42:11] __main__ INFO: Train 175 6786
[2022-06-16 11:42:12] __main__ INFO: Epoch 175 Step 39/39 lr 0.001000 loss 0.0498 (0.0620) acc@1 0.9922 (0.9812) acc@5 1.0000 (0.9998)
[2022-06-16 11:42:12] __main__ INFO: Elapsed 1.63
[2022-06-16 11:42:12] __main__ INFO: Val 175
[2022-06-16 11:42:13] __main__ INFO: Epoch 175 loss 1.8034 acc@1 0.6586 acc@5 0.9632
[2022-06-16 11:42:13] __main__ INFO: Elapsed 0.74
[2022-06-16 11:42:13] __main__ INFO: Train 176 6825
[2022-06-16 11:42:15] __main__ INFO: Epoch 176 Step 39/39 lr 0.001000 loss 0.0434 (0.0670) acc@1 0.9922 (0.9816) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:15] __main__ INFO: Elapsed 1.70
[2022-06-16 11:42:15] __main__ INFO: Val 176
[2022-06-16 11:42:15] __main__ INFO: Epoch 176 loss 1.8272 acc@1 0.6555 acc@5 0.9632
[2022-06-16 11:42:15] __main__ INFO: Elapsed 0.71
[2022-06-16 11:42:15] __main__ INFO: Train 177 6864
[2022-06-16 11:42:17] __main__ INFO: Epoch 177 Step 39/39 lr 0.001000 loss 0.0755 (0.0634) acc@1 0.9766 (0.9830) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:17] __main__ INFO: Elapsed 1.70
[2022-06-16 11:42:17] __main__ INFO: Val 177
[2022-06-16 11:42:18] __main__ INFO: Epoch 177 loss 1.8169 acc@1 0.6564 acc@5 0.9635
[2022-06-16 11:42:18] __main__ INFO: Elapsed 0.76
[2022-06-16 11:42:18] __main__ INFO: Train 178 6903
[2022-06-16 11:42:20] __main__ INFO: Epoch 178 Step 39/39 lr 0.001000 loss 0.0638 (0.0678) acc@1 0.9766 (0.9792) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:20] __main__ INFO: Elapsed 1.66
[2022-06-16 11:42:20] __main__ INFO: Val 178
[2022-06-16 11:42:20] __main__ INFO: Epoch 178 loss 1.8237 acc@1 0.6569 acc@5 0.9631
[2022-06-16 11:42:20] __main__ INFO: Elapsed 0.79
[2022-06-16 11:42:20] __main__ INFO: Train 179 6942
[2022-06-16 11:42:22] __main__ INFO: Epoch 179 Step 39/39 lr 0.001000 loss 0.0448 (0.0632) acc@1 0.9922 (0.9812) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:22] __main__ INFO: Elapsed 1.70
[2022-06-16 11:42:22] __main__ INFO: Val 179
[2022-06-16 11:42:23] __main__ INFO: Epoch 179 loss 1.8299 acc@1 0.6561 acc@5 0.9630
[2022-06-16 11:42:23] __main__ INFO: Elapsed 0.80
[2022-06-16 11:42:23] __main__ INFO: Train 180 6981
[2022-06-16 11:42:24] __main__ INFO: Epoch 180 Step 39/39 lr 0.001000 loss 0.0738 (0.0656) acc@1 0.9766 (0.9804) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:25] __main__ INFO: Elapsed 1.69
[2022-06-16 11:42:25] __main__ INFO: Val 180
[2022-06-16 11:42:25] __main__ INFO: Epoch 180 loss 1.8298 acc@1 0.6556 acc@5 0.9633
[2022-06-16 11:42:25] __main__ INFO: Elapsed 0.72
[2022-06-16 11:42:25] __main__ INFO: Train 181 7020
[2022-06-16 11:42:27] __main__ INFO: Epoch 181 Step 39/39 lr 0.001000 loss 0.0696 (0.0677) acc@1 0.9766 (0.9808) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:27] __main__ INFO: Elapsed 1.67
[2022-06-16 11:42:27] __main__ INFO: Val 181
[2022-06-16 11:42:28] __main__ INFO: Epoch 181 loss 1.8275 acc@1 0.6572 acc@5 0.9638
[2022-06-16 11:42:28] __main__ INFO: Elapsed 0.79
[2022-06-16 11:42:28] __main__ INFO: Train 182 7059
[2022-06-16 11:42:29] __main__ INFO: Epoch 182 Step 39/39 lr 0.001000 loss 0.0655 (0.0618) acc@1 0.9766 (0.9834) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:29] __main__ INFO: Elapsed 1.64
[2022-06-16 11:42:29] __main__ INFO: Val 182
[2022-06-16 11:42:30] __main__ INFO: Epoch 182 loss 1.8255 acc@1 0.6544 acc@5 0.9633
[2022-06-16 11:42:30] __main__ INFO: Elapsed 0.74
[2022-06-16 11:42:30] __main__ INFO: Train 183 7098
[2022-06-16 11:42:32] __main__ INFO: Epoch 183 Step 39/39 lr 0.001000 loss 0.0639 (0.0640) acc@1 0.9766 (0.9820) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:32] __main__ INFO: Elapsed 1.72
[2022-06-16 11:42:32] __main__ INFO: Val 183
[2022-06-16 11:42:33] __main__ INFO: Epoch 183 loss 1.8282 acc@1 0.6550 acc@5 0.9640
[2022-06-16 11:42:33] __main__ INFO: Elapsed 0.73
[2022-06-16 11:42:33] __main__ INFO: Train 184 7137
[2022-06-16 11:42:34] __main__ INFO: Epoch 184 Step 39/39 lr 0.001000 loss 0.0352 (0.0649) acc@1 1.0000 (0.9814) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:34] __main__ INFO: Elapsed 1.66
[2022-06-16 11:42:34] __main__ INFO: Val 184
[2022-06-16 11:42:35] __main__ INFO: Epoch 184 loss 1.8320 acc@1 0.6556 acc@5 0.9644
[2022-06-16 11:42:35] __main__ INFO: Elapsed 0.75
[2022-06-16 11:42:35] __main__ INFO: Train 185 7176
[2022-06-16 11:42:37] __main__ INFO: Epoch 185 Step 39/39 lr 0.001000 loss 0.0865 (0.0642) acc@1 0.9766 (0.9802) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:37] __main__ INFO: Elapsed 1.66
[2022-06-16 11:42:37] __main__ INFO: Val 185
[2022-06-16 11:42:37] __main__ INFO: Epoch 185 loss 1.8290 acc@1 0.6582 acc@5 0.9636
[2022-06-16 11:42:37] __main__ INFO: Elapsed 0.77
[2022-06-16 11:42:37] __main__ INFO: Train 186 7215
[2022-06-16 11:42:39] __main__ INFO: Epoch 186 Step 39/39 lr 0.001000 loss 0.0388 (0.0609) acc@1 0.9922 (0.9820) acc@5 1.0000 (0.9998)
[2022-06-16 11:42:39] __main__ INFO: Elapsed 1.71
[2022-06-16 11:42:39] __main__ INFO: Val 186
[2022-06-16 11:42:40] __main__ INFO: Epoch 186 loss 1.8350 acc@1 0.6567 acc@5 0.9634
[2022-06-16 11:42:40] __main__ INFO: Elapsed 0.77
[2022-06-16 11:42:40] __main__ INFO: Train 187 7254
[2022-06-16 11:42:42] __main__ INFO: Epoch 187 Step 39/39 lr 0.001000 loss 0.1002 (0.0580) acc@1 0.9609 (0.9844) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:42] __main__ INFO: Elapsed 1.65
[2022-06-16 11:42:42] __main__ INFO: Val 187
[2022-06-16 11:42:42] __main__ INFO: Epoch 187 loss 1.8326 acc@1 0.6577 acc@5 0.9639
[2022-06-16 11:42:42] __main__ INFO: Elapsed 0.74
[2022-06-16 11:42:42] __main__ INFO: Train 188 7293
[2022-06-16 11:42:44] __main__ INFO: Epoch 188 Step 39/39 lr 0.001000 loss 0.0649 (0.0578) acc@1 0.9844 (0.9842) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:44] __main__ INFO: Elapsed 1.64
[2022-06-16 11:42:44] __main__ INFO: Val 188
[2022-06-16 11:42:45] __main__ INFO: Epoch 188 loss 1.8195 acc@1 0.6577 acc@5 0.9646
[2022-06-16 11:42:45] __main__ INFO: Elapsed 0.76
[2022-06-16 11:42:45] __main__ INFO: Train 189 7332
[2022-06-16 11:42:46] __main__ INFO: Epoch 189 Step 39/39 lr 0.001000 loss 0.0754 (0.0571) acc@1 0.9766 (0.9834) acc@5 1.0000 (0.9998)
[2022-06-16 11:42:46] __main__ INFO: Elapsed 1.73
[2022-06-16 11:42:46] __main__ INFO: Val 189
[2022-06-16 11:42:47] __main__ INFO: Epoch 189 loss 1.8421 acc@1 0.6566 acc@5 0.9635
[2022-06-16 11:42:47] __main__ INFO: Elapsed 0.74
[2022-06-16 11:42:47] __main__ INFO: Train 190 7371
[2022-06-16 11:42:49] __main__ INFO: Epoch 190 Step 39/39 lr 0.001000 loss 0.0700 (0.0602) acc@1 0.9766 (0.9834) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:49] __main__ INFO: Elapsed 1.62
[2022-06-16 11:42:49] __main__ INFO: Val 190
[2022-06-16 11:42:50] __main__ INFO: Epoch 190 loss 1.8356 acc@1 0.6564 acc@5 0.9630
[2022-06-16 11:42:50] __main__ INFO: Elapsed 0.76
[2022-06-16 11:42:50] __main__ INFO: Train 191 7410
[2022-06-16 11:42:51] __main__ INFO: Epoch 191 Step 39/39 lr 0.001000 loss 0.0524 (0.0645) acc@1 0.9922 (0.9820) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:51] __main__ INFO: Elapsed 1.71
[2022-06-16 11:42:51] __main__ INFO: Val 191
[2022-06-16 11:42:52] __main__ INFO: Epoch 191 loss 1.8352 acc@1 0.6574 acc@5 0.9632
[2022-06-16 11:42:52] __main__ INFO: Elapsed 0.78
[2022-06-16 11:42:52] __main__ INFO: Train 192 7449
[2022-06-16 11:42:54] __main__ INFO: Epoch 192 Step 39/39 lr 0.001000 loss 0.0551 (0.0621) acc@1 0.9844 (0.9814) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:54] __main__ INFO: Elapsed 1.66
[2022-06-16 11:42:54] __main__ INFO: Val 192
[2022-06-16 11:42:55] __main__ INFO: Epoch 192 loss 1.8468 acc@1 0.6568 acc@5 0.9633
[2022-06-16 11:42:55] __main__ INFO: Elapsed 0.78
[2022-06-16 11:42:55] __main__ INFO: Train 193 7488
[2022-06-16 11:42:56] __main__ INFO: Epoch 193 Step 39/39 lr 0.001000 loss 0.1020 (0.0600) acc@1 0.9688 (0.9830) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:56] __main__ INFO: Elapsed 1.59
[2022-06-16 11:42:56] __main__ INFO: Val 193
[2022-06-16 11:42:57] __main__ INFO: Epoch 193 loss 1.8556 acc@1 0.6558 acc@5 0.9629
[2022-06-16 11:42:57] __main__ INFO: Elapsed 0.73
[2022-06-16 11:42:57] __main__ INFO: Train 194 7527
[2022-06-16 11:42:58] __main__ INFO: Epoch 194 Step 39/39 lr 0.001000 loss 0.0780 (0.0618) acc@1 0.9922 (0.9830) acc@5 1.0000 (1.0000)
[2022-06-16 11:42:58] __main__ INFO: Elapsed 1.63
[2022-06-16 11:42:58] __main__ INFO: Val 194
[2022-06-16 11:42:59] __main__ INFO: Epoch 194 loss 1.8587 acc@1 0.6564 acc@5 0.9621
[2022-06-16 11:42:59] __main__ INFO: Elapsed 0.76
[2022-06-16 11:42:59] __main__ INFO: Train 195 7566
[2022-06-16 11:43:01] __main__ INFO: Epoch 195 Step 39/39 lr 0.001000 loss 0.1012 (0.0608) acc@1 0.9766 (0.9840) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:01] __main__ INFO: Elapsed 1.64
[2022-06-16 11:43:01] __main__ INFO: Val 195
[2022-06-16 11:43:02] __main__ INFO: Epoch 195 loss 1.8546 acc@1 0.6574 acc@5 0.9627
[2022-06-16 11:43:02] __main__ INFO: Elapsed 0.75
[2022-06-16 11:43:02] __main__ INFO: Train 196 7605
[2022-06-16 11:43:03] __main__ INFO: Epoch 196 Step 39/39 lr 0.001000 loss 0.0804 (0.0638) acc@1 0.9688 (0.9812) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:03] __main__ INFO: Elapsed 1.74
[2022-06-16 11:43:03] __main__ INFO: Val 196
[2022-06-16 11:43:04] __main__ INFO: Epoch 196 loss 1.8476 acc@1 0.6565 acc@5 0.9635
[2022-06-16 11:43:04] __main__ INFO: Elapsed 0.75
[2022-06-16 11:43:04] __main__ INFO: Train 197 7644
[2022-06-16 11:43:06] __main__ INFO: Epoch 197 Step 39/39 lr 0.001000 loss 0.0837 (0.0553) acc@1 0.9766 (0.9860) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:06] __main__ INFO: Elapsed 1.60
[2022-06-16 11:43:06] __main__ INFO: Val 197
[2022-06-16 11:43:07] __main__ INFO: Epoch 197 loss 1.8534 acc@1 0.6555 acc@5 0.9629
[2022-06-16 11:43:07] __main__ INFO: Elapsed 0.78
[2022-06-16 11:43:07] __main__ INFO: Train 198 7683
[2022-06-16 11:43:08] __main__ INFO: Epoch 198 Step 39/39 lr 0.001000 loss 0.0557 (0.0592) acc@1 0.9844 (0.9830) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:08] __main__ INFO: Elapsed 1.63
[2022-06-16 11:43:08] __main__ INFO: Val 198
[2022-06-16 11:43:09] __main__ INFO: Epoch 198 loss 1.8467 acc@1 0.6557 acc@5 0.9639
[2022-06-16 11:43:09] __main__ INFO: Elapsed 0.75
[2022-06-16 11:43:09] __main__ INFO: Train 199 7722
[2022-06-16 11:43:11] __main__ INFO: Epoch 199 Step 39/39 lr 0.001000 loss 0.0806 (0.0556) acc@1 0.9844 (0.9866) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:11] __main__ INFO: Elapsed 1.67
[2022-06-16 11:43:11] __main__ INFO: Val 199
[2022-06-16 11:43:11] __main__ INFO: Epoch 199 loss 1.8575 acc@1 0.6551 acc@5 0.9635
[2022-06-16 11:43:11] __main__ INFO: Elapsed 0.79
[2022-06-16 11:43:11] __main__ INFO: Train 200 7761
[2022-06-16 11:43:13] __main__ INFO: Epoch 200 Step 39/39 lr 0.001000 loss 0.1020 (0.0620) acc@1 0.9609 (0.9830) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:13] __main__ INFO: Elapsed 1.60
[2022-06-16 11:43:13] __main__ INFO: Val 200
[2022-06-16 11:43:14] __main__ INFO: Epoch 200 loss 1.8547 acc@1 0.6546 acc@5 0.9642
[2022-06-16 11:43:14] __main__ INFO: Elapsed 0.78
[2022-06-16 11:43:14] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00200.pth
[2022-06-16 11:43:14] __main__ INFO: Train 201 7800
[2022-06-16 11:43:15] __main__ INFO: Epoch 201 Step 39/39 lr 0.001000 loss 0.0738 (0.0559) acc@1 0.9766 (0.9836) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:15] __main__ INFO: Elapsed 1.60
[2022-06-16 11:43:15] __main__ INFO: Val 201
[2022-06-16 11:43:16] __main__ INFO: Epoch 201 loss 1.8438 acc@1 0.6578 acc@5 0.9645
[2022-06-16 11:43:16] __main__ INFO: Elapsed 0.82
[2022-06-16 11:43:16] __main__ INFO: Train 202 7839
[2022-06-16 11:43:18] __main__ INFO: Epoch 202 Step 39/39 lr 0.001000 loss 0.0686 (0.0567) acc@1 0.9766 (0.9840) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:18] __main__ INFO: Elapsed 1.72
[2022-06-16 11:43:18] __main__ INFO: Val 202
[2022-06-16 11:43:19] __main__ INFO: Epoch 202 loss 1.8471 acc@1 0.6583 acc@5 0.9638
[2022-06-16 11:43:19] __main__ INFO: Elapsed 0.80
[2022-06-16 11:43:19] __main__ INFO: Train 203 7878
[2022-06-16 11:43:20] __main__ INFO: Epoch 203 Step 39/39 lr 0.001000 loss 0.0385 (0.0544) acc@1 0.9922 (0.9862) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:20] __main__ INFO: Elapsed 1.67
[2022-06-16 11:43:20] __main__ INFO: Val 203
[2022-06-16 11:43:21] __main__ INFO: Epoch 203 loss 1.8410 acc@1 0.6576 acc@5 0.9630
[2022-06-16 11:43:21] __main__ INFO: Elapsed 0.79
[2022-06-16 11:43:21] __main__ INFO: Train 204 7917
[2022-06-16 11:43:23] __main__ INFO: Epoch 204 Step 39/39 lr 0.001000 loss 0.0603 (0.0556) acc@1 0.9844 (0.9842) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:23] __main__ INFO: Elapsed 1.67
[2022-06-16 11:43:23] __main__ INFO: Val 204
[2022-06-16 11:43:24] __main__ INFO: Epoch 204 loss 1.8518 acc@1 0.6561 acc@5 0.9632
[2022-06-16 11:43:24] __main__ INFO: Elapsed 0.77
[2022-06-16 11:43:24] __main__ INFO: Train 205 7956
[2022-06-16 11:43:25] __main__ INFO: Epoch 205 Step 39/39 lr 0.001000 loss 0.0439 (0.0609) acc@1 0.9922 (0.9822) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:25] __main__ INFO: Elapsed 1.74
[2022-06-16 11:43:25] __main__ INFO: Val 205
[2022-06-16 11:43:26] __main__ INFO: Epoch 205 loss 1.8668 acc@1 0.6551 acc@5 0.9628
[2022-06-16 11:43:26] __main__ INFO: Elapsed 0.79
[2022-06-16 11:43:26] __main__ INFO: Train 206 7995
[2022-06-16 11:43:28] __main__ INFO: Epoch 206 Step 39/39 lr 0.001000 loss 0.0780 (0.0598) acc@1 0.9688 (0.9840) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:28] __main__ INFO: Elapsed 1.65
[2022-06-16 11:43:28] __main__ INFO: Val 206
[2022-06-16 11:43:29] __main__ INFO: Epoch 206 loss 1.8622 acc@1 0.6548 acc@5 0.9634
[2022-06-16 11:43:29] __main__ INFO: Elapsed 0.79
[2022-06-16 11:43:29] __main__ INFO: Train 207 8034
[2022-06-16 11:43:30] __main__ INFO: Epoch 207 Step 39/39 lr 0.001000 loss 0.0342 (0.0590) acc@1 0.9922 (0.9844) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:30] __main__ INFO: Elapsed 1.64
[2022-06-16 11:43:30] __main__ INFO: Val 207
[2022-06-16 11:43:31] __main__ INFO: Epoch 207 loss 1.8669 acc@1 0.6558 acc@5 0.9624
[2022-06-16 11:43:31] __main__ INFO: Elapsed 0.72
[2022-06-16 11:43:31] __main__ INFO: Train 208 8073
[2022-06-16 11:43:33] __main__ INFO: Epoch 208 Step 39/39 lr 0.001000 loss 0.0627 (0.0634) acc@1 0.9766 (0.9826) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:33] __main__ INFO: Elapsed 1.73
[2022-06-16 11:43:33] __main__ INFO: Val 208
[2022-06-16 11:43:33] __main__ INFO: Epoch 208 loss 1.8582 acc@1 0.6569 acc@5 0.9633
[2022-06-16 11:43:33] __main__ INFO: Elapsed 0.75
[2022-06-16 11:43:33] __main__ INFO: Train 209 8112
[2022-06-16 11:43:35] __main__ INFO: Epoch 209 Step 39/39 lr 0.001000 loss 0.0587 (0.0586) acc@1 0.9844 (0.9842) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:35] __main__ INFO: Elapsed 1.65
[2022-06-16 11:43:35] __main__ INFO: Val 209
[2022-06-16 11:43:36] __main__ INFO: Epoch 209 loss 1.8659 acc@1 0.6547 acc@5 0.9639
[2022-06-16 11:43:36] __main__ INFO: Elapsed 0.79
[2022-06-16 11:43:36] __main__ INFO: Train 210 8151
[2022-06-16 11:43:38] __main__ INFO: Epoch 210 Step 39/39 lr 0.001000 loss 0.0580 (0.0561) acc@1 0.9844 (0.9864) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:38] __main__ INFO: Elapsed 1.68
[2022-06-16 11:43:38] __main__ INFO: Val 210
[2022-06-16 11:43:38] __main__ INFO: Epoch 210 loss 1.8752 acc@1 0.6547 acc@5 0.9629
[2022-06-16 11:43:38] __main__ INFO: Elapsed 0.71
[2022-06-16 11:43:38] __main__ INFO: Train 211 8190
[2022-06-16 11:43:40] __main__ INFO: Epoch 211 Step 39/39 lr 0.001000 loss 0.0605 (0.0534) acc@1 0.9844 (0.9866) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:40] __main__ INFO: Elapsed 1.65
[2022-06-16 11:43:40] __main__ INFO: Val 211
[2022-06-16 11:43:41] __main__ INFO: Epoch 211 loss 1.8678 acc@1 0.6547 acc@5 0.9626
[2022-06-16 11:43:41] __main__ INFO: Elapsed 0.73
[2022-06-16 11:43:41] __main__ INFO: Train 212 8229
[2022-06-16 11:43:42] __main__ INFO: Epoch 212 Step 39/39 lr 0.001000 loss 0.0430 (0.0566) acc@1 0.9922 (0.9840) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:42] __main__ INFO: Elapsed 1.69
[2022-06-16 11:43:42] __main__ INFO: Val 212
[2022-06-16 11:43:43] __main__ INFO: Epoch 212 loss 1.8793 acc@1 0.6541 acc@5 0.9628
[2022-06-16 11:43:43] __main__ INFO: Elapsed 0.76
[2022-06-16 11:43:43] __main__ INFO: Train 213 8268
[2022-06-16 11:43:45] __main__ INFO: Epoch 213 Step 39/39 lr 0.001000 loss 0.0641 (0.0567) acc@1 0.9688 (0.9832) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:45] __main__ INFO: Elapsed 1.67
[2022-06-16 11:43:45] __main__ INFO: Val 213
[2022-06-16 11:43:45] __main__ INFO: Epoch 213 loss 1.8704 acc@1 0.6545 acc@5 0.9633
[2022-06-16 11:43:45] __main__ INFO: Elapsed 0.67
[2022-06-16 11:43:45] __main__ INFO: Train 214 8307
[2022-06-16 11:43:47] __main__ INFO: Epoch 214 Step 39/39 lr 0.001000 loss 0.0549 (0.0542) acc@1 0.9844 (0.9846) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:47] __main__ INFO: Elapsed 1.62
[2022-06-16 11:43:47] __main__ INFO: Val 214
[2022-06-16 11:43:48] __main__ INFO: Epoch 214 loss 1.8797 acc@1 0.6554 acc@5 0.9628
[2022-06-16 11:43:48] __main__ INFO: Elapsed 0.71
[2022-06-16 11:43:48] __main__ INFO: Train 215 8346
[2022-06-16 11:43:49] __main__ INFO: Epoch 215 Step 39/39 lr 0.001000 loss 0.0308 (0.0535) acc@1 1.0000 (0.9846) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:50] __main__ INFO: Elapsed 1.67
[2022-06-16 11:43:50] __main__ INFO: Val 215
[2022-06-16 11:43:50] __main__ INFO: Epoch 215 loss 1.8774 acc@1 0.6543 acc@5 0.9635
[2022-06-16 11:43:50] __main__ INFO: Elapsed 0.68
[2022-06-16 11:43:50] __main__ INFO: Train 216 8385
[2022-06-16 11:43:52] __main__ INFO: Epoch 216 Step 39/39 lr 0.001000 loss 0.0449 (0.0567) acc@1 0.9922 (0.9852) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:52] __main__ INFO: Elapsed 1.67
[2022-06-16 11:43:52] __main__ INFO: Val 216
[2022-06-16 11:43:53] __main__ INFO: Epoch 216 loss 1.8744 acc@1 0.6553 acc@5 0.9624
[2022-06-16 11:43:53] __main__ INFO: Elapsed 0.74
[2022-06-16 11:43:53] __main__ INFO: Train 217 8424
[2022-06-16 11:43:54] __main__ INFO: Epoch 217 Step 39/39 lr 0.001000 loss 0.0615 (0.0571) acc@1 0.9766 (0.9846) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:54] __main__ INFO: Elapsed 1.65
[2022-06-16 11:43:54] __main__ INFO: Val 217
[2022-06-16 11:43:55] __main__ INFO: Epoch 217 loss 1.8766 acc@1 0.6565 acc@5 0.9630
[2022-06-16 11:43:55] __main__ INFO: Elapsed 0.82
[2022-06-16 11:43:55] __main__ INFO: Train 218 8463
[2022-06-16 11:43:57] __main__ INFO: Epoch 218 Step 39/39 lr 0.001000 loss 0.0438 (0.0491) acc@1 0.9844 (0.9882) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:57] __main__ INFO: Elapsed 1.69
[2022-06-16 11:43:57] __main__ INFO: Val 218
[2022-06-16 11:43:58] __main__ INFO: Epoch 218 loss 1.8837 acc@1 0.6572 acc@5 0.9627
[2022-06-16 11:43:58] __main__ INFO: Elapsed 0.75
[2022-06-16 11:43:58] __main__ INFO: Train 219 8502
[2022-06-16 11:43:59] __main__ INFO: Epoch 219 Step 39/39 lr 0.001000 loss 0.0351 (0.0541) acc@1 0.9922 (0.9850) acc@5 1.0000 (1.0000)
[2022-06-16 11:43:59] __main__ INFO: Elapsed 1.66
[2022-06-16 11:43:59] __main__ INFO: Val 219
[2022-06-16 11:44:00] __main__ INFO: Epoch 219 loss 1.8897 acc@1 0.6555 acc@5 0.9629
[2022-06-16 11:44:00] __main__ INFO: Elapsed 0.72
[2022-06-16 11:44:00] __main__ INFO: Train 220 8541
[2022-06-16 11:44:02] __main__ INFO: Epoch 220 Step 39/39 lr 0.001000 loss 0.0351 (0.0564) acc@1 0.9922 (0.9848) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:02] __main__ INFO: Elapsed 1.68
[2022-06-16 11:44:02] __main__ INFO: Val 220
[2022-06-16 11:44:02] __main__ INFO: Epoch 220 loss 1.8965 acc@1 0.6553 acc@5 0.9628
[2022-06-16 11:44:02] __main__ INFO: Elapsed 0.81
[2022-06-16 11:44:02] __main__ INFO: Train 221 8580
[2022-06-16 11:44:04] __main__ INFO: Epoch 221 Step 39/39 lr 0.001000 loss 0.0698 (0.0561) acc@1 0.9688 (0.9836) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:04] __main__ INFO: Elapsed 1.66
[2022-06-16 11:44:04] __main__ INFO: Val 221
[2022-06-16 11:44:05] __main__ INFO: Epoch 221 loss 1.8953 acc@1 0.6553 acc@5 0.9630
[2022-06-16 11:44:05] __main__ INFO: Elapsed 0.81
[2022-06-16 11:44:05] __main__ INFO: Train 222 8619
[2022-06-16 11:44:06] __main__ INFO: Epoch 222 Step 39/39 lr 0.001000 loss 0.0652 (0.0539) acc@1 0.9922 (0.9864) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:07] __main__ INFO: Elapsed 1.66
[2022-06-16 11:44:07] __main__ INFO: Val 222
[2022-06-16 11:44:07] __main__ INFO: Epoch 222 loss 1.8898 acc@1 0.6555 acc@5 0.9636
[2022-06-16 11:44:07] __main__ INFO: Elapsed 0.74
[2022-06-16 11:44:07] __main__ INFO: Train 223 8658
[2022-06-16 11:44:09] __main__ INFO: Epoch 223 Step 39/39 lr 0.001000 loss 0.1155 (0.0571) acc@1 0.9531 (0.9838) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:09] __main__ INFO: Elapsed 1.67
[2022-06-16 11:44:09] __main__ INFO: Val 223
[2022-06-16 11:44:10] __main__ INFO: Epoch 223 loss 1.9011 acc@1 0.6532 acc@5 0.9624
[2022-06-16 11:44:10] __main__ INFO: Elapsed 0.80
[2022-06-16 11:44:10] __main__ INFO: Train 224 8697
[2022-06-16 11:44:11] __main__ INFO: Epoch 224 Step 39/39 lr 0.001000 loss 0.0626 (0.0544) acc@1 0.9766 (0.9852) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:11] __main__ INFO: Elapsed 1.70
[2022-06-16 11:44:11] __main__ INFO: Val 224
[2022-06-16 11:44:12] __main__ INFO: Epoch 224 loss 1.9016 acc@1 0.6543 acc@5 0.9631
[2022-06-16 11:44:12] __main__ INFO: Elapsed 0.83
[2022-06-16 11:44:12] __main__ INFO: Train 225 8736
[2022-06-16 11:44:14] __main__ INFO: Epoch 225 Step 39/39 lr 0.001000 loss 0.0327 (0.0498) acc@1 0.9922 (0.9880) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:14] __main__ INFO: Elapsed 1.70
[2022-06-16 11:44:14] __main__ INFO: Val 225
[2022-06-16 11:44:15] __main__ INFO: Epoch 225 loss 1.8919 acc@1 0.6556 acc@5 0.9627
[2022-06-16 11:44:15] __main__ INFO: Elapsed 0.79
[2022-06-16 11:44:15] __main__ INFO: Train 226 8775
[2022-06-16 11:44:16] __main__ INFO: Epoch 226 Step 39/39 lr 0.001000 loss 0.0378 (0.0533) acc@1 0.9922 (0.9864) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:16] __main__ INFO: Elapsed 1.68
[2022-06-16 11:44:16] __main__ INFO: Val 226
[2022-06-16 11:44:17] __main__ INFO: Epoch 226 loss 1.8903 acc@1 0.6564 acc@5 0.9625
[2022-06-16 11:44:17] __main__ INFO: Elapsed 0.76
[2022-06-16 11:44:17] __main__ INFO: Train 227 8814
[2022-06-16 11:44:19] __main__ INFO: Epoch 227 Step 39/39 lr 0.001000 loss 0.0511 (0.0542) acc@1 0.9844 (0.9836) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:19] __main__ INFO: Elapsed 1.67
[2022-06-16 11:44:19] __main__ INFO: Val 227
[2022-06-16 11:44:20] __main__ INFO: Epoch 227 loss 1.9085 acc@1 0.6554 acc@5 0.9629
[2022-06-16 11:44:20] __main__ INFO: Elapsed 0.78
[2022-06-16 11:44:20] __main__ INFO: Train 228 8853
[2022-06-16 11:44:21] __main__ INFO: Epoch 228 Step 39/39 lr 0.001000 loss 0.0774 (0.0525) acc@1 0.9922 (0.9854) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:21] __main__ INFO: Elapsed 1.69
[2022-06-16 11:44:21] __main__ INFO: Val 228
[2022-06-16 11:44:22] __main__ INFO: Epoch 228 loss 1.9008 acc@1 0.6554 acc@5 0.9624
[2022-06-16 11:44:22] __main__ INFO: Elapsed 0.75
[2022-06-16 11:44:22] __main__ INFO: Train 229 8892
[2022-06-16 11:44:24] __main__ INFO: Epoch 229 Step 39/39 lr 0.001000 loss 0.0781 (0.0542) acc@1 0.9766 (0.9852) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:24] __main__ INFO: Elapsed 1.65
[2022-06-16 11:44:24] __main__ INFO: Val 229
[2022-06-16 11:44:25] __main__ INFO: Epoch 229 loss 1.8961 acc@1 0.6549 acc@5 0.9618
[2022-06-16 11:44:25] __main__ INFO: Elapsed 0.72
[2022-06-16 11:44:25] __main__ INFO: Train 230 8931
[2022-06-16 11:44:26] __main__ INFO: Epoch 230 Step 39/39 lr 0.001000 loss 0.0409 (0.0583) acc@1 0.9922 (0.9824) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:26] __main__ INFO: Elapsed 1.65
[2022-06-16 11:44:26] __main__ INFO: Val 230
[2022-06-16 11:44:27] __main__ INFO: Epoch 230 loss 1.9093 acc@1 0.6520 acc@5 0.9617
[2022-06-16 11:44:27] __main__ INFO: Elapsed 0.79
[2022-06-16 11:44:27] __main__ INFO: Train 231 8970
[2022-06-16 11:44:29] __main__ INFO: Epoch 231 Step 39/39 lr 0.001000 loss 0.0225 (0.0551) acc@1 1.0000 (0.9838) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:29] __main__ INFO: Elapsed 1.63
[2022-06-16 11:44:29] __main__ INFO: Val 231
[2022-06-16 11:44:29] __main__ INFO: Epoch 231 loss 1.8949 acc@1 0.6559 acc@5 0.9627
[2022-06-16 11:44:29] __main__ INFO: Elapsed 0.73
[2022-06-16 11:44:29] __main__ INFO: Train 232 9009
[2022-06-16 11:44:31] __main__ INFO: Epoch 232 Step 39/39 lr 0.001000 loss 0.0291 (0.0510) acc@1 0.9922 (0.9862) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:31] __main__ INFO: Elapsed 1.68
[2022-06-16 11:44:31] __main__ INFO: Val 232
[2022-06-16 11:44:32] __main__ INFO: Epoch 232 loss 1.9079 acc@1 0.6538 acc@5 0.9624
[2022-06-16 11:44:32] __main__ INFO: Elapsed 0.79
[2022-06-16 11:44:32] __main__ INFO: Train 233 9048
[2022-06-16 11:44:33] __main__ INFO: Epoch 233 Step 39/39 lr 0.001000 loss 0.0814 (0.0524) acc@1 0.9609 (0.9868) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:34] __main__ INFO: Elapsed 1.70
[2022-06-16 11:44:34] __main__ INFO: Val 233
[2022-06-16 11:44:34] __main__ INFO: Epoch 233 loss 1.9068 acc@1 0.6539 acc@5 0.9624
[2022-06-16 11:44:34] __main__ INFO: Elapsed 0.77
[2022-06-16 11:44:34] __main__ INFO: Train 234 9087
[2022-06-16 11:44:36] __main__ INFO: Epoch 234 Step 39/39 lr 0.001000 loss 0.0307 (0.0471) acc@1 1.0000 (0.9882) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:36] __main__ INFO: Elapsed 1.76
[2022-06-16 11:44:36] __main__ INFO: Val 234
[2022-06-16 11:44:37] __main__ INFO: Epoch 234 loss 1.9042 acc@1 0.6572 acc@5 0.9624
[2022-06-16 11:44:37] __main__ INFO: Elapsed 0.80
[2022-06-16 11:44:37] __main__ INFO: Train 235 9126
[2022-06-16 11:44:38] __main__ INFO: Epoch 235 Step 39/39 lr 0.001000 loss 0.0391 (0.0509) acc@1 0.9844 (0.9882) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:38] __main__ INFO: Elapsed 1.62
[2022-06-16 11:44:38] __main__ INFO: Val 235
[2022-06-16 11:44:39] __main__ INFO: Epoch 235 loss 1.9106 acc@1 0.6555 acc@5 0.9628
[2022-06-16 11:44:39] __main__ INFO: Elapsed 0.71
[2022-06-16 11:44:39] __main__ INFO: Train 236 9165
[2022-06-16 11:44:41] __main__ INFO: Epoch 236 Step 39/39 lr 0.001000 loss 0.0275 (0.0519) acc@1 1.0000 (0.9844) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:41] __main__ INFO: Elapsed 1.63
[2022-06-16 11:44:41] __main__ INFO: Val 236
[2022-06-16 11:44:42] __main__ INFO: Epoch 236 loss 1.9069 acc@1 0.6523 acc@5 0.9637
[2022-06-16 11:44:42] __main__ INFO: Elapsed 0.76
[2022-06-16 11:44:42] __main__ INFO: Train 237 9204
[2022-06-16 11:44:43] __main__ INFO: Epoch 237 Step 39/39 lr 0.001000 loss 0.0604 (0.0520) acc@1 0.9766 (0.9872) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:43] __main__ INFO: Elapsed 1.71
[2022-06-16 11:44:43] __main__ INFO: Val 237
[2022-06-16 11:44:44] __main__ INFO: Epoch 237 loss 1.9048 acc@1 0.6560 acc@5 0.9630
[2022-06-16 11:44:44] __main__ INFO: Elapsed 0.76
[2022-06-16 11:44:44] __main__ INFO: Train 238 9243
[2022-06-16 11:44:46] __main__ INFO: Epoch 238 Step 39/39 lr 0.001000 loss 0.0641 (0.0525) acc@1 0.9766 (0.9850) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:46] __main__ INFO: Elapsed 1.69
[2022-06-16 11:44:46] __main__ INFO: Val 238
[2022-06-16 11:44:47] __main__ INFO: Epoch 238 loss 1.9176 acc@1 0.6536 acc@5 0.9632
[2022-06-16 11:44:47] __main__ INFO: Elapsed 0.78
[2022-06-16 11:44:47] __main__ INFO: Train 239 9282
[2022-06-16 11:44:48] __main__ INFO: Epoch 239 Step 39/39 lr 0.001000 loss 0.0656 (0.0540) acc@1 0.9688 (0.9846) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:48] __main__ INFO: Elapsed 1.67
[2022-06-16 11:44:48] __main__ INFO: Val 239
[2022-06-16 11:44:49] __main__ INFO: Epoch 239 loss 1.9207 acc@1 0.6541 acc@5 0.9638
[2022-06-16 11:44:49] __main__ INFO: Elapsed 0.72
[2022-06-16 11:44:49] __main__ INFO: Train 240 9321
[2022-06-16 11:44:50] __main__ INFO: Epoch 240 Step 39/39 lr 0.001000 loss 0.0766 (0.0512) acc@1 0.9844 (0.9870) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:51] __main__ INFO: Elapsed 1.63
[2022-06-16 11:44:51] __main__ INFO: Val 240
[2022-06-16 11:44:51] __main__ INFO: Epoch 240 loss 1.9132 acc@1 0.6545 acc@5 0.9623
[2022-06-16 11:44:51] __main__ INFO: Elapsed 0.78
[2022-06-16 11:44:51] __main__ INFO: Train 241 9360
[2022-06-16 11:44:53] __main__ INFO: Epoch 241 Step 39/39 lr 0.001000 loss 0.0470 (0.0439) acc@1 0.9844 (0.9898) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:53] __main__ INFO: Elapsed 1.68
[2022-06-16 11:44:53] __main__ INFO: Val 241
[2022-06-16 11:44:54] __main__ INFO: Epoch 241 loss 1.9167 acc@1 0.6546 acc@5 0.9625
[2022-06-16 11:44:54] __main__ INFO: Elapsed 0.72
[2022-06-16 11:44:54] __main__ INFO: Train 242 9399
[2022-06-16 11:44:55] __main__ INFO: Epoch 242 Step 39/39 lr 0.001000 loss 0.0243 (0.0481) acc@1 1.0000 (0.9870) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:55] __main__ INFO: Elapsed 1.69
[2022-06-16 11:44:55] __main__ INFO: Val 242
[2022-06-16 11:44:56] __main__ INFO: Epoch 242 loss 1.8975 acc@1 0.6575 acc@5 0.9633
[2022-06-16 11:44:56] __main__ INFO: Elapsed 0.77
[2022-06-16 11:44:56] __main__ INFO: Train 243 9438
[2022-06-16 11:44:58] __main__ INFO: Epoch 243 Step 39/39 lr 0.001000 loss 0.0243 (0.0460) acc@1 0.9922 (0.9902) acc@5 1.0000 (1.0000)
[2022-06-16 11:44:58] __main__ INFO: Elapsed 1.62
[2022-06-16 11:44:58] __main__ INFO: Val 243
[2022-06-16 11:44:59] __main__ INFO: Epoch 243 loss 1.9248 acc@1 0.6546 acc@5 0.9620
[2022-06-16 11:44:59] __main__ INFO: Elapsed 0.77
[2022-06-16 11:44:59] __main__ INFO: Train 244 9477
[2022-06-16 11:45:00] __main__ INFO: Epoch 244 Step 39/39 lr 0.001000 loss 0.0708 (0.0530) acc@1 0.9609 (0.9850) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:00] __main__ INFO: Elapsed 1.66
[2022-06-16 11:45:00] __main__ INFO: Val 244
[2022-06-16 11:45:01] __main__ INFO: Epoch 244 loss 1.9189 acc@1 0.6564 acc@5 0.9638
[2022-06-16 11:45:01] __main__ INFO: Elapsed 0.76
[2022-06-16 11:45:01] __main__ INFO: Train 245 9516
[2022-06-16 11:45:03] __main__ INFO: Epoch 245 Step 39/39 lr 0.001000 loss 0.0304 (0.0463) acc@1 1.0000 (0.9866) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:03] __main__ INFO: Elapsed 1.73
[2022-06-16 11:45:03] __main__ INFO: Val 245
[2022-06-16 11:45:03] __main__ INFO: Epoch 245 loss 1.9113 acc@1 0.6549 acc@5 0.9630
[2022-06-16 11:45:03] __main__ INFO: Elapsed 0.74
[2022-06-16 11:45:03] __main__ INFO: Train 246 9555
[2022-06-16 11:45:05] __main__ INFO: Epoch 246 Step 39/39 lr 0.001000 loss 0.0489 (0.0473) acc@1 0.9844 (0.9864) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:05] __main__ INFO: Elapsed 1.72
[2022-06-16 11:45:05] __main__ INFO: Val 246
[2022-06-16 11:45:06] __main__ INFO: Epoch 246 loss 1.9236 acc@1 0.6552 acc@5 0.9632
[2022-06-16 11:45:06] __main__ INFO: Elapsed 0.76
[2022-06-16 11:45:06] __main__ INFO: Train 247 9594
[2022-06-16 11:45:07] __main__ INFO: Epoch 247 Step 39/39 lr 0.001000 loss 0.0472 (0.0505) acc@1 0.9844 (0.9842) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:07] __main__ INFO: Elapsed 1.50
[2022-06-16 11:45:07] __main__ INFO: Val 247
[2022-06-16 11:45:08] __main__ INFO: Epoch 247 loss 1.9221 acc@1 0.6535 acc@5 0.9637
[2022-06-16 11:45:08] __main__ INFO: Elapsed 0.70
[2022-06-16 11:45:08] __main__ INFO: Train 248 9633
[2022-06-16 11:45:10] __main__ INFO: Epoch 248 Step 39/39 lr 0.001000 loss 0.0700 (0.0430) acc@1 0.9766 (0.9896) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:10] __main__ INFO: Elapsed 1.65
[2022-06-16 11:45:10] __main__ INFO: Val 248
[2022-06-16 11:45:11] __main__ INFO: Epoch 248 loss 1.9216 acc@1 0.6551 acc@5 0.9627
[2022-06-16 11:45:11] __main__ INFO: Elapsed 0.74
[2022-06-16 11:45:11] __main__ INFO: Train 249 9672
[2022-06-16 11:45:12] __main__ INFO: Epoch 249 Step 39/39 lr 0.001000 loss 0.0838 (0.0529) acc@1 0.9766 (0.9860) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:12] __main__ INFO: Elapsed 1.77
[2022-06-16 11:45:12] __main__ INFO: Val 249
[2022-06-16 11:45:13] __main__ INFO: Epoch 249 loss 1.9144 acc@1 0.6546 acc@5 0.9639
[2022-06-16 11:45:13] __main__ INFO: Elapsed 0.72
[2022-06-16 11:45:13] __main__ INFO: Train 250 9711
[2022-06-16 11:45:15] __main__ INFO: Epoch 250 Step 39/39 lr 0.001000 loss 0.0253 (0.0477) acc@1 1.0000 (0.9878) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:15] __main__ INFO: Elapsed 1.63
[2022-06-16 11:45:15] __main__ INFO: Val 250
[2022-06-16 11:45:15] __main__ INFO: Epoch 250 loss 1.9179 acc@1 0.6544 acc@5 0.9624
[2022-06-16 11:45:15] __main__ INFO: Elapsed 0.75
[2022-06-16 11:45:15] __main__ INFO: Train 251 9750
[2022-06-16 11:45:17] __main__ INFO: Epoch 251 Step 39/39 lr 0.001000 loss 0.0399 (0.0495) acc@1 0.9766 (0.9868) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:17] __main__ INFO: Elapsed 1.71
[2022-06-16 11:45:17] __main__ INFO: Val 251
[2022-06-16 11:45:18] __main__ INFO: Epoch 251 loss 1.9415 acc@1 0.6536 acc@5 0.9630
[2022-06-16 11:45:18] __main__ INFO: Elapsed 0.78
[2022-06-16 11:45:18] __main__ INFO: Train 252 9789
[2022-06-16 11:45:20] __main__ INFO: Epoch 252 Step 39/39 lr 0.001000 loss 0.0751 (0.0503) acc@1 0.9766 (0.9864) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:20] __main__ INFO: Elapsed 1.73
[2022-06-16 11:45:20] __main__ INFO: Val 252
[2022-06-16 11:45:20] __main__ INFO: Epoch 252 loss 1.9368 acc@1 0.6552 acc@5 0.9631
[2022-06-16 11:45:20] __main__ INFO: Elapsed 0.78
[2022-06-16 11:45:20] __main__ INFO: Train 253 9828
[2022-06-16 11:45:22] __main__ INFO: Epoch 253 Step 39/39 lr 0.001000 loss 0.0413 (0.0487) acc@1 0.9844 (0.9856) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:22] __main__ INFO: Elapsed 1.71
[2022-06-16 11:45:22] __main__ INFO: Val 253
[2022-06-16 11:45:23] __main__ INFO: Epoch 253 loss 1.9368 acc@1 0.6540 acc@5 0.9629
[2022-06-16 11:45:23] __main__ INFO: Elapsed 0.69
[2022-06-16 11:45:23] __main__ INFO: Train 254 9867
[2022-06-16 11:45:25] __main__ INFO: Epoch 254 Step 39/39 lr 0.001000 loss 0.0684 (0.0454) acc@1 0.9688 (0.9902) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:25] __main__ INFO: Elapsed 1.69
[2022-06-16 11:45:25] __main__ INFO: Val 254
[2022-06-16 11:45:25] __main__ INFO: Epoch 254 loss 1.9241 acc@1 0.6557 acc@5 0.9632
[2022-06-16 11:45:25] __main__ INFO: Elapsed 0.84
[2022-06-16 11:45:25] __main__ INFO: Train 255 9906
[2022-06-16 11:45:27] __main__ INFO: Epoch 255 Step 39/39 lr 0.001000 loss 0.0630 (0.0512) acc@1 0.9844 (0.9854) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:27] __main__ INFO: Elapsed 1.67
[2022-06-16 11:45:27] __main__ INFO: Val 255
[2022-06-16 11:45:28] __main__ INFO: Epoch 255 loss 1.9418 acc@1 0.6553 acc@5 0.9623
[2022-06-16 11:45:28] __main__ INFO: Elapsed 0.70
[2022-06-16 11:45:28] __main__ INFO: Train 256 9945
[2022-06-16 11:45:29] __main__ INFO: Epoch 256 Step 39/39 lr 0.001000 loss 0.1034 (0.0537) acc@1 0.9609 (0.9860) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:29] __main__ INFO: Elapsed 1.70
[2022-06-16 11:45:29] __main__ INFO: Val 256
[2022-06-16 11:45:30] __main__ INFO: Epoch 256 loss 1.9336 acc@1 0.6553 acc@5 0.9621
[2022-06-16 11:45:30] __main__ INFO: Elapsed 0.70
[2022-06-16 11:45:30] __main__ INFO: Train 257 9984
[2022-06-16 11:45:32] __main__ INFO: Epoch 257 Step 39/39 lr 0.001000 loss 0.0499 (0.0513) acc@1 1.0000 (0.9856) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:32] __main__ INFO: Elapsed 1.69
[2022-06-16 11:45:32] __main__ INFO: Val 257
[2022-06-16 11:45:33] __main__ INFO: Epoch 257 loss 1.9397 acc@1 0.6551 acc@5 0.9623
[2022-06-16 11:45:33] __main__ INFO: Elapsed 0.74
[2022-06-16 11:45:33] __main__ INFO: Train 258 10023
[2022-06-16 11:45:34] __main__ INFO: Epoch 258 Step 39/39 lr 0.001000 loss 0.0442 (0.0450) acc@1 0.9922 (0.9890) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:34] __main__ INFO: Elapsed 1.70
[2022-06-16 11:45:34] __main__ INFO: Val 258
[2022-06-16 11:45:35] __main__ INFO: Epoch 258 loss 1.9410 acc@1 0.6555 acc@5 0.9630
[2022-06-16 11:45:35] __main__ INFO: Elapsed 0.68
[2022-06-16 11:45:35] __main__ INFO: Train 259 10062
[2022-06-16 11:45:37] __main__ INFO: Epoch 259 Step 39/39 lr 0.001000 loss 0.0684 (0.0516) acc@1 0.9688 (0.9848) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:37] __main__ INFO: Elapsed 1.66
[2022-06-16 11:45:37] __main__ INFO: Val 259
[2022-06-16 11:45:37] __main__ INFO: Epoch 259 loss 1.9419 acc@1 0.6570 acc@5 0.9630
[2022-06-16 11:45:37] __main__ INFO: Elapsed 0.83
[2022-06-16 11:45:37] __main__ INFO: Train 260 10101
[2022-06-16 11:45:39] __main__ INFO: Epoch 260 Step 39/39 lr 0.001000 loss 0.0441 (0.0513) acc@1 0.9922 (0.9842) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:39] __main__ INFO: Elapsed 1.66
[2022-06-16 11:45:39] __main__ INFO: Val 260
[2022-06-16 11:45:40] __main__ INFO: Epoch 260 loss 1.9353 acc@1 0.6552 acc@5 0.9638
[2022-06-16 11:45:40] __main__ INFO: Elapsed 0.76
[2022-06-16 11:45:40] __main__ INFO: Train 261 10140
[2022-06-16 11:45:42] __main__ INFO: Epoch 261 Step 39/39 lr 0.001000 loss 0.0816 (0.0427) acc@1 0.9844 (0.9888) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:42] __main__ INFO: Elapsed 1.64
[2022-06-16 11:45:42] __main__ INFO: Val 261
[2022-06-16 11:45:42] __main__ INFO: Epoch 261 loss 1.9433 acc@1 0.6570 acc@5 0.9630
[2022-06-16 11:45:42] __main__ INFO: Elapsed 0.78
[2022-06-16 11:45:42] __main__ INFO: Train 262 10179
[2022-06-16 11:45:44] __main__ INFO: Epoch 262 Step 39/39 lr 0.001000 loss 0.0624 (0.0476) acc@1 0.9844 (0.9860) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:44] __main__ INFO: Elapsed 1.69
[2022-06-16 11:45:44] __main__ INFO: Val 262
[2022-06-16 11:45:45] __main__ INFO: Epoch 262 loss 1.9388 acc@1 0.6567 acc@5 0.9633
[2022-06-16 11:45:45] __main__ INFO: Elapsed 0.81
[2022-06-16 11:45:45] __main__ INFO: Train 263 10218
[2022-06-16 11:45:46] __main__ INFO: Epoch 263 Step 39/39 lr 0.001000 loss 0.0337 (0.0523) acc@1 0.9922 (0.9834) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:47] __main__ INFO: Elapsed 1.69
[2022-06-16 11:45:47] __main__ INFO: Val 263
[2022-06-16 11:45:47] __main__ INFO: Epoch 263 loss 1.9457 acc@1 0.6550 acc@5 0.9624
[2022-06-16 11:45:47] __main__ INFO: Elapsed 0.77
[2022-06-16 11:45:47] __main__ INFO: Train 264 10257
[2022-06-16 11:45:49] __main__ INFO: Epoch 264 Step 39/39 lr 0.001000 loss 0.0812 (0.0509) acc@1 0.9766 (0.9860) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:49] __main__ INFO: Elapsed 1.61
[2022-06-16 11:45:49] __main__ INFO: Val 264
[2022-06-16 11:45:50] __main__ INFO: Epoch 264 loss 1.9488 acc@1 0.6552 acc@5 0.9624
[2022-06-16 11:45:50] __main__ INFO: Elapsed 0.76
[2022-06-16 11:45:50] __main__ INFO: Train 265 10296
[2022-06-16 11:45:51] __main__ INFO: Epoch 265 Step 39/39 lr 0.001000 loss 0.0660 (0.0482) acc@1 0.9688 (0.9858) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:51] __main__ INFO: Elapsed 1.74
[2022-06-16 11:45:51] __main__ INFO: Val 265
[2022-06-16 11:45:52] __main__ INFO: Epoch 265 loss 1.9424 acc@1 0.6534 acc@5 0.9631
[2022-06-16 11:45:52] __main__ INFO: Elapsed 0.77
[2022-06-16 11:45:52] __main__ INFO: Train 266 10335
[2022-06-16 11:45:54] __main__ INFO: Epoch 266 Step 39/39 lr 0.001000 loss 0.0404 (0.0433) acc@1 0.9922 (0.9890) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:54] __main__ INFO: Elapsed 1.73
[2022-06-16 11:45:54] __main__ INFO: Val 266
[2022-06-16 11:45:55] __main__ INFO: Epoch 266 loss 1.9579 acc@1 0.6549 acc@5 0.9623
[2022-06-16 11:45:55] __main__ INFO: Elapsed 0.73
[2022-06-16 11:45:55] __main__ INFO: Train 267 10374
[2022-06-16 11:45:56] __main__ INFO: Epoch 267 Step 39/39 lr 0.001000 loss 0.0318 (0.0484) acc@1 0.9922 (0.9872) acc@5 1.0000 (1.0000)
[2022-06-16 11:45:56] __main__ INFO: Elapsed 1.59
[2022-06-16 11:45:56] __main__ INFO: Val 267
[2022-06-16 11:45:57] __main__ INFO: Epoch 267 loss 1.9600 acc@1 0.6538 acc@5 0.9616
[2022-06-16 11:45:57] __main__ INFO: Elapsed 0.77
[2022-06-16 11:45:57] __main__ INFO: Train 268 10413
[2022-06-16 11:45:59] __main__ INFO: Epoch 268 Step 39/39 lr 0.001000 loss 0.0684 (0.0449) acc@1 0.9688 (0.9888) acc@5 1.0000 (0.9998)
[2022-06-16 11:45:59] __main__ INFO: Elapsed 1.70
[2022-06-16 11:45:59] __main__ INFO: Val 268
[2022-06-16 11:46:00] __main__ INFO: Epoch 268 loss 1.9538 acc@1 0.6549 acc@5 0.9633
[2022-06-16 11:46:00] __main__ INFO: Elapsed 0.77
[2022-06-16 11:46:00] __main__ INFO: Train 269 10452
[2022-06-16 11:46:01] __main__ INFO: Epoch 269 Step 39/39 lr 0.001000 loss 0.0336 (0.0439) acc@1 0.9922 (0.9886) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:01] __main__ INFO: Elapsed 1.59
[2022-06-16 11:46:01] __main__ INFO: Val 269
[2022-06-16 11:46:02] __main__ INFO: Epoch 269 loss 1.9644 acc@1 0.6529 acc@5 0.9627
[2022-06-16 11:46:02] __main__ INFO: Elapsed 0.76
[2022-06-16 11:46:02] __main__ INFO: Train 270 10491
[2022-06-16 11:46:04] __main__ INFO: Epoch 270 Step 39/39 lr 0.001000 loss 0.0358 (0.0516) acc@1 0.9922 (0.9868) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:04] __main__ INFO: Elapsed 1.70
[2022-06-16 11:46:04] __main__ INFO: Val 270
[2022-06-16 11:46:04] __main__ INFO: Epoch 270 loss 1.9383 acc@1 0.6552 acc@5 0.9628
[2022-06-16 11:46:04] __main__ INFO: Elapsed 0.69
[2022-06-16 11:46:04] __main__ INFO: Train 271 10530
[2022-06-16 11:46:06] __main__ INFO: Epoch 271 Step 39/39 lr 0.001000 loss 0.0776 (0.0483) acc@1 0.9844 (0.9858) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:06] __main__ INFO: Elapsed 1.68
[2022-06-16 11:46:06] __main__ INFO: Val 271
[2022-06-16 11:46:07] __main__ INFO: Epoch 271 loss 1.9601 acc@1 0.6542 acc@5 0.9627
[2022-06-16 11:46:07] __main__ INFO: Elapsed 0.66
[2022-06-16 11:46:07] __main__ INFO: Train 272 10569
[2022-06-16 11:46:08] __main__ INFO: Epoch 272 Step 39/39 lr 0.001000 loss 0.1060 (0.0561) acc@1 0.9609 (0.9846) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:08] __main__ INFO: Elapsed 1.66
[2022-06-16 11:46:08] __main__ INFO: Val 272
[2022-06-16 11:46:09] __main__ INFO: Epoch 272 loss 1.9730 acc@1 0.6521 acc@5 0.9623
[2022-06-16 11:46:09] __main__ INFO: Elapsed 0.76
[2022-06-16 11:46:09] __main__ INFO: Train 273 10608
[2022-06-16 11:46:11] __main__ INFO: Epoch 273 Step 39/39 lr 0.001000 loss 0.0798 (0.0526) acc@1 0.9688 (0.9848) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:11] __main__ INFO: Elapsed 1.73
[2022-06-16 11:46:11] __main__ INFO: Val 273
[2022-06-16 11:46:12] __main__ INFO: Epoch 273 loss 1.9613 acc@1 0.6540 acc@5 0.9617
[2022-06-16 11:46:12] __main__ INFO: Elapsed 0.73
[2022-06-16 11:46:12] __main__ INFO: Train 274 10647
[2022-06-16 11:46:13] __main__ INFO: Epoch 274 Step 39/39 lr 0.001000 loss 0.0318 (0.0447) acc@1 0.9922 (0.9864) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:13] __main__ INFO: Elapsed 1.62
[2022-06-16 11:46:13] __main__ INFO: Val 274
[2022-06-16 11:46:14] __main__ INFO: Epoch 274 loss 1.9624 acc@1 0.6534 acc@5 0.9625
[2022-06-16 11:46:14] __main__ INFO: Elapsed 0.71
[2022-06-16 11:46:14] __main__ INFO: Train 275 10686
[2022-06-16 11:46:15] __main__ INFO: Epoch 275 Step 39/39 lr 0.001000 loss 0.0314 (0.0450) acc@1 1.0000 (0.9888) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:16] __main__ INFO: Elapsed 1.70
[2022-06-16 11:46:16] __main__ INFO: Val 275
[2022-06-16 11:46:16] __main__ INFO: Epoch 275 loss 1.9588 acc@1 0.6532 acc@5 0.9628
[2022-06-16 11:46:16] __main__ INFO: Elapsed 0.75
[2022-06-16 11:46:16] __main__ INFO: Train 276 10725
[2022-06-16 11:46:18] __main__ INFO: Epoch 276 Step 39/39 lr 0.001000 loss 0.0431 (0.0454) acc@1 0.9844 (0.9880) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:18] __main__ INFO: Elapsed 1.70
[2022-06-16 11:46:18] __main__ INFO: Val 276
[2022-06-16 11:46:19] __main__ INFO: Epoch 276 loss 1.9557 acc@1 0.6543 acc@5 0.9623
[2022-06-16 11:46:19] __main__ INFO: Elapsed 0.75
[2022-06-16 11:46:19] __main__ INFO: Train 277 10764
[2022-06-16 11:46:20] __main__ INFO: Epoch 277 Step 39/39 lr 0.001000 loss 0.0328 (0.0453) acc@1 0.9922 (0.9878) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:20] __main__ INFO: Elapsed 1.66
[2022-06-16 11:46:20] __main__ INFO: Val 277
[2022-06-16 11:46:21] __main__ INFO: Epoch 277 loss 1.9593 acc@1 0.6556 acc@5 0.9629
[2022-06-16 11:46:21] __main__ INFO: Elapsed 0.68
[2022-06-16 11:46:21] __main__ INFO: Train 278 10803
[2022-06-16 11:46:23] __main__ INFO: Epoch 278 Step 39/39 lr 0.001000 loss 0.0744 (0.0514) acc@1 0.9688 (0.9848) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:23] __main__ INFO: Elapsed 1.67
[2022-06-16 11:46:23] __main__ INFO: Val 278
[2022-06-16 11:46:24] __main__ INFO: Epoch 278 loss 1.9487 acc@1 0.6562 acc@5 0.9623
[2022-06-16 11:46:24] __main__ INFO: Elapsed 0.77
[2022-06-16 11:46:24] __main__ INFO: Train 279 10842
[2022-06-16 11:46:25] __main__ INFO: Epoch 279 Step 39/39 lr 0.001000 loss 0.0496 (0.0453) acc@1 0.9922 (0.9884) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:25] __main__ INFO: Elapsed 1.63
[2022-06-16 11:46:25] __main__ INFO: Val 279
[2022-06-16 11:46:26] __main__ INFO: Epoch 279 loss 1.9646 acc@1 0.6544 acc@5 0.9631
[2022-06-16 11:46:26] __main__ INFO: Elapsed 0.82
[2022-06-16 11:46:26] __main__ INFO: Train 280 10881
[2022-06-16 11:46:28] __main__ INFO: Epoch 280 Step 39/39 lr 0.001000 loss 0.0608 (0.0476) acc@1 0.9844 (0.9864) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:28] __main__ INFO: Elapsed 1.69
[2022-06-16 11:46:28] __main__ INFO: Val 280
[2022-06-16 11:46:28] __main__ INFO: Epoch 280 loss 1.9545 acc@1 0.6559 acc@5 0.9629
[2022-06-16 11:46:28] __main__ INFO: Elapsed 0.79
[2022-06-16 11:46:28] __main__ INFO: Train 281 10920
[2022-06-16 11:46:30] __main__ INFO: Epoch 281 Step 39/39 lr 0.001000 loss 0.0543 (0.0447) acc@1 0.9844 (0.9880) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:30] __main__ INFO: Elapsed 1.73
[2022-06-16 11:46:30] __main__ INFO: Val 281
[2022-06-16 11:46:31] __main__ INFO: Epoch 281 loss 1.9567 acc@1 0.6564 acc@5 0.9621
[2022-06-16 11:46:31] __main__ INFO: Elapsed 0.78
[2022-06-16 11:46:31] __main__ INFO: Train 282 10959
[2022-06-16 11:46:33] __main__ INFO: Epoch 282 Step 39/39 lr 0.001000 loss 0.0358 (0.0456) acc@1 0.9844 (0.9874) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:33] __main__ INFO: Elapsed 1.64
[2022-06-16 11:46:33] __main__ INFO: Val 282
[2022-06-16 11:46:33] __main__ INFO: Epoch 282 loss 1.9564 acc@1 0.6553 acc@5 0.9627
[2022-06-16 11:46:33] __main__ INFO: Elapsed 0.69
[2022-06-16 11:46:33] __main__ INFO: Train 283 10998
[2022-06-16 11:46:35] __main__ INFO: Epoch 283 Step 39/39 lr 0.001000 loss 0.0399 (0.0440) acc@1 0.9922 (0.9902) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:35] __main__ INFO: Elapsed 1.68
[2022-06-16 11:46:35] __main__ INFO: Val 283
[2022-06-16 11:46:36] __main__ INFO: Epoch 283 loss 1.9710 acc@1 0.6555 acc@5 0.9628
[2022-06-16 11:46:36] __main__ INFO: Elapsed 0.85
[2022-06-16 11:46:36] __main__ INFO: Train 284 11037
[2022-06-16 11:46:37] __main__ INFO: Epoch 284 Step 39/39 lr 0.001000 loss 0.0517 (0.0496) acc@1 0.9922 (0.9868) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:37] __main__ INFO: Elapsed 1.62
[2022-06-16 11:46:37] __main__ INFO: Val 284
[2022-06-16 11:46:38] __main__ INFO: Epoch 284 loss 1.9677 acc@1 0.6524 acc@5 0.9626
[2022-06-16 11:46:38] __main__ INFO: Elapsed 0.80
[2022-06-16 11:46:38] __main__ INFO: Train 285 11076
[2022-06-16 11:46:40] __main__ INFO: Epoch 285 Step 39/39 lr 0.001000 loss 0.0297 (0.0432) acc@1 0.9922 (0.9882) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:40] __main__ INFO: Elapsed 1.73
[2022-06-16 11:46:40] __main__ INFO: Val 285
[2022-06-16 11:46:41] __main__ INFO: Epoch 285 loss 1.9771 acc@1 0.6555 acc@5 0.9622
[2022-06-16 11:46:41] __main__ INFO: Elapsed 0.73
[2022-06-16 11:46:41] __main__ INFO: Train 286 11115
[2022-06-16 11:46:42] __main__ INFO: Epoch 286 Step 39/39 lr 0.001000 loss 0.0848 (0.0443) acc@1 0.9609 (0.9876) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:42] __main__ INFO: Elapsed 1.70
[2022-06-16 11:46:42] __main__ INFO: Val 286
[2022-06-16 11:46:43] __main__ INFO: Epoch 286 loss 1.9756 acc@1 0.6537 acc@5 0.9629
[2022-06-16 11:46:43] __main__ INFO: Elapsed 0.70
[2022-06-16 11:46:43] __main__ INFO: Train 287 11154
[2022-06-16 11:46:45] __main__ INFO: Epoch 287 Step 39/39 lr 0.001000 loss 0.0553 (0.0453) acc@1 0.9844 (0.9880) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:45] __main__ INFO: Elapsed 1.61
[2022-06-16 11:46:45] __main__ INFO: Val 287
[2022-06-16 11:46:46] __main__ INFO: Epoch 287 loss 1.9806 acc@1 0.6554 acc@5 0.9625
[2022-06-16 11:46:46] __main__ INFO: Elapsed 0.76
[2022-06-16 11:46:46] __main__ INFO: Train 288 11193
[2022-06-16 11:46:47] __main__ INFO: Epoch 288 Step 39/39 lr 0.001000 loss 0.0331 (0.0435) acc@1 0.9922 (0.9884) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:47] __main__ INFO: Elapsed 1.72
[2022-06-16 11:46:47] __main__ INFO: Val 288
[2022-06-16 11:46:48] __main__ INFO: Epoch 288 loss 1.9822 acc@1 0.6550 acc@5 0.9628
[2022-06-16 11:46:48] __main__ INFO: Elapsed 0.68
[2022-06-16 11:46:48] __main__ INFO: Train 289 11232
[2022-06-16 11:46:50] __main__ INFO: Epoch 289 Step 39/39 lr 0.001000 loss 0.0401 (0.0468) acc@1 0.9922 (0.9860) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:50] __main__ INFO: Elapsed 1.68
[2022-06-16 11:46:50] __main__ INFO: Val 289
[2022-06-16 11:46:50] __main__ INFO: Epoch 289 loss 1.9725 acc@1 0.6560 acc@5 0.9629
[2022-06-16 11:46:50] __main__ INFO: Elapsed 0.74
[2022-06-16 11:46:50] __main__ INFO: Train 290 11271
[2022-06-16 11:46:52] __main__ INFO: Epoch 290 Step 39/39 lr 0.001000 loss 0.0463 (0.0447) acc@1 0.9844 (0.9876) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:52] __main__ INFO: Elapsed 1.71
[2022-06-16 11:46:52] __main__ INFO: Val 290
[2022-06-16 11:46:53] __main__ INFO: Epoch 290 loss 1.9649 acc@1 0.6547 acc@5 0.9640
[2022-06-16 11:46:53] __main__ INFO: Elapsed 0.72
[2022-06-16 11:46:53] __main__ INFO: Train 291 11310
[2022-06-16 11:46:54] __main__ INFO: Epoch 291 Step 39/39 lr 0.001000 loss 0.0226 (0.0456) acc@1 1.0000 (0.9876) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:54] __main__ INFO: Elapsed 1.66
[2022-06-16 11:46:54] __main__ INFO: Val 291
[2022-06-16 11:46:55] __main__ INFO: Epoch 291 loss 1.9791 acc@1 0.6554 acc@5 0.9625
[2022-06-16 11:46:55] __main__ INFO: Elapsed 0.79
[2022-06-16 11:46:55] __main__ INFO: Train 292 11349
[2022-06-16 11:46:57] __main__ INFO: Epoch 292 Step 39/39 lr 0.001000 loss 0.0676 (0.0445) acc@1 0.9766 (0.9882) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:57] __main__ INFO: Elapsed 1.66
[2022-06-16 11:46:57] __main__ INFO: Val 292
[2022-06-16 11:46:58] __main__ INFO: Epoch 292 loss 1.9809 acc@1 0.6546 acc@5 0.9625
[2022-06-16 11:46:58] __main__ INFO: Elapsed 0.79
[2022-06-16 11:46:58] __main__ INFO: Train 293 11388
[2022-06-16 11:46:59] __main__ INFO: Epoch 293 Step 39/39 lr 0.001000 loss 0.0173 (0.0410) acc@1 1.0000 (0.9906) acc@5 1.0000 (1.0000)
[2022-06-16 11:46:59] __main__ INFO: Elapsed 1.69
[2022-06-16 11:46:59] __main__ INFO: Val 293
[2022-06-16 11:47:00] __main__ INFO: Epoch 293 loss 1.9703 acc@1 0.6569 acc@5 0.9627
[2022-06-16 11:47:00] __main__ INFO: Elapsed 0.74
[2022-06-16 11:47:00] __main__ INFO: Train 294 11427
[2022-06-16 11:47:02] __main__ INFO: Epoch 294 Step 39/39 lr 0.001000 loss 0.0413 (0.0430) acc@1 0.9922 (0.9872) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:02] __main__ INFO: Elapsed 1.72
[2022-06-16 11:47:02] __main__ INFO: Val 294
[2022-06-16 11:47:03] __main__ INFO: Epoch 294 loss 1.9739 acc@1 0.6558 acc@5 0.9633
[2022-06-16 11:47:03] __main__ INFO: Elapsed 0.73
[2022-06-16 11:47:03] __main__ INFO: Train 295 11466
[2022-06-16 11:47:04] __main__ INFO: Epoch 295 Step 39/39 lr 0.001000 loss 0.0623 (0.0367) acc@1 0.9609 (0.9910) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:04] __main__ INFO: Elapsed 1.73
[2022-06-16 11:47:04] __main__ INFO: Val 295
[2022-06-16 11:47:05] __main__ INFO: Epoch 295 loss 1.9767 acc@1 0.6549 acc@5 0.9632
[2022-06-16 11:47:05] __main__ INFO: Elapsed 0.77
[2022-06-16 11:47:05] __main__ INFO: Train 296 11505
[2022-06-16 11:47:07] __main__ INFO: Epoch 296 Step 39/39 lr 0.001000 loss 0.0670 (0.0418) acc@1 0.9688 (0.9876) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:07] __main__ INFO: Elapsed 1.63
[2022-06-16 11:47:07] __main__ INFO: Val 296
[2022-06-16 11:47:08] __main__ INFO: Epoch 296 loss 1.9865 acc@1 0.6556 acc@5 0.9631
[2022-06-16 11:47:08] __main__ INFO: Elapsed 0.81
[2022-06-16 11:47:08] __main__ INFO: Train 297 11544
[2022-06-16 11:47:09] __main__ INFO: Epoch 297 Step 39/39 lr 0.001000 loss 0.0467 (0.0447) acc@1 0.9766 (0.9876) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:09] __main__ INFO: Elapsed 1.75
[2022-06-16 11:47:09] __main__ INFO: Val 297
[2022-06-16 11:47:10] __main__ INFO: Epoch 297 loss 1.9810 acc@1 0.6557 acc@5 0.9643
[2022-06-16 11:47:10] __main__ INFO: Elapsed 0.72
[2022-06-16 11:47:10] __main__ INFO: Train 298 11583
[2022-06-16 11:47:12] __main__ INFO: Epoch 298 Step 39/39 lr 0.001000 loss 0.0214 (0.0425) acc@1 1.0000 (0.9892) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:12] __main__ INFO: Elapsed 1.75
[2022-06-16 11:47:12] __main__ INFO: Val 298
[2022-06-16 11:47:13] __main__ INFO: Epoch 298 loss 1.9738 acc@1 0.6577 acc@5 0.9621
[2022-06-16 11:47:13] __main__ INFO: Elapsed 0.77
[2022-06-16 11:47:13] __main__ INFO: Train 299 11622
[2022-06-16 11:47:14] __main__ INFO: Epoch 299 Step 39/39 lr 0.001000 loss 0.0437 (0.0394) acc@1 0.9844 (0.9912) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:14] __main__ INFO: Elapsed 1.64
[2022-06-16 11:47:14] __main__ INFO: Val 299
[2022-06-16 11:47:15] __main__ INFO: Epoch 299 loss 2.0062 acc@1 0.6541 acc@5 0.9619
[2022-06-16 11:47:15] __main__ INFO: Elapsed 0.73
[2022-06-16 11:47:15] __main__ INFO: Train 300 11661
[2022-06-16 11:47:17] __main__ INFO: Epoch 300 Step 39/39 lr 0.001000 loss 0.0298 (0.0424) acc@1 0.9922 (0.9884) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:17] __main__ INFO: Elapsed 1.73
[2022-06-16 11:47:17] __main__ INFO: Val 300
[2022-06-16 11:47:17] __main__ INFO: Epoch 300 loss 1.9937 acc@1 0.6566 acc@5 0.9623
[2022-06-16 11:47:17] __main__ INFO: Elapsed 0.74
[2022-06-16 11:47:17] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00300.pth
[2022-06-16 11:47:17] __main__ INFO: Train 301 11700
[2022-06-16 11:47:19] __main__ INFO: Epoch 301 Step 39/39 lr 0.001000 loss 0.0328 (0.0457) acc@1 0.9922 (0.9872) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:19] __main__ INFO: Elapsed 1.66
[2022-06-16 11:47:19] __main__ INFO: Val 301
[2022-06-16 11:47:20] __main__ INFO: Epoch 301 loss 1.9979 acc@1 0.6544 acc@5 0.9629
[2022-06-16 11:47:20] __main__ INFO: Elapsed 0.76
[2022-06-16 11:47:20] __main__ INFO: Train 302 11739
[2022-06-16 11:47:21] __main__ INFO: Epoch 302 Step 39/39 lr 0.001000 loss 0.0940 (0.0414) acc@1 0.9844 (0.9892) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:22] __main__ INFO: Elapsed 1.64
[2022-06-16 11:47:22] __main__ INFO: Val 302
[2022-06-16 11:47:22] __main__ INFO: Epoch 302 loss 1.9900 acc@1 0.6560 acc@5 0.9634
[2022-06-16 11:47:22] __main__ INFO: Elapsed 0.71
[2022-06-16 11:47:22] __main__ INFO: Train 303 11778
[2022-06-16 11:47:24] __main__ INFO: Epoch 303 Step 39/39 lr 0.001000 loss 0.0220 (0.0420) acc@1 1.0000 (0.9882) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:24] __main__ INFO: Elapsed 1.63
[2022-06-16 11:47:24] __main__ INFO: Val 303
[2022-06-16 11:47:25] __main__ INFO: Epoch 303 loss 1.9900 acc@1 0.6560 acc@5 0.9623
[2022-06-16 11:47:25] __main__ INFO: Elapsed 0.73
[2022-06-16 11:47:25] __main__ INFO: Train 304 11817
[2022-06-16 11:47:26] __main__ INFO: Epoch 304 Step 39/39 lr 0.001000 loss 0.0232 (0.0376) acc@1 1.0000 (0.9914) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:26] __main__ INFO: Elapsed 1.71
[2022-06-16 11:47:26] __main__ INFO: Val 304
[2022-06-16 11:47:27] __main__ INFO: Epoch 304 loss 1.9877 acc@1 0.6563 acc@5 0.9628
[2022-06-16 11:47:27] __main__ INFO: Elapsed 0.75
[2022-06-16 11:47:27] __main__ INFO: Train 305 11856
[2022-06-16 11:47:29] __main__ INFO: Epoch 305 Step 39/39 lr 0.001000 loss 0.0191 (0.0386) acc@1 1.0000 (0.9902) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:29] __main__ INFO: Elapsed 1.70
[2022-06-16 11:47:29] __main__ INFO: Val 305
[2022-06-16 11:47:30] __main__ INFO: Epoch 305 loss 1.9840 acc@1 0.6560 acc@5 0.9625
[2022-06-16 11:47:30] __main__ INFO: Elapsed 0.82
[2022-06-16 11:47:30] __main__ INFO: Train 306 11895
[2022-06-16 11:47:31] __main__ INFO: Epoch 306 Step 39/39 lr 0.001000 loss 0.0402 (0.0432) acc@1 0.9922 (0.9904) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:31] __main__ INFO: Elapsed 1.64
[2022-06-16 11:47:31] __main__ INFO: Val 306
[2022-06-16 11:47:32] __main__ INFO: Epoch 306 loss 2.0016 acc@1 0.6559 acc@5 0.9627
[2022-06-16 11:47:32] __main__ INFO: Elapsed 0.72
[2022-06-16 11:47:32] __main__ INFO: Train 307 11934
[2022-06-16 11:47:34] __main__ INFO: Epoch 307 Step 39/39 lr 0.001000 loss 0.0244 (0.0465) acc@1 1.0000 (0.9884) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:34] __main__ INFO: Elapsed 1.74
[2022-06-16 11:47:34] __main__ INFO: Val 307
[2022-06-16 11:47:34] __main__ INFO: Epoch 307 loss 1.9949 acc@1 0.6566 acc@5 0.9622
[2022-06-16 11:47:34] __main__ INFO: Elapsed 0.72
[2022-06-16 11:47:34] __main__ INFO: Train 308 11973
[2022-06-16 11:47:36] __main__ INFO: Epoch 308 Step 39/39 lr 0.001000 loss 0.0162 (0.0427) acc@1 1.0000 (0.9882) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:36] __main__ INFO: Elapsed 1.74
[2022-06-16 11:47:36] __main__ INFO: Val 308
[2022-06-16 11:47:37] __main__ INFO: Epoch 308 loss 2.0023 acc@1 0.6556 acc@5 0.9624
[2022-06-16 11:47:37] __main__ INFO: Elapsed 0.72
[2022-06-16 11:47:37] __main__ INFO: Train 309 12012
[2022-06-16 11:47:38] __main__ INFO: Epoch 309 Step 39/39 lr 0.001000 loss 0.0596 (0.0474) acc@1 0.9766 (0.9864) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:38] __main__ INFO: Elapsed 1.58
[2022-06-16 11:47:38] __main__ INFO: Val 309
[2022-06-16 11:47:39] __main__ INFO: Epoch 309 loss 2.0298 acc@1 0.6528 acc@5 0.9614
[2022-06-16 11:47:39] __main__ INFO: Elapsed 0.73
[2022-06-16 11:47:39] __main__ INFO: Train 310 12051
[2022-06-16 11:47:41] __main__ INFO: Epoch 310 Step 39/39 lr 0.001000 loss 0.0809 (0.0393) acc@1 0.9844 (0.9904) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:41] __main__ INFO: Elapsed 1.68
[2022-06-16 11:47:41] __main__ INFO: Val 310
[2022-06-16 11:47:42] __main__ INFO: Epoch 310 loss 2.0083 acc@1 0.6550 acc@5 0.9614
[2022-06-16 11:47:42] __main__ INFO: Elapsed 0.80
[2022-06-16 11:47:42] __main__ INFO: Train 311 12090
[2022-06-16 11:47:43] __main__ INFO: Epoch 311 Step 39/39 lr 0.001000 loss 0.0313 (0.0418) acc@1 1.0000 (0.9898) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:43] __main__ INFO: Elapsed 1.69
[2022-06-16 11:47:43] __main__ INFO: Val 311
[2022-06-16 11:47:44] __main__ INFO: Epoch 311 loss 2.0062 acc@1 0.6569 acc@5 0.9625
[2022-06-16 11:47:44] __main__ INFO: Elapsed 0.74
[2022-06-16 11:47:44] __main__ INFO: Train 312 12129
[2022-06-16 11:47:46] __main__ INFO: Epoch 312 Step 39/39 lr 0.001000 loss 0.0725 (0.0422) acc@1 0.9844 (0.9886) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:46] __main__ INFO: Elapsed 1.70
[2022-06-16 11:47:46] __main__ INFO: Val 312
[2022-06-16 11:47:47] __main__ INFO: Epoch 312 loss 2.0093 acc@1 0.6533 acc@5 0.9623
[2022-06-16 11:47:47] __main__ INFO: Elapsed 0.80
[2022-06-16 11:47:47] __main__ INFO: Train 313 12168
[2022-06-16 11:47:48] __main__ INFO: Epoch 313 Step 39/39 lr 0.001000 loss 0.0306 (0.0410) acc@1 1.0000 (0.9906) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:48] __main__ INFO: Elapsed 1.62
[2022-06-16 11:47:48] __main__ INFO: Val 313
[2022-06-16 11:47:49] __main__ INFO: Epoch 313 loss 1.9992 acc@1 0.6558 acc@5 0.9619
[2022-06-16 11:47:49] __main__ INFO: Elapsed 0.74
[2022-06-16 11:47:49] __main__ INFO: Train 314 12207
[2022-06-16 11:47:51] __main__ INFO: Epoch 314 Step 39/39 lr 0.001000 loss 0.0319 (0.0396) acc@1 0.9922 (0.9908) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:51] __main__ INFO: Elapsed 1.66
[2022-06-16 11:47:51] __main__ INFO: Val 314
[2022-06-16 11:47:51] __main__ INFO: Epoch 314 loss 2.0074 acc@1 0.6560 acc@5 0.9622
[2022-06-16 11:47:51] __main__ INFO: Elapsed 0.72
[2022-06-16 11:47:51] __main__ INFO: Train 315 12246
[2022-06-16 11:47:53] __main__ INFO: Epoch 315 Step 39/39 lr 0.001000 loss 0.0449 (0.0370) acc@1 0.9766 (0.9894) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:53] __main__ INFO: Elapsed 1.70
[2022-06-16 11:47:53] __main__ INFO: Val 315
[2022-06-16 11:47:54] __main__ INFO: Epoch 315 loss 2.0033 acc@1 0.6555 acc@5 0.9621
[2022-06-16 11:47:54] __main__ INFO: Elapsed 0.77
[2022-06-16 11:47:54] __main__ INFO: Train 316 12285
[2022-06-16 11:47:55] __main__ INFO: Epoch 316 Step 39/39 lr 0.001000 loss 0.0494 (0.0381) acc@1 0.9844 (0.9908) acc@5 1.0000 (0.9998)
[2022-06-16 11:47:55] __main__ INFO: Elapsed 1.61
[2022-06-16 11:47:55] __main__ INFO: Val 316
[2022-06-16 11:47:56] __main__ INFO: Epoch 316 loss 1.9924 acc@1 0.6563 acc@5 0.9627
[2022-06-16 11:47:56] __main__ INFO: Elapsed 0.74
[2022-06-16 11:47:56] __main__ INFO: Train 317 12324
[2022-06-16 11:47:58] __main__ INFO: Epoch 317 Step 39/39 lr 0.001000 loss 0.0287 (0.0405) acc@1 0.9922 (0.9902) acc@5 1.0000 (1.0000)
[2022-06-16 11:47:58] __main__ INFO: Elapsed 1.63
[2022-06-16 11:47:58] __main__ INFO: Val 317
[2022-06-16 11:47:59] __main__ INFO: Epoch 317 loss 2.0081 acc@1 0.6560 acc@5 0.9611
[2022-06-16 11:47:59] __main__ INFO: Elapsed 0.77
[2022-06-16 11:47:59] __main__ INFO: Train 318 12363
[2022-06-16 11:48:00] __main__ INFO: Epoch 318 Step 39/39 lr 0.001000 loss 0.0512 (0.0372) acc@1 0.9844 (0.9920) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:00] __main__ INFO: Elapsed 1.65
[2022-06-16 11:48:00] __main__ INFO: Val 318
[2022-06-16 11:48:01] __main__ INFO: Epoch 318 loss 2.0067 acc@1 0.6557 acc@5 0.9636
[2022-06-16 11:48:01] __main__ INFO: Elapsed 0.68
[2022-06-16 11:48:01] __main__ INFO: Train 319 12402
[2022-06-16 11:48:03] __main__ INFO: Epoch 319 Step 39/39 lr 0.001000 loss 0.0285 (0.0408) acc@1 1.0000 (0.9910) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:03] __main__ INFO: Elapsed 1.63
[2022-06-16 11:48:03] __main__ INFO: Val 319
[2022-06-16 11:48:03] __main__ INFO: Epoch 319 loss 2.0178 acc@1 0.6541 acc@5 0.9624
[2022-06-16 11:48:03] __main__ INFO: Elapsed 0.82
[2022-06-16 11:48:03] __main__ INFO: Train 320 12441
[2022-06-16 11:48:05] __main__ INFO: Epoch 320 Step 39/39 lr 0.001000 loss 0.0307 (0.0403) acc@1 0.9922 (0.9884) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:05] __main__ INFO: Elapsed 1.66
[2022-06-16 11:48:05] __main__ INFO: Val 320
[2022-06-16 11:48:06] __main__ INFO: Epoch 320 loss 2.0147 acc@1 0.6546 acc@5 0.9627
[2022-06-16 11:48:06] __main__ INFO: Elapsed 0.72
[2022-06-16 11:48:06] __main__ INFO: Train 321 12480
[2022-06-16 11:48:07] __main__ INFO: Epoch 321 Step 39/39 lr 0.001000 loss 0.0355 (0.0388) acc@1 0.9922 (0.9912) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:07] __main__ INFO: Elapsed 1.59
[2022-06-16 11:48:07] __main__ INFO: Val 321
[2022-06-16 11:48:08] __main__ INFO: Epoch 321 loss 2.0204 acc@1 0.6545 acc@5 0.9632
[2022-06-16 11:48:08] __main__ INFO: Elapsed 0.79
[2022-06-16 11:48:08] __main__ INFO: Train 322 12519
[2022-06-16 11:48:10] __main__ INFO: Epoch 322 Step 39/39 lr 0.001000 loss 0.0553 (0.0377) acc@1 0.9766 (0.9906) acc@5 1.0000 (0.9998)
[2022-06-16 11:48:10] __main__ INFO: Elapsed 1.68
[2022-06-16 11:48:10] __main__ INFO: Val 322
[2022-06-16 11:48:11] __main__ INFO: Epoch 322 loss 2.0167 acc@1 0.6552 acc@5 0.9616
[2022-06-16 11:48:11] __main__ INFO: Elapsed 0.75
[2022-06-16 11:48:11] __main__ INFO: Train 323 12558
[2022-06-16 11:48:12] __main__ INFO: Epoch 323 Step 39/39 lr 0.001000 loss 0.0487 (0.0386) acc@1 0.9766 (0.9890) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:12] __main__ INFO: Elapsed 1.69
[2022-06-16 11:48:12] __main__ INFO: Val 323
[2022-06-16 11:48:13] __main__ INFO: Epoch 323 loss 2.0089 acc@1 0.6562 acc@5 0.9634
[2022-06-16 11:48:13] __main__ INFO: Elapsed 0.75
[2022-06-16 11:48:13] __main__ INFO: Train 324 12597
[2022-06-16 11:48:15] __main__ INFO: Epoch 324 Step 39/39 lr 0.001000 loss 0.0305 (0.0357) acc@1 1.0000 (0.9910) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:15] __main__ INFO: Elapsed 1.65
[2022-06-16 11:48:15] __main__ INFO: Val 324
[2022-06-16 11:48:15] __main__ INFO: Epoch 324 loss 2.0117 acc@1 0.6563 acc@5 0.9620
[2022-06-16 11:48:15] __main__ INFO: Elapsed 0.75
[2022-06-16 11:48:15] __main__ INFO: Train 325 12636
[2022-06-16 11:48:17] __main__ INFO: Epoch 325 Step 39/39 lr 0.001000 loss 0.0340 (0.0388) acc@1 0.9922 (0.9894) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:17] __main__ INFO: Elapsed 1.66
[2022-06-16 11:48:17] __main__ INFO: Val 325
[2022-06-16 11:48:18] __main__ INFO: Epoch 325 loss 2.0246 acc@1 0.6561 acc@5 0.9621
[2022-06-16 11:48:18] __main__ INFO: Elapsed 0.75
[2022-06-16 11:48:18] __main__ INFO: Train 326 12675
[2022-06-16 11:48:19] __main__ INFO: Epoch 326 Step 39/39 lr 0.001000 loss 0.0349 (0.0422) acc@1 0.9922 (0.9880) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:20] __main__ INFO: Elapsed 1.67
[2022-06-16 11:48:20] __main__ INFO: Val 326
[2022-06-16 11:48:20] __main__ INFO: Epoch 326 loss 2.0139 acc@1 0.6549 acc@5 0.9629
[2022-06-16 11:48:20] __main__ INFO: Elapsed 0.74
[2022-06-16 11:48:20] __main__ INFO: Train 327 12714
[2022-06-16 11:48:22] __main__ INFO: Epoch 327 Step 39/39 lr 0.001000 loss 0.0349 (0.0450) acc@1 0.9922 (0.9884) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:22] __main__ INFO: Elapsed 1.71
[2022-06-16 11:48:22] __main__ INFO: Val 327
[2022-06-16 11:48:23] __main__ INFO: Epoch 327 loss 2.0246 acc@1 0.6535 acc@5 0.9623
[2022-06-16 11:48:23] __main__ INFO: Elapsed 0.74
[2022-06-16 11:48:23] __main__ INFO: Train 328 12753
[2022-06-16 11:48:24] __main__ INFO: Epoch 328 Step 39/39 lr 0.001000 loss 0.0458 (0.0419) acc@1 0.9922 (0.9888) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:24] __main__ INFO: Elapsed 1.67
[2022-06-16 11:48:24] __main__ INFO: Val 328
[2022-06-16 11:48:25] __main__ INFO: Epoch 328 loss 2.0156 acc@1 0.6538 acc@5 0.9627
[2022-06-16 11:48:25] __main__ INFO: Elapsed 0.70
[2022-06-16 11:48:25] __main__ INFO: Train 329 12792
[2022-06-16 11:48:27] __main__ INFO: Epoch 329 Step 39/39 lr 0.001000 loss 0.0271 (0.0344) acc@1 0.9922 (0.9916) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:27] __main__ INFO: Elapsed 1.63
[2022-06-16 11:48:27] __main__ INFO: Val 329
[2022-06-16 11:48:28] __main__ INFO: Epoch 329 loss 2.0231 acc@1 0.6524 acc@5 0.9625
[2022-06-16 11:48:28] __main__ INFO: Elapsed 0.83
[2022-06-16 11:48:28] __main__ INFO: Train 330 12831
[2022-06-16 11:48:29] __main__ INFO: Epoch 330 Step 39/39 lr 0.001000 loss 0.0475 (0.0414) acc@1 0.9922 (0.9900) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:29] __main__ INFO: Elapsed 1.64
[2022-06-16 11:48:29] __main__ INFO: Val 330
[2022-06-16 11:48:30] __main__ INFO: Epoch 330 loss 2.0226 acc@1 0.6520 acc@5 0.9624
[2022-06-16 11:48:30] __main__ INFO: Elapsed 0.71
[2022-06-16 11:48:30] __main__ INFO: Train 331 12870
[2022-06-16 11:48:32] __main__ INFO: Epoch 331 Step 39/39 lr 0.001000 loss 0.0523 (0.0392) acc@1 0.9844 (0.9890) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:32] __main__ INFO: Elapsed 1.68
[2022-06-16 11:48:32] __main__ INFO: Val 331
[2022-06-16 11:48:32] __main__ INFO: Epoch 331 loss 2.0126 acc@1 0.6536 acc@5 0.9640
[2022-06-16 11:48:32] __main__ INFO: Elapsed 0.80
[2022-06-16 11:48:32] __main__ INFO: Train 332 12909
[2022-06-16 11:48:34] __main__ INFO: Epoch 332 Step 39/39 lr 0.001000 loss 0.0346 (0.0413) acc@1 1.0000 (0.9890) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:34] __main__ INFO: Elapsed 1.72
[2022-06-16 11:48:34] __main__ INFO: Val 332
[2022-06-16 11:48:35] __main__ INFO: Epoch 332 loss 2.0267 acc@1 0.6536 acc@5 0.9626
[2022-06-16 11:48:35] __main__ INFO: Elapsed 0.74
[2022-06-16 11:48:35] __main__ INFO: Train 333 12948
[2022-06-16 11:48:37] __main__ INFO: Epoch 333 Step 39/39 lr 0.001000 loss 0.0509 (0.0450) acc@1 0.9844 (0.9872) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:37] __main__ INFO: Elapsed 1.67
[2022-06-16 11:48:37] __main__ INFO: Val 333
[2022-06-16 11:48:37] __main__ INFO: Epoch 333 loss 2.0399 acc@1 0.6513 acc@5 0.9626
[2022-06-16 11:48:37] __main__ INFO: Elapsed 0.80
[2022-06-16 11:48:37] __main__ INFO: Train 334 12987
[2022-06-16 11:48:39] __main__ INFO: Epoch 334 Step 39/39 lr 0.001000 loss 0.0420 (0.0373) acc@1 0.9922 (0.9912) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:39] __main__ INFO: Elapsed 1.68
[2022-06-16 11:48:39] __main__ INFO: Val 334
[2022-06-16 11:48:40] __main__ INFO: Epoch 334 loss 2.0280 acc@1 0.6516 acc@5 0.9634
[2022-06-16 11:48:40] __main__ INFO: Elapsed 0.76
[2022-06-16 11:48:40] __main__ INFO: Train 335 13026
[2022-06-16 11:48:41] __main__ INFO: Epoch 335 Step 39/39 lr 0.001000 loss 0.0202 (0.0391) acc@1 1.0000 (0.9888) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:41] __main__ INFO: Elapsed 1.62
[2022-06-16 11:48:41] __main__ INFO: Val 335
[2022-06-16 11:48:42] __main__ INFO: Epoch 335 loss 2.0225 acc@1 0.6553 acc@5 0.9625
[2022-06-16 11:48:42] __main__ INFO: Elapsed 0.80
[2022-06-16 11:48:42] __main__ INFO: Train 336 13065
[2022-06-16 11:48:44] __main__ INFO: Epoch 336 Step 39/39 lr 0.001000 loss 0.0441 (0.0396) acc@1 0.9844 (0.9900) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:44] __main__ INFO: Elapsed 1.76
[2022-06-16 11:48:44] __main__ INFO: Val 336
[2022-06-16 11:48:45] __main__ INFO: Epoch 336 loss 2.0222 acc@1 0.6572 acc@5 0.9626
[2022-06-16 11:48:45] __main__ INFO: Elapsed 0.70
[2022-06-16 11:48:45] __main__ INFO: Train 337 13104
[2022-06-16 11:48:46] __main__ INFO: Epoch 337 Step 39/39 lr 0.001000 loss 0.0480 (0.0358) acc@1 0.9922 (0.9916) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:46] __main__ INFO: Elapsed 1.68
[2022-06-16 11:48:46] __main__ INFO: Val 337
[2022-06-16 11:48:47] __main__ INFO: Epoch 337 loss 2.0257 acc@1 0.6542 acc@5 0.9623
[2022-06-16 11:48:47] __main__ INFO: Elapsed 0.74
[2022-06-16 11:48:47] __main__ INFO: Train 338 13143
[2022-06-16 11:48:49] __main__ INFO: Epoch 338 Step 39/39 lr 0.001000 loss 0.0306 (0.0422) acc@1 0.9922 (0.9882) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:49] __main__ INFO: Elapsed 1.59
[2022-06-16 11:48:49] __main__ INFO: Val 338
[2022-06-16 11:48:49] __main__ INFO: Epoch 338 loss 2.0178 acc@1 0.6555 acc@5 0.9632
[2022-06-16 11:48:49] __main__ INFO: Elapsed 0.74
[2022-06-16 11:48:49] __main__ INFO: Train 339 13182
[2022-06-16 11:48:51] __main__ INFO: Epoch 339 Step 39/39 lr 0.001000 loss 0.0426 (0.0425) acc@1 0.9922 (0.9884) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:51] __main__ INFO: Elapsed 1.67
[2022-06-16 11:48:51] __main__ INFO: Val 339
[2022-06-16 11:48:52] __main__ INFO: Epoch 339 loss 2.0229 acc@1 0.6548 acc@5 0.9636
[2022-06-16 11:48:52] __main__ INFO: Elapsed 0.73
[2022-06-16 11:48:52] __main__ INFO: Train 340 13221
[2022-06-16 11:48:54] __main__ INFO: Epoch 340 Step 39/39 lr 0.001000 loss 0.0181 (0.0408) acc@1 0.9922 (0.9892) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:54] __main__ INFO: Elapsed 1.70
[2022-06-16 11:48:54] __main__ INFO: Val 340
[2022-06-16 11:48:54] __main__ INFO: Epoch 340 loss 2.0230 acc@1 0.6533 acc@5 0.9621
[2022-06-16 11:48:54] __main__ INFO: Elapsed 0.72
[2022-06-16 11:48:54] __main__ INFO: Train 341 13260
[2022-06-16 11:48:56] __main__ INFO: Epoch 341 Step 39/39 lr 0.001000 loss 0.0381 (0.0350) acc@1 0.9844 (0.9902) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:56] __main__ INFO: Elapsed 1.73
[2022-06-16 11:48:56] __main__ INFO: Val 341
[2022-06-16 11:48:57] __main__ INFO: Epoch 341 loss 2.0307 acc@1 0.6551 acc@5 0.9623
[2022-06-16 11:48:57] __main__ INFO: Elapsed 0.74
[2022-06-16 11:48:57] __main__ INFO: Train 342 13299
[2022-06-16 11:48:58] __main__ INFO: Epoch 342 Step 39/39 lr 0.001000 loss 0.0910 (0.0382) acc@1 0.9688 (0.9900) acc@5 1.0000 (1.0000)
[2022-06-16 11:48:58] __main__ INFO: Elapsed 1.63
[2022-06-16 11:48:58] __main__ INFO: Val 342
[2022-06-16 11:48:59] __main__ INFO: Epoch 342 loss 2.0558 acc@1 0.6531 acc@5 0.9623
[2022-06-16 11:48:59] __main__ INFO: Elapsed 0.69
[2022-06-16 11:48:59] __main__ INFO: Train 343 13338
[2022-06-16 11:49:01] __main__ INFO: Epoch 343 Step 39/39 lr 0.001000 loss 0.0399 (0.0393) acc@1 0.9844 (0.9898) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:01] __main__ INFO: Elapsed 1.67
[2022-06-16 11:49:01] __main__ INFO: Val 343
[2022-06-16 11:49:02] __main__ INFO: Epoch 343 loss 2.0326 acc@1 0.6535 acc@5 0.9631
[2022-06-16 11:49:02] __main__ INFO: Elapsed 0.75
[2022-06-16 11:49:02] __main__ INFO: Train 344 13377
[2022-06-16 11:49:03] __main__ INFO: Epoch 344 Step 39/39 lr 0.001000 loss 0.0368 (0.0366) acc@1 0.9922 (0.9920) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:03] __main__ INFO: Elapsed 1.67
[2022-06-16 11:49:03] __main__ INFO: Val 344
[2022-06-16 11:49:04] __main__ INFO: Epoch 344 loss 2.0387 acc@1 0.6528 acc@5 0.9629
[2022-06-16 11:49:04] __main__ INFO: Elapsed 0.76
[2022-06-16 11:49:04] __main__ INFO: Train 345 13416
[2022-06-16 11:49:05] __main__ INFO: Epoch 345 Step 39/39 lr 0.001000 loss 0.0379 (0.0367) acc@1 0.9922 (0.9908) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:06] __main__ INFO: Elapsed 1.59
[2022-06-16 11:49:06] __main__ INFO: Val 345
[2022-06-16 11:49:06] __main__ INFO: Epoch 345 loss 2.0405 acc@1 0.6525 acc@5 0.9629
[2022-06-16 11:49:06] __main__ INFO: Elapsed 0.79
[2022-06-16 11:49:06] __main__ INFO: Train 346 13455
[2022-06-16 11:49:08] __main__ INFO: Epoch 346 Step 39/39 lr 0.001000 loss 0.0327 (0.0391) acc@1 0.9922 (0.9900) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:08] __main__ INFO: Elapsed 1.64
[2022-06-16 11:49:08] __main__ INFO: Val 346
[2022-06-16 11:49:09] __main__ INFO: Epoch 346 loss 2.0410 acc@1 0.6532 acc@5 0.9625
[2022-06-16 11:49:09] __main__ INFO: Elapsed 0.75
[2022-06-16 11:49:09] __main__ INFO: Train 347 13494
[2022-06-16 11:49:10] __main__ INFO: Epoch 347 Step 39/39 lr 0.001000 loss 0.0294 (0.0339) acc@1 0.9922 (0.9912) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:10] __main__ INFO: Elapsed 1.65
[2022-06-16 11:49:10] __main__ INFO: Val 347
[2022-06-16 11:49:11] __main__ INFO: Epoch 347 loss 2.0448 acc@1 0.6541 acc@5 0.9625
[2022-06-16 11:49:11] __main__ INFO: Elapsed 0.71
[2022-06-16 11:49:11] __main__ INFO: Train 348 13533
[2022-06-16 11:49:13] __main__ INFO: Epoch 348 Step 39/39 lr 0.001000 loss 0.0211 (0.0418) acc@1 1.0000 (0.9882) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:13] __main__ INFO: Elapsed 1.65
[2022-06-16 11:49:13] __main__ INFO: Val 348
[2022-06-16 11:49:13] __main__ INFO: Epoch 348 loss 2.0365 acc@1 0.6548 acc@5 0.9622
[2022-06-16 11:49:13] __main__ INFO: Elapsed 0.72
[2022-06-16 11:49:13] __main__ INFO: Train 349 13572
[2022-06-16 11:49:15] __main__ INFO: Epoch 349 Step 39/39 lr 0.001000 loss 0.0493 (0.0361) acc@1 0.9844 (0.9900) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:15] __main__ INFO: Elapsed 1.63
[2022-06-16 11:49:15] __main__ INFO: Val 349
[2022-06-16 11:49:16] __main__ INFO: Epoch 349 loss 2.0367 acc@1 0.6538 acc@5 0.9623
[2022-06-16 11:49:16] __main__ INFO: Elapsed 0.78
[2022-06-16 11:49:16] __main__ INFO: Train 350 13611
[2022-06-16 11:49:17] __main__ INFO: Epoch 350 Step 39/39 lr 0.001000 loss 0.0388 (0.0350) acc@1 0.9844 (0.9904) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:18] __main__ INFO: Elapsed 1.66
[2022-06-16 11:49:18] __main__ INFO: Val 350
[2022-06-16 11:49:18] __main__ INFO: Epoch 350 loss 2.0317 acc@1 0.6563 acc@5 0.9628
[2022-06-16 11:49:18] __main__ INFO: Elapsed 0.67
[2022-06-16 11:49:18] __main__ INFO: Train 351 13650
[2022-06-16 11:49:20] __main__ INFO: Epoch 351 Step 39/39 lr 0.001000 loss 0.0210 (0.0338) acc@1 1.0000 (0.9930) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:20] __main__ INFO: Elapsed 1.67
[2022-06-16 11:49:20] __main__ INFO: Val 351
[2022-06-16 11:49:21] __main__ INFO: Epoch 351 loss 2.0312 acc@1 0.6555 acc@5 0.9618
[2022-06-16 11:49:21] __main__ INFO: Elapsed 0.75
[2022-06-16 11:49:21] __main__ INFO: Train 352 13689
[2022-06-16 11:49:22] __main__ INFO: Epoch 352 Step 39/39 lr 0.001000 loss 0.0224 (0.0384) acc@1 0.9922 (0.9902) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:22] __main__ INFO: Elapsed 1.68
[2022-06-16 11:49:22] __main__ INFO: Val 352
[2022-06-16 11:49:23] __main__ INFO: Epoch 352 loss 2.0408 acc@1 0.6563 acc@5 0.9619
[2022-06-16 11:49:23] __main__ INFO: Elapsed 0.75
[2022-06-16 11:49:23] __main__ INFO: Train 353 13728
[2022-06-16 11:49:25] __main__ INFO: Epoch 353 Step 39/39 lr 0.001000 loss 0.0447 (0.0392) acc@1 0.9844 (0.9910) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:25] __main__ INFO: Elapsed 1.64
[2022-06-16 11:49:25] __main__ INFO: Val 353
[2022-06-16 11:49:25] __main__ INFO: Epoch 353 loss 2.0423 acc@1 0.6566 acc@5 0.9624
[2022-06-16 11:49:25] __main__ INFO: Elapsed 0.77
[2022-06-16 11:49:25] __main__ INFO: Train 354 13767
[2022-06-16 11:49:27] __main__ INFO: Epoch 354 Step 39/39 lr 0.001000 loss 0.0343 (0.0332) acc@1 0.9922 (0.9918) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:27] __main__ INFO: Elapsed 1.69
[2022-06-16 11:49:27] __main__ INFO: Val 354
[2022-06-16 11:49:28] __main__ INFO: Epoch 354 loss 2.0450 acc@1 0.6553 acc@5 0.9629
[2022-06-16 11:49:28] __main__ INFO: Elapsed 0.77
[2022-06-16 11:49:28] __main__ INFO: Train 355 13806
[2022-06-16 11:49:30] __main__ INFO: Epoch 355 Step 39/39 lr 0.001000 loss 0.0484 (0.0373) acc@1 0.9766 (0.9900) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:30] __main__ INFO: Elapsed 1.65
[2022-06-16 11:49:30] __main__ INFO: Val 355
[2022-06-16 11:49:30] __main__ INFO: Epoch 355 loss 2.0523 acc@1 0.6566 acc@5 0.9636
[2022-06-16 11:49:30] __main__ INFO: Elapsed 0.75
[2022-06-16 11:49:30] __main__ INFO: Train 356 13845
[2022-06-16 11:49:32] __main__ INFO: Epoch 356 Step 39/39 lr 0.001000 loss 0.0330 (0.0375) acc@1 0.9922 (0.9900) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:32] __main__ INFO: Elapsed 1.66
[2022-06-16 11:49:32] __main__ INFO: Val 356
[2022-06-16 11:49:33] __main__ INFO: Epoch 356 loss 2.0505 acc@1 0.6543 acc@5 0.9630
[2022-06-16 11:49:33] __main__ INFO: Elapsed 0.69
[2022-06-16 11:49:33] __main__ INFO: Train 357 13884
[2022-06-16 11:49:34] __main__ INFO: Epoch 357 Step 39/39 lr 0.001000 loss 0.0251 (0.0336) acc@1 1.0000 (0.9922) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:34] __main__ INFO: Elapsed 1.68
[2022-06-16 11:49:34] __main__ INFO: Val 357
[2022-06-16 11:49:35] __main__ INFO: Epoch 357 loss 2.0406 acc@1 0.6584 acc@5 0.9630
[2022-06-16 11:49:35] __main__ INFO: Elapsed 0.76
[2022-06-16 11:49:35] __main__ INFO: Train 358 13923
[2022-06-16 11:49:37] __main__ INFO: Epoch 358 Step 39/39 lr 0.001000 loss 0.0174 (0.0354) acc@1 0.9922 (0.9908) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:37] __main__ INFO: Elapsed 1.67
[2022-06-16 11:49:37] __main__ INFO: Val 358
[2022-06-16 11:49:38] __main__ INFO: Epoch 358 loss 2.0584 acc@1 0.6554 acc@5 0.9620
[2022-06-16 11:49:38] __main__ INFO: Elapsed 0.76
[2022-06-16 11:49:38] __main__ INFO: Train 359 13962
[2022-06-16 11:49:39] __main__ INFO: Epoch 359 Step 39/39 lr 0.001000 loss 0.0443 (0.0341) acc@1 0.9844 (0.9922) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:39] __main__ INFO: Elapsed 1.67
[2022-06-16 11:49:39] __main__ INFO: Val 359
[2022-06-16 11:49:40] __main__ INFO: Epoch 359 loss 2.0475 acc@1 0.6567 acc@5 0.9626
[2022-06-16 11:49:40] __main__ INFO: Elapsed 0.80
[2022-06-16 11:49:40] __main__ INFO: Train 360 14001
[2022-06-16 11:49:42] __main__ INFO: Epoch 360 Step 39/39 lr 0.001000 loss 0.0327 (0.0362) acc@1 0.9922 (0.9886) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:42] __main__ INFO: Elapsed 1.69
[2022-06-16 11:49:42] __main__ INFO: Val 360
[2022-06-16 11:49:43] __main__ INFO: Epoch 360 loss 2.0623 acc@1 0.6561 acc@5 0.9636
[2022-06-16 11:49:43] __main__ INFO: Elapsed 0.75
[2022-06-16 11:49:43] __main__ INFO: Train 361 14040
[2022-06-16 11:49:44] __main__ INFO: Epoch 361 Step 39/39 lr 0.001000 loss 0.0367 (0.0307) acc@1 0.9922 (0.9928) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:44] __main__ INFO: Elapsed 1.68
[2022-06-16 11:49:44] __main__ INFO: Val 361
[2022-06-16 11:49:45] __main__ INFO: Epoch 361 loss 2.0611 acc@1 0.6566 acc@5 0.9627
[2022-06-16 11:49:45] __main__ INFO: Elapsed 0.69
[2022-06-16 11:49:45] __main__ INFO: Train 362 14079
[2022-06-16 11:49:46] __main__ INFO: Epoch 362 Step 39/39 lr 0.001000 loss 0.0550 (0.0353) acc@1 0.9844 (0.9912) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:46] __main__ INFO: Elapsed 1.62
[2022-06-16 11:49:46] __main__ INFO: Val 362
[2022-06-16 11:49:47] __main__ INFO: Epoch 362 loss 2.0680 acc@1 0.6555 acc@5 0.9627
[2022-06-16 11:49:47] __main__ INFO: Elapsed 0.76
[2022-06-16 11:49:47] __main__ INFO: Train 363 14118
[2022-06-16 11:49:49] __main__ INFO: Epoch 363 Step 39/39 lr 0.001000 loss 0.0246 (0.0348) acc@1 1.0000 (0.9920) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:49] __main__ INFO: Elapsed 1.69
[2022-06-16 11:49:49] __main__ INFO: Val 363
[2022-06-16 11:49:50] __main__ INFO: Epoch 363 loss 2.0578 acc@1 0.6573 acc@5 0.9623
[2022-06-16 11:49:50] __main__ INFO: Elapsed 0.76
[2022-06-16 11:49:50] __main__ INFO: Train 364 14157
[2022-06-16 11:49:51] __main__ INFO: Epoch 364 Step 39/39 lr 0.001000 loss 0.0234 (0.0373) acc@1 1.0000 (0.9910) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:51] __main__ INFO: Elapsed 1.72
[2022-06-16 11:49:51] __main__ INFO: Val 364
[2022-06-16 11:49:52] __main__ INFO: Epoch 364 loss 2.0692 acc@1 0.6573 acc@5 0.9629
[2022-06-16 11:49:52] __main__ INFO: Elapsed 0.72
[2022-06-16 11:49:52] __main__ INFO: Train 365 14196
[2022-06-16 11:49:54] __main__ INFO: Epoch 365 Step 39/39 lr 0.001000 loss 0.0408 (0.0399) acc@1 0.9922 (0.9894) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:54] __main__ INFO: Elapsed 1.67
[2022-06-16 11:49:54] __main__ INFO: Val 365
[2022-06-16 11:49:55] __main__ INFO: Epoch 365 loss 2.0702 acc@1 0.6549 acc@5 0.9631
[2022-06-16 11:49:55] __main__ INFO: Elapsed 0.76
[2022-06-16 11:49:55] __main__ INFO: Train 366 14235
[2022-06-16 11:49:56] __main__ INFO: Epoch 366 Step 39/39 lr 0.001000 loss 0.0232 (0.0327) acc@1 1.0000 (0.9928) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:56] __main__ INFO: Elapsed 1.67
[2022-06-16 11:49:56] __main__ INFO: Val 366
[2022-06-16 11:49:57] __main__ INFO: Epoch 366 loss 2.0606 acc@1 0.6551 acc@5 0.9637
[2022-06-16 11:49:57] __main__ INFO: Elapsed 0.72
[2022-06-16 11:49:57] __main__ INFO: Train 367 14274
[2022-06-16 11:49:59] __main__ INFO: Epoch 367 Step 39/39 lr 0.001000 loss 0.0250 (0.0317) acc@1 0.9922 (0.9924) acc@5 1.0000 (1.0000)
[2022-06-16 11:49:59] __main__ INFO: Elapsed 1.68
[2022-06-16 11:49:59] __main__ INFO: Val 367
[2022-06-16 11:49:59] __main__ INFO: Epoch 367 loss 2.0686 acc@1 0.6544 acc@5 0.9629
[2022-06-16 11:49:59] __main__ INFO: Elapsed 0.69
[2022-06-16 11:49:59] __main__ INFO: Train 368 14313
[2022-06-16 11:50:01] __main__ INFO: Epoch 368 Step 39/39 lr 0.001000 loss 0.0270 (0.0362) acc@1 0.9922 (0.9910) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:01] __main__ INFO: Elapsed 1.67
[2022-06-16 11:50:01] __main__ INFO: Val 368
[2022-06-16 11:50:02] __main__ INFO: Epoch 368 loss 2.0623 acc@1 0.6569 acc@5 0.9630
[2022-06-16 11:50:02] __main__ INFO: Elapsed 0.79
[2022-06-16 11:50:02] __main__ INFO: Train 369 14352
[2022-06-16 11:50:03] __main__ INFO: Epoch 369 Step 39/39 lr 0.001000 loss 0.0284 (0.0396) acc@1 0.9844 (0.9882) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:03] __main__ INFO: Elapsed 1.60
[2022-06-16 11:50:03] __main__ INFO: Val 369
[2022-06-16 11:50:04] __main__ INFO: Epoch 369 loss 2.0590 acc@1 0.6544 acc@5 0.9628
[2022-06-16 11:50:04] __main__ INFO: Elapsed 0.73
[2022-06-16 11:50:04] __main__ INFO: Train 370 14391
[2022-06-16 11:50:06] __main__ INFO: Epoch 370 Step 39/39 lr 0.001000 loss 0.0685 (0.0395) acc@1 0.9609 (0.9886) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:06] __main__ INFO: Elapsed 1.66
[2022-06-16 11:50:06] __main__ INFO: Val 370
[2022-06-16 11:50:07] __main__ INFO: Epoch 370 loss 2.0776 acc@1 0.6544 acc@5 0.9631
[2022-06-16 11:50:07] __main__ INFO: Elapsed 0.72
[2022-06-16 11:50:07] __main__ INFO: Train 371 14430
[2022-06-16 11:50:08] __main__ INFO: Epoch 371 Step 39/39 lr 0.001000 loss 0.0275 (0.0366) acc@1 0.9922 (0.9912) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:08] __main__ INFO: Elapsed 1.62
[2022-06-16 11:50:08] __main__ INFO: Val 371
[2022-06-16 11:50:09] __main__ INFO: Epoch 371 loss 2.0751 acc@1 0.6542 acc@5 0.9625
[2022-06-16 11:50:09] __main__ INFO: Elapsed 0.65
[2022-06-16 11:50:09] __main__ INFO: Train 372 14469
[2022-06-16 11:50:10] __main__ INFO: Epoch 372 Step 39/39 lr 0.001000 loss 0.0247 (0.0344) acc@1 0.9922 (0.9924) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:10] __main__ INFO: Elapsed 1.67
[2022-06-16 11:50:10] __main__ INFO: Val 372
[2022-06-16 11:50:11] __main__ INFO: Epoch 372 loss 2.0597 acc@1 0.6543 acc@5 0.9630
[2022-06-16 11:50:11] __main__ INFO: Elapsed 0.71
[2022-06-16 11:50:11] __main__ INFO: Train 373 14508
[2022-06-16 11:50:13] __main__ INFO: Epoch 373 Step 39/39 lr 0.001000 loss 0.0261 (0.0359) acc@1 0.9922 (0.9884) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:13] __main__ INFO: Elapsed 1.68
[2022-06-16 11:50:13] __main__ INFO: Val 373
[2022-06-16 11:50:14] __main__ INFO: Epoch 373 loss 2.0573 acc@1 0.6556 acc@5 0.9630
[2022-06-16 11:50:14] __main__ INFO: Elapsed 0.74
[2022-06-16 11:50:14] __main__ INFO: Train 374 14547
[2022-06-16 11:50:15] __main__ INFO: Epoch 374 Step 39/39 lr 0.001000 loss 0.0101 (0.0356) acc@1 1.0000 (0.9898) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:15] __main__ INFO: Elapsed 1.70
[2022-06-16 11:50:15] __main__ INFO: Val 374
[2022-06-16 11:50:16] __main__ INFO: Epoch 374 loss 2.0722 acc@1 0.6534 acc@5 0.9625
[2022-06-16 11:50:16] __main__ INFO: Elapsed 0.68
[2022-06-16 11:50:16] __main__ INFO: Train 375 14586
[2022-06-16 11:50:18] __main__ INFO: Epoch 375 Step 39/39 lr 0.001000 loss 0.0624 (0.0394) acc@1 0.9844 (0.9888) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:18] __main__ INFO: Elapsed 1.66
[2022-06-16 11:50:18] __main__ INFO: Val 375
[2022-06-16 11:50:18] __main__ INFO: Epoch 375 loss 2.0722 acc@1 0.6550 acc@5 0.9621
[2022-06-16 11:50:18] __main__ INFO: Elapsed 0.70
[2022-06-16 11:50:18] __main__ INFO: Train 376 14625
[2022-06-16 11:50:20] __main__ INFO: Epoch 376 Step 39/39 lr 0.001000 loss 0.0437 (0.0342) acc@1 0.9844 (0.9926) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:20] __main__ INFO: Elapsed 1.68
[2022-06-16 11:50:20] __main__ INFO: Val 376
[2022-06-16 11:50:21] __main__ INFO: Epoch 376 loss 2.0673 acc@1 0.6559 acc@5 0.9626
[2022-06-16 11:50:21] __main__ INFO: Elapsed 0.82
[2022-06-16 11:50:21] __main__ INFO: Train 377 14664
[2022-06-16 11:50:22] __main__ INFO: Epoch 377 Step 39/39 lr 0.001000 loss 0.0118 (0.0277) acc@1 1.0000 (0.9942) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:23] __main__ INFO: Elapsed 1.61
[2022-06-16 11:50:23] __main__ INFO: Val 377
[2022-06-16 11:50:23] __main__ INFO: Epoch 377 loss 2.0684 acc@1 0.6546 acc@5 0.9637
[2022-06-16 11:50:23] __main__ INFO: Elapsed 0.72
[2022-06-16 11:50:23] __main__ INFO: Train 378 14703
[2022-06-16 11:50:25] __main__ INFO: Epoch 378 Step 39/39 lr 0.001000 loss 0.0146 (0.0372) acc@1 1.0000 (0.9886) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:25] __main__ INFO: Elapsed 1.60
[2022-06-16 11:50:25] __main__ INFO: Val 378
[2022-06-16 11:50:26] __main__ INFO: Epoch 378 loss 2.0684 acc@1 0.6557 acc@5 0.9635
[2022-06-16 11:50:26] __main__ INFO: Elapsed 0.78
[2022-06-16 11:50:26] __main__ INFO: Train 379 14742
[2022-06-16 11:50:27] __main__ INFO: Epoch 379 Step 39/39 lr 0.001000 loss 0.0393 (0.0342) acc@1 0.9922 (0.9918) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:27] __main__ INFO: Elapsed 1.69
[2022-06-16 11:50:27] __main__ INFO: Val 379
[2022-06-16 11:50:28] __main__ INFO: Epoch 379 loss 2.0620 acc@1 0.6566 acc@5 0.9630
[2022-06-16 11:50:28] __main__ INFO: Elapsed 0.74
[2022-06-16 11:50:28] __main__ INFO: Train 380 14781
[2022-06-16 11:50:30] __main__ INFO: Epoch 380 Step 39/39 lr 0.001000 loss 0.0346 (0.0341) acc@1 0.9922 (0.9914) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:30] __main__ INFO: Elapsed 1.69
[2022-06-16 11:50:30] __main__ INFO: Val 380
[2022-06-16 11:50:30] __main__ INFO: Epoch 380 loss 2.0699 acc@1 0.6557 acc@5 0.9626
[2022-06-16 11:50:30] __main__ INFO: Elapsed 0.74
[2022-06-16 11:50:30] __main__ INFO: Train 381 14820
[2022-06-16 11:50:32] __main__ INFO: Epoch 381 Step 39/39 lr 0.001000 loss 0.0571 (0.0360) acc@1 0.9922 (0.9906) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:32] __main__ INFO: Elapsed 1.73
[2022-06-16 11:50:32] __main__ INFO: Val 381
[2022-06-16 11:50:33] __main__ INFO: Epoch 381 loss 2.0779 acc@1 0.6548 acc@5 0.9617
[2022-06-16 11:50:33] __main__ INFO: Elapsed 0.71
[2022-06-16 11:50:33] __main__ INFO: Train 382 14859
[2022-06-16 11:50:35] __main__ INFO: Epoch 382 Step 39/39 lr 0.001000 loss 0.0488 (0.0356) acc@1 0.9922 (0.9908) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:35] __main__ INFO: Elapsed 1.62
[2022-06-16 11:50:35] __main__ INFO: Val 382
[2022-06-16 11:50:35] __main__ INFO: Epoch 382 loss 2.0779 acc@1 0.6557 acc@5 0.9632
[2022-06-16 11:50:35] __main__ INFO: Elapsed 0.79
[2022-06-16 11:50:35] __main__ INFO: Train 383 14898
[2022-06-16 11:50:37] __main__ INFO: Epoch 383 Step 39/39 lr 0.001000 loss 0.0113 (0.0349) acc@1 1.0000 (0.9914) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:37] __main__ INFO: Elapsed 1.69
[2022-06-16 11:50:37] __main__ INFO: Val 383
[2022-06-16 11:50:38] __main__ INFO: Epoch 383 loss 2.0755 acc@1 0.6566 acc@5 0.9629
[2022-06-16 11:50:38] __main__ INFO: Elapsed 0.70
[2022-06-16 11:50:38] __main__ INFO: Train 384 14937
[2022-06-16 11:50:39] __main__ INFO: Epoch 384 Step 39/39 lr 0.001000 loss 0.0167 (0.0401) acc@1 1.0000 (0.9902) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:39] __main__ INFO: Elapsed 1.65
[2022-06-16 11:50:39] __main__ INFO: Val 384
[2022-06-16 11:50:40] __main__ INFO: Epoch 384 loss 2.0674 acc@1 0.6556 acc@5 0.9624
[2022-06-16 11:50:40] __main__ INFO: Elapsed 0.79
[2022-06-16 11:50:40] __main__ INFO: Train 385 14976
[2022-06-16 11:50:42] __main__ INFO: Epoch 385 Step 39/39 lr 0.001000 loss 0.0288 (0.0297) acc@1 0.9922 (0.9924) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:42] __main__ INFO: Elapsed 1.66
[2022-06-16 11:50:42] __main__ INFO: Val 385
[2022-06-16 11:50:43] __main__ INFO: Epoch 385 loss 2.0871 acc@1 0.6544 acc@5 0.9621
[2022-06-16 11:50:43] __main__ INFO: Elapsed 0.69
[2022-06-16 11:50:43] __main__ INFO: Train 386 15015
[2022-06-16 11:50:44] __main__ INFO: Epoch 386 Step 39/39 lr 0.001000 loss 0.0279 (0.0322) acc@1 1.0000 (0.9926) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:44] __main__ INFO: Elapsed 1.70
[2022-06-16 11:50:44] __main__ INFO: Val 386
[2022-06-16 11:50:45] __main__ INFO: Epoch 386 loss 2.0834 acc@1 0.6543 acc@5 0.9630
[2022-06-16 11:50:45] __main__ INFO: Elapsed 0.76
[2022-06-16 11:50:45] __main__ INFO: Train 387 15054
[2022-06-16 11:50:47] __main__ INFO: Epoch 387 Step 39/39 lr 0.001000 loss 0.0217 (0.0297) acc@1 1.0000 (0.9944) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:47] __main__ INFO: Elapsed 1.72
[2022-06-16 11:50:47] __main__ INFO: Val 387
[2022-06-16 11:50:47] __main__ INFO: Epoch 387 loss 2.0841 acc@1 0.6546 acc@5 0.9622
[2022-06-16 11:50:47] __main__ INFO: Elapsed 0.72
[2022-06-16 11:50:47] __main__ INFO: Train 388 15093
[2022-06-16 11:50:49] __main__ INFO: Epoch 388 Step 39/39 lr 0.001000 loss 0.0343 (0.0352) acc@1 0.9844 (0.9894) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:49] __main__ INFO: Elapsed 1.66
[2022-06-16 11:50:49] __main__ INFO: Val 388
[2022-06-16 11:50:50] __main__ INFO: Epoch 388 loss 2.0829 acc@1 0.6547 acc@5 0.9623
[2022-06-16 11:50:50] __main__ INFO: Elapsed 0.73
[2022-06-16 11:50:50] __main__ INFO: Train 389 15132
[2022-06-16 11:50:52] __main__ INFO: Epoch 389 Step 39/39 lr 0.001000 loss 0.0268 (0.0337) acc@1 1.0000 (0.9914) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:52] __main__ INFO: Elapsed 1.75
[2022-06-16 11:50:52] __main__ INFO: Val 389
[2022-06-16 11:50:52] __main__ INFO: Epoch 389 loss 2.0849 acc@1 0.6546 acc@5 0.9616
[2022-06-16 11:50:52] __main__ INFO: Elapsed 0.74
[2022-06-16 11:50:52] __main__ INFO: Train 390 15171
[2022-06-16 11:50:54] __main__ INFO: Epoch 390 Step 39/39 lr 0.001000 loss 0.0308 (0.0395) acc@1 0.9922 (0.9894) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:54] __main__ INFO: Elapsed 1.66
[2022-06-16 11:50:54] __main__ INFO: Val 390
[2022-06-16 11:50:55] __main__ INFO: Epoch 390 loss 2.0824 acc@1 0.6538 acc@5 0.9619
[2022-06-16 11:50:55] __main__ INFO: Elapsed 0.79
[2022-06-16 11:50:55] __main__ INFO: Train 391 15210
[2022-06-16 11:50:56] __main__ INFO: Epoch 391 Step 39/39 lr 0.001000 loss 0.0314 (0.0336) acc@1 0.9922 (0.9914) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:56] __main__ INFO: Elapsed 1.70
[2022-06-16 11:50:56] __main__ INFO: Val 391
[2022-06-16 11:50:57] __main__ INFO: Epoch 391 loss 2.0927 acc@1 0.6543 acc@5 0.9626
[2022-06-16 11:50:57] __main__ INFO: Elapsed 0.77
[2022-06-16 11:50:57] __main__ INFO: Train 392 15249
[2022-06-16 11:50:59] __main__ INFO: Epoch 392 Step 39/39 lr 0.001000 loss 0.0408 (0.0357) acc@1 0.9844 (0.9888) acc@5 1.0000 (1.0000)
[2022-06-16 11:50:59] __main__ INFO: Elapsed 1.67
[2022-06-16 11:50:59] __main__ INFO: Val 392
[2022-06-16 11:51:00] __main__ INFO: Epoch 392 loss 2.0918 acc@1 0.6547 acc@5 0.9616
[2022-06-16 11:51:00] __main__ INFO: Elapsed 0.77
[2022-06-16 11:51:00] __main__ INFO: Train 393 15288
[2022-06-16 11:51:01] __main__ INFO: Epoch 393 Step 39/39 lr 0.001000 loss 0.0219 (0.0320) acc@1 1.0000 (0.9920) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:01] __main__ INFO: Elapsed 1.68
[2022-06-16 11:51:01] __main__ INFO: Val 393
[2022-06-16 11:51:02] __main__ INFO: Epoch 393 loss 2.0807 acc@1 0.6549 acc@5 0.9615
[2022-06-16 11:51:02] __main__ INFO: Elapsed 0.74
[2022-06-16 11:51:02] __main__ INFO: Train 394 15327
[2022-06-16 11:51:04] __main__ INFO: Epoch 394 Step 39/39 lr 0.001000 loss 0.0371 (0.0312) acc@1 1.0000 (0.9918) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:04] __main__ INFO: Elapsed 1.66
[2022-06-16 11:51:04] __main__ INFO: Val 394
[2022-06-16 11:51:05] __main__ INFO: Epoch 394 loss 2.0882 acc@1 0.6551 acc@5 0.9614
[2022-06-16 11:51:05] __main__ INFO: Elapsed 0.75
[2022-06-16 11:51:05] __main__ INFO: Train 395 15366
[2022-06-16 11:51:06] __main__ INFO: Epoch 395 Step 39/39 lr 0.001000 loss 0.0426 (0.0353) acc@1 0.9922 (0.9906) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:06] __main__ INFO: Elapsed 1.67
[2022-06-16 11:51:06] __main__ INFO: Val 395
[2022-06-16 11:51:07] __main__ INFO: Epoch 395 loss 2.0761 acc@1 0.6553 acc@5 0.9622
[2022-06-16 11:51:07] __main__ INFO: Elapsed 0.77
[2022-06-16 11:51:07] __main__ INFO: Train 396 15405
[2022-06-16 11:51:09] __main__ INFO: Epoch 396 Step 39/39 lr 0.001000 loss 0.0324 (0.0300) acc@1 0.9922 (0.9930) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:09] __main__ INFO: Elapsed 1.73
[2022-06-16 11:51:09] __main__ INFO: Val 396
[2022-06-16 11:51:09] __main__ INFO: Epoch 396 loss 2.0783 acc@1 0.6552 acc@5 0.9619
[2022-06-16 11:51:09] __main__ INFO: Elapsed 0.72
[2022-06-16 11:51:09] __main__ INFO: Train 397 15444
[2022-06-16 11:51:11] __main__ INFO: Epoch 397 Step 39/39 lr 0.001000 loss 0.0177 (0.0294) acc@1 0.9922 (0.9920) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:11] __main__ INFO: Elapsed 1.71
[2022-06-16 11:51:11] __main__ INFO: Val 397
[2022-06-16 11:51:12] __main__ INFO: Epoch 397 loss 2.0853 acc@1 0.6560 acc@5 0.9624
[2022-06-16 11:51:12] __main__ INFO: Elapsed 0.80
[2022-06-16 11:51:12] __main__ INFO: Train 398 15483
[2022-06-16 11:51:14] __main__ INFO: Epoch 398 Step 39/39 lr 0.001000 loss 0.0169 (0.0299) acc@1 1.0000 (0.9934) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:14] __main__ INFO: Elapsed 1.65
[2022-06-16 11:51:14] __main__ INFO: Val 398
[2022-06-16 11:51:14] __main__ INFO: Epoch 398 loss 2.0814 acc@1 0.6556 acc@5 0.9629
[2022-06-16 11:51:14] __main__ INFO: Elapsed 0.78
[2022-06-16 11:51:14] __main__ INFO: Train 399 15522
[2022-06-16 11:51:16] __main__ INFO: Epoch 399 Step 39/39 lr 0.001000 loss 0.0174 (0.0314) acc@1 1.0000 (0.9928) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:16] __main__ INFO: Elapsed 1.68
[2022-06-16 11:51:16] __main__ INFO: Val 399
[2022-06-16 11:51:17] __main__ INFO: Epoch 399 loss 2.0801 acc@1 0.6561 acc@5 0.9627
[2022-06-16 11:51:17] __main__ INFO: Elapsed 0.78
[2022-06-16 11:51:17] __main__ INFO: Train 400 15561
[2022-06-16 11:51:19] __main__ INFO: Epoch 400 Step 39/39 lr 0.001000 loss 0.0511 (0.0332) acc@1 0.9844 (0.9914) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:19] __main__ INFO: Elapsed 1.71
[2022-06-16 11:51:19] __main__ INFO: Val 400
[2022-06-16 11:51:19] __main__ INFO: Epoch 400 loss 2.0809 acc@1 0.6559 acc@5 0.9625
[2022-06-16 11:51:19] __main__ INFO: Elapsed 0.70
[2022-06-16 11:51:19] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00400.pth
[2022-06-16 11:51:19] __main__ INFO: Train 401 15600
[2022-06-16 11:51:21] __main__ INFO: Epoch 401 Step 39/39 lr 0.001000 loss 0.0311 (0.0368) acc@1 0.9844 (0.9896) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:21] __main__ INFO: Elapsed 1.71
[2022-06-16 11:51:21] __main__ INFO: Val 401
[2022-06-16 11:51:22] __main__ INFO: Epoch 401 loss 2.0854 acc@1 0.6568 acc@5 0.9627
[2022-06-16 11:51:22] __main__ INFO: Elapsed 0.75
[2022-06-16 11:51:22] __main__ INFO: Train 402 15639
[2022-06-16 11:51:23] __main__ INFO: Epoch 402 Step 39/39 lr 0.001000 loss 0.0128 (0.0293) acc@1 1.0000 (0.9930) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:23] __main__ INFO: Elapsed 1.65
[2022-06-16 11:51:23] __main__ INFO: Val 402
[2022-06-16 11:51:24] __main__ INFO: Epoch 402 loss 2.0952 acc@1 0.6553 acc@5 0.9628
[2022-06-16 11:51:24] __main__ INFO: Elapsed 0.77
[2022-06-16 11:51:24] __main__ INFO: Train 403 15678
[2022-06-16 11:51:26] __main__ INFO: Epoch 403 Step 39/39 lr 0.001000 loss 0.0210 (0.0258) acc@1 0.9922 (0.9938) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:26] __main__ INFO: Elapsed 1.63
[2022-06-16 11:51:26] __main__ INFO: Val 403
[2022-06-16 11:51:27] __main__ INFO: Epoch 403 loss 2.0942 acc@1 0.6558 acc@5 0.9628
[2022-06-16 11:51:27] __main__ INFO: Elapsed 0.73
[2022-06-16 11:51:27] __main__ INFO: Train 404 15717
[2022-06-16 11:51:28] __main__ INFO: Epoch 404 Step 39/39 lr 0.001000 loss 0.0353 (0.0317) acc@1 0.9844 (0.9924) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:28] __main__ INFO: Elapsed 1.66
[2022-06-16 11:51:28] __main__ INFO: Val 404
[2022-06-16 11:51:29] __main__ INFO: Epoch 404 loss 2.0791 acc@1 0.6563 acc@5 0.9636
[2022-06-16 11:51:29] __main__ INFO: Elapsed 0.78
[2022-06-16 11:51:29] __main__ INFO: Train 405 15756
[2022-06-16 11:51:31] __main__ INFO: Epoch 405 Step 39/39 lr 0.001000 loss 0.0376 (0.0301) acc@1 0.9922 (0.9942) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:31] __main__ INFO: Elapsed 1.69
[2022-06-16 11:51:31] __main__ INFO: Val 405
[2022-06-16 11:51:32] __main__ INFO: Epoch 405 loss 2.0901 acc@1 0.6566 acc@5 0.9623
[2022-06-16 11:51:32] __main__ INFO: Elapsed 0.79
[2022-06-16 11:51:32] __main__ INFO: Train 406 15795
[2022-06-16 11:51:33] __main__ INFO: Epoch 406 Step 39/39 lr 0.001000 loss 0.0261 (0.0284) acc@1 1.0000 (0.9952) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:33] __main__ INFO: Elapsed 1.64
[2022-06-16 11:51:33] __main__ INFO: Val 406
[2022-06-16 11:51:34] __main__ INFO: Epoch 406 loss 2.1046 acc@1 0.6549 acc@5 0.9619
[2022-06-16 11:51:34] __main__ INFO: Elapsed 0.79
[2022-06-16 11:51:34] __main__ INFO: Train 407 15834
[2022-06-16 11:51:36] __main__ INFO: Epoch 407 Step 39/39 lr 0.001000 loss 0.0290 (0.0325) acc@1 1.0000 (0.9910) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:36] __main__ INFO: Elapsed 1.75
[2022-06-16 11:51:36] __main__ INFO: Val 407
[2022-06-16 11:51:36] __main__ INFO: Epoch 407 loss 2.1009 acc@1 0.6565 acc@5 0.9615
[2022-06-16 11:51:36] __main__ INFO: Elapsed 0.78
[2022-06-16 11:51:36] __main__ INFO: Train 408 15873
[2022-06-16 11:51:38] __main__ INFO: Epoch 408 Step 39/39 lr 0.001000 loss 0.0171 (0.0306) acc@1 1.0000 (0.9938) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:38] __main__ INFO: Elapsed 1.71
[2022-06-16 11:51:38] __main__ INFO: Val 408
[2022-06-16 11:51:39] __main__ INFO: Epoch 408 loss 2.0966 acc@1 0.6587 acc@5 0.9628
[2022-06-16 11:51:39] __main__ INFO: Elapsed 0.71
[2022-06-16 11:51:39] __main__ INFO: Train 409 15912
[2022-06-16 11:51:41] __main__ INFO: Epoch 409 Step 39/39 lr 0.001000 loss 0.0317 (0.0306) acc@1 1.0000 (0.9938) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:41] __main__ INFO: Elapsed 1.69
[2022-06-16 11:51:41] __main__ INFO: Val 409
[2022-06-16 11:51:41] __main__ INFO: Epoch 409 loss 2.1102 acc@1 0.6573 acc@5 0.9620
[2022-06-16 11:51:41] __main__ INFO: Elapsed 0.76
[2022-06-16 11:51:41] __main__ INFO: Train 410 15951
[2022-06-16 11:51:43] __main__ INFO: Epoch 410 Step 39/39 lr 0.001000 loss 0.0206 (0.0273) acc@1 0.9922 (0.9946) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:43] __main__ INFO: Elapsed 1.71
[2022-06-16 11:51:43] __main__ INFO: Val 410
[2022-06-16 11:51:44] __main__ INFO: Epoch 410 loss 2.1066 acc@1 0.6558 acc@5 0.9624
[2022-06-16 11:51:44] __main__ INFO: Elapsed 0.73
[2022-06-16 11:51:44] __main__ INFO: Train 411 15990
[2022-06-16 11:51:46] __main__ INFO: Epoch 411 Step 39/39 lr 0.001000 loss 0.0112 (0.0320) acc@1 1.0000 (0.9922) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:46] __main__ INFO: Elapsed 1.77
[2022-06-16 11:51:46] __main__ INFO: Val 411
[2022-06-16 11:51:46] __main__ INFO: Epoch 411 loss 2.1095 acc@1 0.6559 acc@5 0.9625
[2022-06-16 11:51:46] __main__ INFO: Elapsed 0.77
[2022-06-16 11:51:46] __main__ INFO: Train 412 16029
[2022-06-16 11:51:48] __main__ INFO: Epoch 412 Step 39/39 lr 0.001000 loss 0.0384 (0.0332) acc@1 0.9922 (0.9904) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:48] __main__ INFO: Elapsed 1.60
[2022-06-16 11:51:48] __main__ INFO: Val 412
[2022-06-16 11:51:49] __main__ INFO: Epoch 412 loss 2.1028 acc@1 0.6557 acc@5 0.9623
[2022-06-16 11:51:49] __main__ INFO: Elapsed 0.75
[2022-06-16 11:51:49] __main__ INFO: Train 413 16068
[2022-06-16 11:51:50] __main__ INFO: Epoch 413 Step 39/39 lr 0.001000 loss 0.0212 (0.0299) acc@1 1.0000 (0.9942) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:50] __main__ INFO: Elapsed 1.66
[2022-06-16 11:51:50] __main__ INFO: Val 413
[2022-06-16 11:51:51] __main__ INFO: Epoch 413 loss 2.1218 acc@1 0.6553 acc@5 0.9616
[2022-06-16 11:51:51] __main__ INFO: Elapsed 0.80
[2022-06-16 11:51:51] __main__ INFO: Train 414 16107
[2022-06-16 11:51:53] __main__ INFO: Epoch 414 Step 39/39 lr 0.001000 loss 0.0111 (0.0288) acc@1 1.0000 (0.9934) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:53] __main__ INFO: Elapsed 1.65
[2022-06-16 11:51:53] __main__ INFO: Val 414
[2022-06-16 11:51:54] __main__ INFO: Epoch 414 loss 2.1140 acc@1 0.6560 acc@5 0.9628
[2022-06-16 11:51:54] __main__ INFO: Elapsed 0.76
[2022-06-16 11:51:54] __main__ INFO: Train 415 16146
[2022-06-16 11:51:55] __main__ INFO: Epoch 415 Step 39/39 lr 0.001000 loss 0.0329 (0.0286) acc@1 0.9922 (0.9934) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:55] __main__ INFO: Elapsed 1.72
[2022-06-16 11:51:55] __main__ INFO: Val 415
[2022-06-16 11:51:56] __main__ INFO: Epoch 415 loss 2.1378 acc@1 0.6540 acc@5 0.9621
[2022-06-16 11:51:56] __main__ INFO: Elapsed 0.86
[2022-06-16 11:51:56] __main__ INFO: Train 416 16185
[2022-06-16 11:51:58] __main__ INFO: Epoch 416 Step 39/39 lr 0.001000 loss 0.0363 (0.0306) acc@1 1.0000 (0.9928) acc@5 1.0000 (1.0000)
[2022-06-16 11:51:58] __main__ INFO: Elapsed 1.68
[2022-06-16 11:51:58] __main__ INFO: Val 416
[2022-06-16 11:51:59] __main__ INFO: Epoch 416 loss 2.1385 acc@1 0.6560 acc@5 0.9613
[2022-06-16 11:51:59] __main__ INFO: Elapsed 0.69
[2022-06-16 11:51:59] __main__ INFO: Train 417 16224
[2022-06-16 11:52:00] __main__ INFO: Epoch 417 Step 39/39 lr 0.001000 loss 0.0395 (0.0280) acc@1 0.9922 (0.9920) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:00] __main__ INFO: Elapsed 1.70
[2022-06-16 11:52:00] __main__ INFO: Val 417
[2022-06-16 11:52:01] __main__ INFO: Epoch 417 loss 2.1281 acc@1 0.6560 acc@5 0.9615
[2022-06-16 11:52:01] __main__ INFO: Elapsed 0.74
[2022-06-16 11:52:01] __main__ INFO: Train 418 16263
[2022-06-16 11:52:03] __main__ INFO: Epoch 418 Step 39/39 lr 0.001000 loss 0.0168 (0.0300) acc@1 1.0000 (0.9936) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:03] __main__ INFO: Elapsed 1.61
[2022-06-16 11:52:03] __main__ INFO: Val 418
[2022-06-16 11:52:03] __main__ INFO: Epoch 418 loss 2.1124 acc@1 0.6561 acc@5 0.9615
[2022-06-16 11:52:03] __main__ INFO: Elapsed 0.67
[2022-06-16 11:52:03] __main__ INFO: Train 419 16302
[2022-06-16 11:52:05] __main__ INFO: Epoch 419 Step 39/39 lr 0.001000 loss 0.0496 (0.0322) acc@1 0.9688 (0.9926) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:05] __main__ INFO: Elapsed 1.69
[2022-06-16 11:52:05] __main__ INFO: Val 419
[2022-06-16 11:52:06] __main__ INFO: Epoch 419 loss 2.1177 acc@1 0.6566 acc@5 0.9628
[2022-06-16 11:52:06] __main__ INFO: Elapsed 0.76
[2022-06-16 11:52:06] __main__ INFO: Train 420 16341
[2022-06-16 11:52:07] __main__ INFO: Epoch 420 Step 39/39 lr 0.001000 loss 0.0220 (0.0313) acc@1 1.0000 (0.9912) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:07] __main__ INFO: Elapsed 1.63
[2022-06-16 11:52:07] __main__ INFO: Val 420
[2022-06-16 11:52:08] __main__ INFO: Epoch 420 loss 2.1217 acc@1 0.6537 acc@5 0.9629
[2022-06-16 11:52:08] __main__ INFO: Elapsed 0.76
[2022-06-16 11:52:08] __main__ INFO: Train 421 16380
[2022-06-16 11:52:10] __main__ INFO: Epoch 421 Step 39/39 lr 0.001000 loss 0.0340 (0.0343) acc@1 0.9922 (0.9894) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:10] __main__ INFO: Elapsed 1.67
[2022-06-16 11:52:10] __main__ INFO: Val 421
[2022-06-16 11:52:11] __main__ INFO: Epoch 421 loss 2.1159 acc@1 0.6519 acc@5 0.9627
[2022-06-16 11:52:11] __main__ INFO: Elapsed 0.72
[2022-06-16 11:52:11] __main__ INFO: Train 422 16419
[2022-06-16 11:52:12] __main__ INFO: Epoch 422 Step 39/39 lr 0.001000 loss 0.0675 (0.0314) acc@1 0.9844 (0.9922) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:12] __main__ INFO: Elapsed 1.65
[2022-06-16 11:52:12] __main__ INFO: Val 422
[2022-06-16 11:52:13] __main__ INFO: Epoch 422 loss 2.1152 acc@1 0.6543 acc@5 0.9621
[2022-06-16 11:52:13] __main__ INFO: Elapsed 0.73
[2022-06-16 11:52:13] __main__ INFO: Train 423 16458
[2022-06-16 11:52:15] __main__ INFO: Epoch 423 Step 39/39 lr 0.001000 loss 0.0325 (0.0290) acc@1 0.9922 (0.9930) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:15] __main__ INFO: Elapsed 1.70
[2022-06-16 11:52:15] __main__ INFO: Val 423
[2022-06-16 11:52:15] __main__ INFO: Epoch 423 loss 2.1149 acc@1 0.6544 acc@5 0.9632
[2022-06-16 11:52:15] __main__ INFO: Elapsed 0.73
[2022-06-16 11:52:15] __main__ INFO: Train 424 16497
[2022-06-16 11:52:17] __main__ INFO: Epoch 424 Step 39/39 lr 0.001000 loss 0.0173 (0.0308) acc@1 1.0000 (0.9908) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:17] __main__ INFO: Elapsed 1.69
[2022-06-16 11:52:17] __main__ INFO: Val 424
[2022-06-16 11:52:18] __main__ INFO: Epoch 424 loss 2.1071 acc@1 0.6573 acc@5 0.9626
[2022-06-16 11:52:18] __main__ INFO: Elapsed 0.80
[2022-06-16 11:52:18] __main__ INFO: Train 425 16536
[2022-06-16 11:52:19] __main__ INFO: Epoch 425 Step 39/39 lr 0.001000 loss 0.0801 (0.0330) acc@1 0.9844 (0.9906) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:19] __main__ INFO: Elapsed 1.63
[2022-06-16 11:52:19] __main__ INFO: Val 425
[2022-06-16 11:52:20] __main__ INFO: Epoch 425 loss 2.1149 acc@1 0.6571 acc@5 0.9632
[2022-06-16 11:52:20] __main__ INFO: Elapsed 0.75
[2022-06-16 11:52:20] __main__ INFO: Train 426 16575
[2022-06-16 11:52:22] __main__ INFO: Epoch 426 Step 39/39 lr 0.001000 loss 0.0359 (0.0289) acc@1 0.9922 (0.9916) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:22] __main__ INFO: Elapsed 1.61
[2022-06-16 11:52:22] __main__ INFO: Val 426
[2022-06-16 11:52:23] __main__ INFO: Epoch 426 loss 2.1154 acc@1 0.6550 acc@5 0.9624
[2022-06-16 11:52:23] __main__ INFO: Elapsed 0.75
[2022-06-16 11:52:23] __main__ INFO: Train 427 16614
[2022-06-16 11:52:24] __main__ INFO: Epoch 427 Step 39/39 lr 0.001000 loss 0.0300 (0.0297) acc@1 0.9844 (0.9930) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:24] __main__ INFO: Elapsed 1.71
[2022-06-16 11:52:24] __main__ INFO: Val 427
[2022-06-16 11:52:25] __main__ INFO: Epoch 427 loss 2.1168 acc@1 0.6553 acc@5 0.9628
[2022-06-16 11:52:25] __main__ INFO: Elapsed 0.71
[2022-06-16 11:52:25] __main__ INFO: Train 428 16653
[2022-06-16 11:52:27] __main__ INFO: Epoch 428 Step 39/39 lr 0.001000 loss 0.0556 (0.0299) acc@1 0.9922 (0.9926) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:27] __main__ INFO: Elapsed 1.76
[2022-06-16 11:52:27] __main__ INFO: Val 428
[2022-06-16 11:52:28] __main__ INFO: Epoch 428 loss 2.1288 acc@1 0.6558 acc@5 0.9613
[2022-06-16 11:52:28] __main__ INFO: Elapsed 0.73
[2022-06-16 11:52:28] __main__ INFO: Train 429 16692
[2022-06-16 11:52:29] __main__ INFO: Epoch 429 Step 39/39 lr 0.001000 loss 0.0206 (0.0293) acc@1 0.9922 (0.9924) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:29] __main__ INFO: Elapsed 1.64
[2022-06-16 11:52:29] __main__ INFO: Val 429
[2022-06-16 11:52:30] __main__ INFO: Epoch 429 loss 2.1206 acc@1 0.6569 acc@5 0.9619
[2022-06-16 11:52:30] __main__ INFO: Elapsed 0.79
[2022-06-16 11:52:30] __main__ INFO: Train 430 16731
[2022-06-16 11:52:32] __main__ INFO: Epoch 430 Step 39/39 lr 0.001000 loss 0.0207 (0.0296) acc@1 1.0000 (0.9936) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:32] __main__ INFO: Elapsed 1.63
[2022-06-16 11:52:32] __main__ INFO: Val 430
[2022-06-16 11:52:32] __main__ INFO: Epoch 430 loss 2.1175 acc@1 0.6540 acc@5 0.9623
[2022-06-16 11:52:32] __main__ INFO: Elapsed 0.74
[2022-06-16 11:52:32] __main__ INFO: Train 431 16770
[2022-06-16 11:52:34] __main__ INFO: Epoch 431 Step 39/39 lr 0.001000 loss 0.0449 (0.0317) acc@1 0.9844 (0.9920) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:34] __main__ INFO: Elapsed 1.72
[2022-06-16 11:52:34] __main__ INFO: Val 431
[2022-06-16 11:52:35] __main__ INFO: Epoch 431 loss 2.1189 acc@1 0.6556 acc@5 0.9629
[2022-06-16 11:52:35] __main__ INFO: Elapsed 0.74
[2022-06-16 11:52:35] __main__ INFO: Train 432 16809
[2022-06-16 11:52:36] __main__ INFO: Epoch 432 Step 39/39 lr 0.001000 loss 0.0340 (0.0311) acc@1 0.9922 (0.9930) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:36] __main__ INFO: Elapsed 1.68
[2022-06-16 11:52:36] __main__ INFO: Val 432
[2022-06-16 11:52:37] __main__ INFO: Epoch 432 loss 2.1147 acc@1 0.6559 acc@5 0.9629
[2022-06-16 11:52:37] __main__ INFO: Elapsed 0.77
[2022-06-16 11:52:37] __main__ INFO: Train 433 16848
[2022-06-16 11:52:39] __main__ INFO: Epoch 433 Step 39/39 lr 0.001000 loss 0.0282 (0.0291) acc@1 0.9844 (0.9924) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:39] __main__ INFO: Elapsed 1.65
[2022-06-16 11:52:39] __main__ INFO: Val 433
[2022-06-16 11:52:40] __main__ INFO: Epoch 433 loss 2.1260 acc@1 0.6542 acc@5 0.9630
[2022-06-16 11:52:40] __main__ INFO: Elapsed 0.68
[2022-06-16 11:52:40] __main__ INFO: Train 434 16887
[2022-06-16 11:52:41] __main__ INFO: Epoch 434 Step 39/39 lr 0.001000 loss 0.0090 (0.0274) acc@1 1.0000 (0.9938) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:41] __main__ INFO: Elapsed 1.61
[2022-06-16 11:52:41] __main__ INFO: Val 434
[2022-06-16 11:52:42] __main__ INFO: Epoch 434 loss 2.1115 acc@1 0.6555 acc@5 0.9619
[2022-06-16 11:52:42] __main__ INFO: Elapsed 0.75
[2022-06-16 11:52:42] __main__ INFO: Train 435 16926
[2022-06-16 11:52:44] __main__ INFO: Epoch 435 Step 39/39 lr 0.001000 loss 0.0235 (0.0294) acc@1 1.0000 (0.9926) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:44] __main__ INFO: Elapsed 1.63
[2022-06-16 11:52:44] __main__ INFO: Val 435
[2022-06-16 11:52:44] __main__ INFO: Epoch 435 loss 2.1224 acc@1 0.6547 acc@5 0.9622
[2022-06-16 11:52:44] __main__ INFO: Elapsed 0.71
[2022-06-16 11:52:44] __main__ INFO: Train 436 16965
[2022-06-16 11:52:46] __main__ INFO: Epoch 436 Step 39/39 lr 0.001000 loss 0.0391 (0.0288) acc@1 0.9844 (0.9932) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:46] __main__ INFO: Elapsed 1.66
[2022-06-16 11:52:46] __main__ INFO: Val 436
[2022-06-16 11:52:47] __main__ INFO: Epoch 436 loss 2.1186 acc@1 0.6564 acc@5 0.9626
[2022-06-16 11:52:47] __main__ INFO: Elapsed 0.73
[2022-06-16 11:52:47] __main__ INFO: Train 437 17004
[2022-06-16 11:52:48] __main__ INFO: Epoch 437 Step 39/39 lr 0.001000 loss 0.0203 (0.0308) acc@1 0.9922 (0.9920) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:48] __main__ INFO: Elapsed 1.67
[2022-06-16 11:52:48] __main__ INFO: Val 437
[2022-06-16 11:52:49] __main__ INFO: Epoch 437 loss 2.1375 acc@1 0.6533 acc@5 0.9625
[2022-06-16 11:52:49] __main__ INFO: Elapsed 0.78
[2022-06-16 11:52:49] __main__ INFO: Train 438 17043
[2022-06-16 11:52:51] __main__ INFO: Epoch 438 Step 39/39 lr 0.001000 loss 0.0240 (0.0305) acc@1 1.0000 (0.9914) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:51] __main__ INFO: Elapsed 1.70
[2022-06-16 11:52:51] __main__ INFO: Val 438
[2022-06-16 11:52:52] __main__ INFO: Epoch 438 loss 2.1364 acc@1 0.6549 acc@5 0.9625
[2022-06-16 11:52:52] __main__ INFO: Elapsed 0.74
[2022-06-16 11:52:52] __main__ INFO: Train 439 17082
[2022-06-16 11:52:53] __main__ INFO: Epoch 439 Step 39/39 lr 0.001000 loss 0.0212 (0.0272) acc@1 0.9922 (0.9938) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:53] __main__ INFO: Elapsed 1.76
[2022-06-16 11:52:53] __main__ INFO: Val 439
[2022-06-16 11:52:54] __main__ INFO: Epoch 439 loss 2.1444 acc@1 0.6522 acc@5 0.9625
[2022-06-16 11:52:54] __main__ INFO: Elapsed 0.77
[2022-06-16 11:52:54] __main__ INFO: Train 440 17121
[2022-06-16 11:52:56] __main__ INFO: Epoch 440 Step 39/39 lr 0.001000 loss 0.0229 (0.0271) acc@1 1.0000 (0.9940) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:56] __main__ INFO: Elapsed 1.73
[2022-06-16 11:52:56] __main__ INFO: Val 440
[2022-06-16 11:52:57] __main__ INFO: Epoch 440 loss 2.1388 acc@1 0.6523 acc@5 0.9626
[2022-06-16 11:52:57] __main__ INFO: Elapsed 0.75
[2022-06-16 11:52:57] __main__ INFO: Train 441 17160
[2022-06-16 11:52:58] __main__ INFO: Epoch 441 Step 39/39 lr 0.001000 loss 0.0309 (0.0289) acc@1 0.9922 (0.9940) acc@5 1.0000 (1.0000)
[2022-06-16 11:52:58] __main__ INFO: Elapsed 1.66
[2022-06-16 11:52:58] __main__ INFO: Val 441
[2022-06-16 11:52:59] __main__ INFO: Epoch 441 loss 2.1385 acc@1 0.6554 acc@5 0.9611
[2022-06-16 11:52:59] __main__ INFO: Elapsed 0.79
[2022-06-16 11:52:59] __main__ INFO: Train 442 17199
[2022-06-16 11:53:01] __main__ INFO: Epoch 442 Step 39/39 lr 0.001000 loss 0.0307 (0.0317) acc@1 0.9844 (0.9916) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:01] __main__ INFO: Elapsed 1.68
[2022-06-16 11:53:01] __main__ INFO: Val 442
[2022-06-16 11:53:01] __main__ INFO: Epoch 442 loss 2.1220 acc@1 0.6543 acc@5 0.9623
[2022-06-16 11:53:01] __main__ INFO: Elapsed 0.76
[2022-06-16 11:53:01] __main__ INFO: Train 443 17238
[2022-06-16 11:53:03] __main__ INFO: Epoch 443 Step 39/39 lr 0.001000 loss 0.0146 (0.0291) acc@1 1.0000 (0.9928) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:03] __main__ INFO: Elapsed 1.75
[2022-06-16 11:53:03] __main__ INFO: Val 443
[2022-06-16 11:53:04] __main__ INFO: Epoch 443 loss 2.1284 acc@1 0.6533 acc@5 0.9626
[2022-06-16 11:53:04] __main__ INFO: Elapsed 0.76
[2022-06-16 11:53:04] __main__ INFO: Train 444 17277
[2022-06-16 11:53:06] __main__ INFO: Epoch 444 Step 39/39 lr 0.001000 loss 0.0344 (0.0326) acc@1 0.9922 (0.9922) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:06] __main__ INFO: Elapsed 1.63
[2022-06-16 11:53:06] __main__ INFO: Val 444
[2022-06-16 11:53:06] __main__ INFO: Epoch 444 loss 2.1352 acc@1 0.6537 acc@5 0.9618
[2022-06-16 11:53:06] __main__ INFO: Elapsed 0.75
[2022-06-16 11:53:06] __main__ INFO: Train 445 17316
[2022-06-16 11:53:08] __main__ INFO: Epoch 445 Step 39/39 lr 0.001000 loss 0.0189 (0.0266) acc@1 1.0000 (0.9950) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:08] __main__ INFO: Elapsed 1.70
[2022-06-16 11:53:08] __main__ INFO: Val 445
[2022-06-16 11:53:09] __main__ INFO: Epoch 445 loss 2.1390 acc@1 0.6536 acc@5 0.9619
[2022-06-16 11:53:09] __main__ INFO: Elapsed 0.75
[2022-06-16 11:53:09] __main__ INFO: Train 446 17355
[2022-06-16 11:53:10] __main__ INFO: Epoch 446 Step 39/39 lr 0.001000 loss 0.0316 (0.0274) acc@1 0.9922 (0.9944) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:11] __main__ INFO: Elapsed 1.70
[2022-06-16 11:53:11] __main__ INFO: Val 446
[2022-06-16 11:53:11] __main__ INFO: Epoch 446 loss 2.1431 acc@1 0.6531 acc@5 0.9622
[2022-06-16 11:53:11] __main__ INFO: Elapsed 0.71
[2022-06-16 11:53:11] __main__ INFO: Train 447 17394
[2022-06-16 11:53:13] __main__ INFO: Epoch 447 Step 39/39 lr 0.001000 loss 0.0265 (0.0312) acc@1 0.9922 (0.9916) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:13] __main__ INFO: Elapsed 1.68
[2022-06-16 11:53:13] __main__ INFO: Val 447
[2022-06-16 11:53:14] __main__ INFO: Epoch 447 loss 2.1285 acc@1 0.6525 acc@5 0.9618
[2022-06-16 11:53:14] __main__ INFO: Elapsed 0.75
[2022-06-16 11:53:14] __main__ INFO: Train 448 17433
[2022-06-16 11:53:15] __main__ INFO: Epoch 448 Step 39/39 lr 0.001000 loss 0.0215 (0.0302) acc@1 1.0000 (0.9920) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:15] __main__ INFO: Elapsed 1.72
[2022-06-16 11:53:15] __main__ INFO: Val 448
[2022-06-16 11:53:16] __main__ INFO: Epoch 448 loss 2.1274 acc@1 0.6550 acc@5 0.9623
[2022-06-16 11:53:16] __main__ INFO: Elapsed 0.71
[2022-06-16 11:53:16] __main__ INFO: Train 449 17472
[2022-06-16 11:53:18] __main__ INFO: Epoch 449 Step 39/39 lr 0.001000 loss 0.0262 (0.0274) acc@1 0.9922 (0.9926) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:18] __main__ INFO: Elapsed 1.68
[2022-06-16 11:53:18] __main__ INFO: Val 449
[2022-06-16 11:53:19] __main__ INFO: Epoch 449 loss 2.1266 acc@1 0.6539 acc@5 0.9633
[2022-06-16 11:53:19] __main__ INFO: Elapsed 0.74
[2022-06-16 11:53:19] __main__ INFO: Train 450 17511
[2022-06-16 11:53:20] __main__ INFO: Epoch 450 Step 39/39 lr 0.001000 loss 0.0212 (0.0289) acc@1 0.9922 (0.9922) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:20] __main__ INFO: Elapsed 1.62
[2022-06-16 11:53:20] __main__ INFO: Val 450
[2022-06-16 11:53:21] __main__ INFO: Epoch 450 loss 2.1310 acc@1 0.6549 acc@5 0.9632
[2022-06-16 11:53:21] __main__ INFO: Elapsed 0.73
[2022-06-16 11:53:21] __main__ INFO: Train 451 17550
[2022-06-16 11:53:23] __main__ INFO: Epoch 451 Step 39/39 lr 0.001000 loss 0.0270 (0.0295) acc@1 0.9922 (0.9928) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:23] __main__ INFO: Elapsed 1.77
[2022-06-16 11:53:23] __main__ INFO: Val 451
[2022-06-16 11:53:23] __main__ INFO: Epoch 451 loss 2.1298 acc@1 0.6538 acc@5 0.9619
[2022-06-16 11:53:23] __main__ INFO: Elapsed 0.79
[2022-06-16 11:53:23] __main__ INFO: Train 452 17589
[2022-06-16 11:53:25] __main__ INFO: Epoch 452 Step 39/39 lr 0.001000 loss 0.0370 (0.0286) acc@1 0.9844 (0.9934) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:25] __main__ INFO: Elapsed 1.70
[2022-06-16 11:53:25] __main__ INFO: Val 452
[2022-06-16 11:53:26] __main__ INFO: Epoch 452 loss 2.1354 acc@1 0.6542 acc@5 0.9625
[2022-06-16 11:53:26] __main__ INFO: Elapsed 0.73
[2022-06-16 11:53:26] __main__ INFO: Train 453 17628
[2022-06-16 11:53:27] __main__ INFO: Epoch 453 Step 39/39 lr 0.001000 loss 0.0095 (0.0268) acc@1 1.0000 (0.9946) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:28] __main__ INFO: Elapsed 1.64
[2022-06-16 11:53:28] __main__ INFO: Val 453
[2022-06-16 11:53:28] __main__ INFO: Epoch 453 loss 2.1408 acc@1 0.6560 acc@5 0.9631
[2022-06-16 11:53:28] __main__ INFO: Elapsed 0.73
[2022-06-16 11:53:28] __main__ INFO: Train 454 17667
[2022-06-16 11:53:30] __main__ INFO: Epoch 454 Step 39/39 lr 0.001000 loss 0.0209 (0.0292) acc@1 1.0000 (0.9916) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:30] __main__ INFO: Elapsed 1.69
[2022-06-16 11:53:30] __main__ INFO: Val 454
[2022-06-16 11:53:31] __main__ INFO: Epoch 454 loss 2.1486 acc@1 0.6526 acc@5 0.9631
[2022-06-16 11:53:31] __main__ INFO: Elapsed 0.68
[2022-06-16 11:53:31] __main__ INFO: Train 455 17706
[2022-06-16 11:53:32] __main__ INFO: Epoch 455 Step 39/39 lr 0.001000 loss 0.0271 (0.0287) acc@1 0.9922 (0.9934) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:32] __main__ INFO: Elapsed 1.64
[2022-06-16 11:53:32] __main__ INFO: Val 455
[2022-06-16 11:53:33] __main__ INFO: Epoch 455 loss 2.1356 acc@1 0.6541 acc@5 0.9625
[2022-06-16 11:53:33] __main__ INFO: Elapsed 0.72
[2022-06-16 11:53:33] __main__ INFO: Train 456 17745
[2022-06-16 11:53:35] __main__ INFO: Epoch 456 Step 39/39 lr 0.001000 loss 0.0253 (0.0279) acc@1 1.0000 (0.9940) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:35] __main__ INFO: Elapsed 1.67
[2022-06-16 11:53:35] __main__ INFO: Val 456
[2022-06-16 11:53:35] __main__ INFO: Epoch 456 loss 2.1443 acc@1 0.6549 acc@5 0.9631
[2022-06-16 11:53:35] __main__ INFO: Elapsed 0.69
[2022-06-16 11:53:35] __main__ INFO: Train 457 17784
[2022-06-16 11:53:37] __main__ INFO: Epoch 457 Step 39/39 lr 0.001000 loss 0.0299 (0.0279) acc@1 0.9922 (0.9934) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:37] __main__ INFO: Elapsed 1.62
[2022-06-16 11:53:37] __main__ INFO: Val 457
[2022-06-16 11:53:38] __main__ INFO: Epoch 457 loss 2.1301 acc@1 0.6559 acc@5 0.9623
[2022-06-16 11:53:38] __main__ INFO: Elapsed 0.76
[2022-06-16 11:53:38] __main__ INFO: Train 458 17823
[2022-06-16 11:53:39] __main__ INFO: Epoch 458 Step 39/39 lr 0.001000 loss 0.0438 (0.0268) acc@1 0.9922 (0.9940) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:39] __main__ INFO: Elapsed 1.69
[2022-06-16 11:53:39] __main__ INFO: Val 458
[2022-06-16 11:53:40] __main__ INFO: Epoch 458 loss 2.1567 acc@1 0.6538 acc@5 0.9630
[2022-06-16 11:53:40] __main__ INFO: Elapsed 0.75
[2022-06-16 11:53:40] __main__ INFO: Train 459 17862
[2022-06-16 11:53:42] __main__ INFO: Epoch 459 Step 39/39 lr 0.001000 loss 0.0276 (0.0294) acc@1 0.9922 (0.9928) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:42] __main__ INFO: Elapsed 1.63
[2022-06-16 11:53:42] __main__ INFO: Val 459
[2022-06-16 11:53:43] __main__ INFO: Epoch 459 loss 2.1372 acc@1 0.6564 acc@5 0.9635
[2022-06-16 11:53:43] __main__ INFO: Elapsed 0.71
[2022-06-16 11:53:43] __main__ INFO: Train 460 17901
[2022-06-16 11:53:44] __main__ INFO: Epoch 460 Step 39/39 lr 0.001000 loss 0.0130 (0.0285) acc@1 1.0000 (0.9924) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:44] __main__ INFO: Elapsed 1.68
[2022-06-16 11:53:44] __main__ INFO: Val 460
[2022-06-16 11:53:45] __main__ INFO: Epoch 460 loss 2.1363 acc@1 0.6552 acc@5 0.9631
[2022-06-16 11:53:45] __main__ INFO: Elapsed 0.73
[2022-06-16 11:53:45] __main__ INFO: Train 461 17940
[2022-06-16 11:53:47] __main__ INFO: Epoch 461 Step 39/39 lr 0.001000 loss 0.0189 (0.0259) acc@1 0.9922 (0.9952) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:47] __main__ INFO: Elapsed 1.72
[2022-06-16 11:53:47] __main__ INFO: Val 461
[2022-06-16 11:53:47] __main__ INFO: Epoch 461 loss 2.1387 acc@1 0.6565 acc@5 0.9625
[2022-06-16 11:53:47] __main__ INFO: Elapsed 0.76
[2022-06-16 11:53:47] __main__ INFO: Train 462 17979
[2022-06-16 11:53:49] __main__ INFO: Epoch 462 Step 39/39 lr 0.001000 loss 0.0724 (0.0259) acc@1 0.9688 (0.9936) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:49] __main__ INFO: Elapsed 1.65
[2022-06-16 11:53:49] __main__ INFO: Val 462
[2022-06-16 11:53:50] __main__ INFO: Epoch 462 loss 2.1469 acc@1 0.6547 acc@5 0.9623
[2022-06-16 11:53:50] __main__ INFO: Elapsed 0.72
[2022-06-16 11:53:50] __main__ INFO: Train 463 18018
[2022-06-16 11:53:51] __main__ INFO: Epoch 463 Step 39/39 lr 0.001000 loss 0.0330 (0.0266) acc@1 0.9922 (0.9946) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:51] __main__ INFO: Elapsed 1.59
[2022-06-16 11:53:51] __main__ INFO: Val 463
[2022-06-16 11:53:52] __main__ INFO: Epoch 463 loss 2.1323 acc@1 0.6561 acc@5 0.9625
[2022-06-16 11:53:52] __main__ INFO: Elapsed 0.68
[2022-06-16 11:53:52] __main__ INFO: Train 464 18057
[2022-06-16 11:53:54] __main__ INFO: Epoch 464 Step 39/39 lr 0.001000 loss 0.0101 (0.0284) acc@1 1.0000 (0.9934) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:54] __main__ INFO: Elapsed 1.64
[2022-06-16 11:53:54] __main__ INFO: Val 464
[2022-06-16 11:53:55] __main__ INFO: Epoch 464 loss 2.1261 acc@1 0.6557 acc@5 0.9615
[2022-06-16 11:53:55] __main__ INFO: Elapsed 0.79
[2022-06-16 11:53:55] __main__ INFO: Train 465 18096
[2022-06-16 11:53:56] __main__ INFO: Epoch 465 Step 39/39 lr 0.001000 loss 0.0218 (0.0261) acc@1 1.0000 (0.9938) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:56] __main__ INFO: Elapsed 1.68
[2022-06-16 11:53:56] __main__ INFO: Val 465
[2022-06-16 11:53:57] __main__ INFO: Epoch 465 loss 2.1289 acc@1 0.6585 acc@5 0.9628
[2022-06-16 11:53:57] __main__ INFO: Elapsed 0.80
[2022-06-16 11:53:57] __main__ INFO: Train 466 18135
[2022-06-16 11:53:59] __main__ INFO: Epoch 466 Step 39/39 lr 0.001000 loss 0.0561 (0.0251) acc@1 0.9766 (0.9948) acc@5 1.0000 (1.0000)
[2022-06-16 11:53:59] __main__ INFO: Elapsed 1.70
[2022-06-16 11:53:59] __main__ INFO: Val 466
[2022-06-16 11:53:59] __main__ INFO: Epoch 466 loss 2.1363 acc@1 0.6565 acc@5 0.9611
[2022-06-16 11:53:59] __main__ INFO: Elapsed 0.77
[2022-06-16 11:53:59] __main__ INFO: Train 467 18174
[2022-06-16 11:54:01] __main__ INFO: Epoch 467 Step 39/39 lr 0.001000 loss 0.0205 (0.0291) acc@1 0.9922 (0.9926) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:01] __main__ INFO: Elapsed 1.72
[2022-06-16 11:54:01] __main__ INFO: Val 467
[2022-06-16 11:54:02] __main__ INFO: Epoch 467 loss 2.1265 acc@1 0.6603 acc@5 0.9623
[2022-06-16 11:54:02] __main__ INFO: Elapsed 0.69
[2022-06-16 11:54:02] __main__ INFO: Train 468 18213
[2022-06-16 11:54:04] __main__ INFO: Epoch 468 Step 39/39 lr 0.001000 loss 0.0099 (0.0271) acc@1 1.0000 (0.9922) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:04] __main__ INFO: Elapsed 1.69
[2022-06-16 11:54:04] __main__ INFO: Val 468
[2022-06-16 11:54:04] __main__ INFO: Epoch 468 loss 2.1405 acc@1 0.6577 acc@5 0.9611
[2022-06-16 11:54:04] __main__ INFO: Elapsed 0.74
[2022-06-16 11:54:04] __main__ INFO: Train 469 18252
[2022-06-16 11:54:06] __main__ INFO: Epoch 469 Step 39/39 lr 0.001000 loss 0.0121 (0.0261) acc@1 1.0000 (0.9932) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:06] __main__ INFO: Elapsed 1.68
[2022-06-16 11:54:06] __main__ INFO: Val 469
[2022-06-16 11:54:07] __main__ INFO: Epoch 469 loss 2.1560 acc@1 0.6544 acc@5 0.9617
[2022-06-16 11:54:07] __main__ INFO: Elapsed 0.70
[2022-06-16 11:54:07] __main__ INFO: Train 470 18291
[2022-06-16 11:54:08] __main__ INFO: Epoch 470 Step 39/39 lr 0.001000 loss 0.0376 (0.0287) acc@1 0.9844 (0.9934) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:08] __main__ INFO: Elapsed 1.72
[2022-06-16 11:54:08] __main__ INFO: Val 470
[2022-06-16 11:54:09] __main__ INFO: Epoch 470 loss 2.1530 acc@1 0.6551 acc@5 0.9616
[2022-06-16 11:54:09] __main__ INFO: Elapsed 0.77
[2022-06-16 11:54:09] __main__ INFO: Train 471 18330
[2022-06-16 11:54:11] __main__ INFO: Epoch 471 Step 39/39 lr 0.001000 loss 0.0342 (0.0256) acc@1 0.9922 (0.9926) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:11] __main__ INFO: Elapsed 1.66
[2022-06-16 11:54:11] __main__ INFO: Val 471
[2022-06-16 11:54:12] __main__ INFO: Epoch 471 loss 2.1540 acc@1 0.6551 acc@5 0.9617
[2022-06-16 11:54:12] __main__ INFO: Elapsed 0.76
[2022-06-16 11:54:12] __main__ INFO: Train 472 18369
[2022-06-16 11:54:13] __main__ INFO: Epoch 472 Step 39/39 lr 0.001000 loss 0.0382 (0.0282) acc@1 0.9922 (0.9924) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:13] __main__ INFO: Elapsed 1.68
[2022-06-16 11:54:13] __main__ INFO: Val 472
[2022-06-16 11:54:14] __main__ INFO: Epoch 472 loss 2.1454 acc@1 0.6546 acc@5 0.9629
[2022-06-16 11:54:14] __main__ INFO: Elapsed 0.78
[2022-06-16 11:54:14] __main__ INFO: Train 473 18408
[2022-06-16 11:54:16] __main__ INFO: Epoch 473 Step 39/39 lr 0.001000 loss 0.0235 (0.0302) acc@1 0.9922 (0.9918) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:16] __main__ INFO: Elapsed 1.67
[2022-06-16 11:54:16] __main__ INFO: Val 473
[2022-06-16 11:54:17] __main__ INFO: Epoch 473 loss 2.1528 acc@1 0.6552 acc@5 0.9628
[2022-06-16 11:54:17] __main__ INFO: Elapsed 0.74
[2022-06-16 11:54:17] __main__ INFO: Train 474 18447
[2022-06-16 11:54:18] __main__ INFO: Epoch 474 Step 39/39 lr 0.001000 loss 0.0290 (0.0249) acc@1 0.9844 (0.9944) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:18] __main__ INFO: Elapsed 1.60
[2022-06-16 11:54:18] __main__ INFO: Val 474
[2022-06-16 11:54:19] __main__ INFO: Epoch 474 loss 2.1390 acc@1 0.6572 acc@5 0.9619
[2022-06-16 11:54:19] __main__ INFO: Elapsed 0.72
[2022-06-16 11:54:19] __main__ INFO: Train 475 18486
[2022-06-16 11:54:20] __main__ INFO: Epoch 475 Step 39/39 lr 0.001000 loss 0.0087 (0.0282) acc@1 1.0000 (0.9930) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:21] __main__ INFO: Elapsed 1.69
[2022-06-16 11:54:21] __main__ INFO: Val 475
[2022-06-16 11:54:21] __main__ INFO: Epoch 475 loss 2.1539 acc@1 0.6565 acc@5 0.9610
[2022-06-16 11:54:21] __main__ INFO: Elapsed 0.71
[2022-06-16 11:54:21] __main__ INFO: Train 476 18525
[2022-06-16 11:54:23] __main__ INFO: Epoch 476 Step 39/39 lr 0.001000 loss 0.0217 (0.0278) acc@1 1.0000 (0.9926) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:23] __main__ INFO: Elapsed 1.71
[2022-06-16 11:54:23] __main__ INFO: Val 476
[2022-06-16 11:54:24] __main__ INFO: Epoch 476 loss 2.1586 acc@1 0.6556 acc@5 0.9615
[2022-06-16 11:54:24] __main__ INFO: Elapsed 0.70
[2022-06-16 11:54:24] __main__ INFO: Train 477 18564
[2022-06-16 11:54:25] __main__ INFO: Epoch 477 Step 39/39 lr 0.001000 loss 0.0367 (0.0292) acc@1 0.9922 (0.9928) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:25] __main__ INFO: Elapsed 1.62
[2022-06-16 11:54:25] __main__ INFO: Val 477
[2022-06-16 11:54:26] __main__ INFO: Epoch 477 loss 2.1445 acc@1 0.6557 acc@5 0.9619
[2022-06-16 11:54:26] __main__ INFO: Elapsed 0.78
[2022-06-16 11:54:26] __main__ INFO: Train 478 18603
[2022-06-16 11:54:28] __main__ INFO: Epoch 478 Step 39/39 lr 0.001000 loss 0.0159 (0.0244) acc@1 1.0000 (0.9940) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:28] __main__ INFO: Elapsed 1.69
[2022-06-16 11:54:28] __main__ INFO: Val 478
[2022-06-16 11:54:29] __main__ INFO: Epoch 478 loss 2.1575 acc@1 0.6555 acc@5 0.9621
[2022-06-16 11:54:29] __main__ INFO: Elapsed 0.76
[2022-06-16 11:54:29] __main__ INFO: Train 479 18642
[2022-06-16 11:54:30] __main__ INFO: Epoch 479 Step 39/39 lr 0.001000 loss 0.0241 (0.0262) acc@1 1.0000 (0.9944) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:30] __main__ INFO: Elapsed 1.64
[2022-06-16 11:54:30] __main__ INFO: Val 479
[2022-06-16 11:54:31] __main__ INFO: Epoch 479 loss 2.1486 acc@1 0.6559 acc@5 0.9620
[2022-06-16 11:54:31] __main__ INFO: Elapsed 0.72
[2022-06-16 11:54:31] __main__ INFO: Train 480 18681
[2022-06-16 11:54:32] __main__ INFO: Epoch 480 Step 39/39 lr 0.001000 loss 0.0456 (0.0257) acc@1 0.9844 (0.9928) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:33] __main__ INFO: Elapsed 1.65
[2022-06-16 11:54:33] __main__ INFO: Val 480
[2022-06-16 11:54:33] __main__ INFO: Epoch 480 loss 2.1658 acc@1 0.6529 acc@5 0.9621
[2022-06-16 11:54:33] __main__ INFO: Elapsed 0.74
[2022-06-16 11:54:33] __main__ INFO: Train 481 18720
[2022-06-16 11:54:35] __main__ INFO: Epoch 481 Step 39/39 lr 0.001000 loss 0.0112 (0.0227) acc@1 1.0000 (0.9962) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:35] __main__ INFO: Elapsed 1.58
[2022-06-16 11:54:35] __main__ INFO: Val 481
[2022-06-16 11:54:36] __main__ INFO: Epoch 481 loss 2.1716 acc@1 0.6541 acc@5 0.9630
[2022-06-16 11:54:36] __main__ INFO: Elapsed 0.77
[2022-06-16 11:54:36] __main__ INFO: Train 482 18759
[2022-06-16 11:54:37] __main__ INFO: Epoch 482 Step 39/39 lr 0.001000 loss 0.0194 (0.0266) acc@1 0.9922 (0.9934) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:37] __main__ INFO: Elapsed 1.69
[2022-06-16 11:54:37] __main__ INFO: Val 482
[2022-06-16 11:54:38] __main__ INFO: Epoch 482 loss 2.1628 acc@1 0.6530 acc@5 0.9624
[2022-06-16 11:54:38] __main__ INFO: Elapsed 0.78
[2022-06-16 11:54:38] __main__ INFO: Train 483 18798
[2022-06-16 11:54:40] __main__ INFO: Epoch 483 Step 39/39 lr 0.001000 loss 0.0218 (0.0272) acc@1 1.0000 (0.9930) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:40] __main__ INFO: Elapsed 1.66
[2022-06-16 11:54:40] __main__ INFO: Val 483
[2022-06-16 11:54:41] __main__ INFO: Epoch 483 loss 2.1497 acc@1 0.6567 acc@5 0.9622
[2022-06-16 11:54:41] __main__ INFO: Elapsed 0.77
[2022-06-16 11:54:41] __main__ INFO: Train 484 18837
[2022-06-16 11:54:42] __main__ INFO: Epoch 484 Step 39/39 lr 0.001000 loss 0.0324 (0.0316) acc@1 0.9844 (0.9910) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:42] __main__ INFO: Elapsed 1.72
[2022-06-16 11:54:42] __main__ INFO: Val 484
[2022-06-16 11:54:43] __main__ INFO: Epoch 484 loss 2.1675 acc@1 0.6539 acc@5 0.9638
[2022-06-16 11:54:43] __main__ INFO: Elapsed 0.75
[2022-06-16 11:54:43] __main__ INFO: Train 485 18876
[2022-06-16 11:54:45] __main__ INFO: Epoch 485 Step 39/39 lr 0.001000 loss 0.0187 (0.0266) acc@1 1.0000 (0.9936) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:45] __main__ INFO: Elapsed 1.63
[2022-06-16 11:54:45] __main__ INFO: Val 485
[2022-06-16 11:54:45] __main__ INFO: Epoch 485 loss 2.1780 acc@1 0.6540 acc@5 0.9629
[2022-06-16 11:54:45] __main__ INFO: Elapsed 0.74
[2022-06-16 11:54:45] __main__ INFO: Train 486 18915
[2022-06-16 11:54:47] __main__ INFO: Epoch 486 Step 39/39 lr 0.001000 loss 0.0609 (0.0255) acc@1 0.9688 (0.9940) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:47] __main__ INFO: Elapsed 1.62
[2022-06-16 11:54:47] __main__ INFO: Val 486
[2022-06-16 11:54:48] __main__ INFO: Epoch 486 loss 2.1768 acc@1 0.6548 acc@5 0.9633
[2022-06-16 11:54:48] __main__ INFO: Elapsed 0.76
[2022-06-16 11:54:48] __main__ INFO: Train 487 18954
[2022-06-16 11:54:49] __main__ INFO: Epoch 487 Step 39/39 lr 0.001000 loss 0.0420 (0.0256) acc@1 0.9922 (0.9928) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:49] __main__ INFO: Elapsed 1.62
[2022-06-16 11:54:49] __main__ INFO: Val 487
[2022-06-16 11:54:50] __main__ INFO: Epoch 487 loss 2.1664 acc@1 0.6548 acc@5 0.9627
[2022-06-16 11:54:50] __main__ INFO: Elapsed 0.77
[2022-06-16 11:54:50] __main__ INFO: Train 488 18993
[2022-06-16 11:54:52] __main__ INFO: Epoch 488 Step 39/39 lr 0.001000 loss 0.0194 (0.0244) acc@1 1.0000 (0.9942) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:52] __main__ INFO: Elapsed 1.63
[2022-06-16 11:54:52] __main__ INFO: Val 488
[2022-06-16 11:54:53] __main__ INFO: Epoch 488 loss 2.1786 acc@1 0.6535 acc@5 0.9620
[2022-06-16 11:54:53] __main__ INFO: Elapsed 0.71
[2022-06-16 11:54:53] __main__ INFO: Train 489 19032
[2022-06-16 11:54:54] __main__ INFO: Epoch 489 Step 39/39 lr 0.001000 loss 0.0351 (0.0256) acc@1 1.0000 (0.9952) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:54] __main__ INFO: Elapsed 1.66
[2022-06-16 11:54:54] __main__ INFO: Val 489
[2022-06-16 11:54:55] __main__ INFO: Epoch 489 loss 2.1695 acc@1 0.6575 acc@5 0.9625
[2022-06-16 11:54:55] __main__ INFO: Elapsed 0.75
[2022-06-16 11:54:55] __main__ INFO: Train 490 19071
[2022-06-16 11:54:57] __main__ INFO: Epoch 490 Step 39/39 lr 0.001000 loss 0.0042 (0.0293) acc@1 1.0000 (0.9920) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:57] __main__ INFO: Elapsed 1.69
[2022-06-16 11:54:57] __main__ INFO: Val 490
[2022-06-16 11:54:57] __main__ INFO: Epoch 490 loss 2.1924 acc@1 0.6535 acc@5 0.9613
[2022-06-16 11:54:57] __main__ INFO: Elapsed 0.76
[2022-06-16 11:54:57] __main__ INFO: Train 491 19110
[2022-06-16 11:54:59] __main__ INFO: Epoch 491 Step 39/39 lr 0.001000 loss 0.0265 (0.0264) acc@1 0.9922 (0.9930) acc@5 1.0000 (1.0000)
[2022-06-16 11:54:59] __main__ INFO: Elapsed 1.70
[2022-06-16 11:54:59] __main__ INFO: Val 491
[2022-06-16 11:55:00] __main__ INFO: Epoch 491 loss 2.1796 acc@1 0.6535 acc@5 0.9631
[2022-06-16 11:55:00] __main__ INFO: Elapsed 0.74
[2022-06-16 11:55:00] __main__ INFO: Train 492 19149
[2022-06-16 11:55:02] __main__ INFO: Epoch 492 Step 39/39 lr 0.001000 loss 0.0543 (0.0259) acc@1 0.9844 (0.9936) acc@5 1.0000 (1.0000)
[2022-06-16 11:55:02] __main__ INFO: Elapsed 1.72
[2022-06-16 11:55:02] __main__ INFO: Val 492
[2022-06-16 11:55:02] __main__ INFO: Epoch 492 loss 2.1670 acc@1 0.6542 acc@5 0.9623
[2022-06-16 11:55:02] __main__ INFO: Elapsed 0.78
[2022-06-16 11:55:02] __main__ INFO: Train 493 19188
[2022-06-16 11:55:04] __main__ INFO: Epoch 493 Step 39/39 lr 0.001000 loss 0.0260 (0.0215) acc@1 0.9922 (0.9958) acc@5 1.0000 (1.0000)
[2022-06-16 11:55:04] __main__ INFO: Elapsed 1.69
[2022-06-16 11:55:04] __main__ INFO: Val 493
[2022-06-16 11:55:05] __main__ INFO: Epoch 493 loss 2.1771 acc@1 0.6551 acc@5 0.9615
[2022-06-16 11:55:05] __main__ INFO: Elapsed 0.75
[2022-06-16 11:55:05] __main__ INFO: Train 494 19227
[2022-06-16 11:55:06] __main__ INFO: Epoch 494 Step 39/39 lr 0.001000 loss 0.0422 (0.0274) acc@1 0.9766 (0.9934) acc@5 1.0000 (1.0000)
[2022-06-16 11:55:06] __main__ INFO: Elapsed 1.59
[2022-06-16 11:55:06] __main__ INFO: Val 494
[2022-06-16 11:55:07] __main__ INFO: Epoch 494 loss 2.1593 acc@1 0.6541 acc@5 0.9632
[2022-06-16 11:55:07] __main__ INFO: Elapsed 0.69
[2022-06-16 11:55:07] __main__ INFO: Train 495 19266
[2022-06-16 11:55:09] __main__ INFO: Epoch 495 Step 39/39 lr 0.001000 loss 0.0142 (0.0235) acc@1 0.9922 (0.9936) acc@5 1.0000 (1.0000)
[2022-06-16 11:55:09] __main__ INFO: Elapsed 1.65
[2022-06-16 11:55:09] __main__ INFO: Val 495
[2022-06-16 11:55:09] __main__ INFO: Epoch 495 loss 2.1598 acc@1 0.6558 acc@5 0.9626
[2022-06-16 11:55:09] __main__ INFO: Elapsed 0.75
[2022-06-16 11:55:09] __main__ INFO: Train 496 19305
[2022-06-16 11:55:11] __main__ INFO: Epoch 496 Step 39/39 lr 0.001000 loss 0.0339 (0.0246) acc@1 0.9844 (0.9940) acc@5 1.0000 (1.0000)
[2022-06-16 11:55:11] __main__ INFO: Elapsed 1.68
[2022-06-16 11:55:11] __main__ INFO: Val 496
[2022-06-16 11:55:12] __main__ INFO: Epoch 496 loss 2.1649 acc@1 0.6543 acc@5 0.9630
[2022-06-16 11:55:12] __main__ INFO: Elapsed 0.75
[2022-06-16 11:55:12] __main__ INFO: Train 497 19344
[2022-06-16 11:55:14] __main__ INFO: Epoch 497 Step 39/39 lr 0.001000 loss 0.0086 (0.0292) acc@1 1.0000 (0.9926) acc@5 1.0000 (1.0000)
[2022-06-16 11:55:14] __main__ INFO: Elapsed 1.67
[2022-06-16 11:55:14] __main__ INFO: Val 497
[2022-06-16 11:55:14] __main__ INFO: Epoch 497 loss 2.1674 acc@1 0.6542 acc@5 0.9622
[2022-06-16 11:55:14] __main__ INFO: Elapsed 0.78
[2022-06-16 11:55:14] __main__ INFO: Train 498 19383
[2022-06-16 11:55:16] __main__ INFO: Epoch 498 Step 39/39 lr 0.001000 loss 0.0056 (0.0251) acc@1 1.0000 (0.9934) acc@5 1.0000 (1.0000)
[2022-06-16 11:55:16] __main__ INFO: Elapsed 1.77
[2022-06-16 11:55:16] __main__ INFO: Val 498
[2022-06-16 11:55:17] __main__ INFO: Epoch 498 loss 2.1594 acc@1 0.6532 acc@5 0.9621
[2022-06-16 11:55:17] __main__ INFO: Elapsed 0.71
[2022-06-16 11:55:17] __main__ INFO: Train 499 19422
[2022-06-16 11:55:18] __main__ INFO: Epoch 499 Step 39/39 lr 0.001000 loss 0.0390 (0.0261) acc@1 0.9922 (0.9942) acc@5 1.0000 (1.0000)
[2022-06-16 11:55:19] __main__ INFO: Elapsed 1.66
[2022-06-16 11:55:19] __main__ INFO: Val 499
[2022-06-16 11:55:19] __main__ INFO: Epoch 499 loss 2.1697 acc@1 0.6549 acc@5 0.9614
[2022-06-16 11:55:19] __main__ INFO: Elapsed 0.69
[2022-06-16 11:55:19] __main__ INFO: Train 500 19461
[2022-06-16 11:55:21] __main__ INFO: Epoch 500 Step 39/39 lr 0.001000 loss 0.0553 (0.0281) acc@1 0.9844 (0.9932) acc@5 1.0000 (1.0000)
[2022-06-16 11:55:21] __main__ INFO: Elapsed 1.61
[2022-06-16 11:55:21] __main__ INFO: Val 500
[2022-06-16 11:55:22] __main__ INFO: Epoch 500 loss 2.1742 acc@1 0.6553 acc@5 0.9627
[2022-06-16 11:55:22] __main__ INFO: Elapsed 0.73
[2022-06-16 11:55:22] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00500.pth
[2022-06-17 04:22:58] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: ''
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: True
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 10
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: True
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-17 04:22:58] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-17 04:23:02] __main__ INFO: MACs   : 112.57M
[2022-06-17 04:23:02] __main__ INFO: #params: 758.55K
[2022-06-17 04:23:02] __main__ INFO: Val 0
[2022-06-17 04:23:03] __main__ INFO: Epoch 0 loss 27477.4463 acc@1 0.1002 acc@5 0.5020
[2022-06-17 04:23:03] __main__ INFO: Elapsed 0.84
[2022-06-17 04:23:03] __main__ INFO: Train 1 0
[2022-06-17 04:23:05] __main__ INFO: Epoch 1 Step 39/39 lr 0.100000 loss 2.3365 (4.0211) acc@1 0.1355 (0.0997) acc@5 0.5546 (0.5110)
[2022-06-17 04:23:06] __main__ INFO: Elapsed 2.24
[2022-06-17 04:23:06] __main__ INFO: Val 1
[2022-06-17 04:23:06] __main__ INFO: Epoch 1 loss 5.8008 acc@1 0.0947 acc@5 0.5237
[2022-06-17 04:23:06] __main__ INFO: Elapsed 0.75
[2022-06-17 04:23:06] __main__ INFO: Train 2 39
[2022-06-17 04:23:08] __main__ INFO: Epoch 2 Step 39/39 lr 0.100000 loss 2.3730 (2.3377) acc@1 0.1220 (0.1027) acc@5 0.5030 (0.5087)
[2022-06-17 04:23:08] __main__ INFO: Elapsed 1.76
[2022-06-17 04:23:08] __main__ INFO: Val 2
[2022-06-17 04:23:09] __main__ INFO: Epoch 2 loss 2.3191 acc@1 0.1097 acc@5 0.5037
[2022-06-17 04:23:09] __main__ INFO: Elapsed 0.79
[2022-06-17 04:23:09] __main__ INFO: Train 3 78
[2022-06-17 04:23:11] __main__ INFO: Epoch 3 Step 39/39 lr 0.100000 loss 2.3299 (2.3285) acc@1 0.1357 (0.0996) acc@5 0.5598 (0.5102)
[2022-06-17 04:23:11] __main__ INFO: Elapsed 1.76
[2022-06-17 04:23:11] __main__ INFO: Val 3
[2022-06-17 04:23:11] __main__ INFO: Epoch 3 loss 2.3336 acc@1 0.1047 acc@5 0.5059
[2022-06-17 04:23:11] __main__ INFO: Elapsed 0.85
[2022-06-17 04:23:11] __main__ INFO: Train 4 117
[2022-06-17 04:23:13] __main__ INFO: Epoch 4 Step 39/39 lr 0.100000 loss 2.3250 (2.3306) acc@1 0.0928 (0.1022) acc@5 0.4674 (0.5160)
[2022-06-17 04:23:13] __main__ INFO: Elapsed 1.79
[2022-06-17 04:23:13] __main__ INFO: Val 4
[2022-06-17 04:23:14] __main__ INFO: Epoch 4 loss 2.3364 acc@1 0.1107 acc@5 0.5168
[2022-06-17 04:23:14] __main__ INFO: Elapsed 0.74
[2022-06-17 04:23:14] __main__ INFO: Train 5 156
[2022-06-17 04:23:16] __main__ INFO: Epoch 5 Step 39/39 lr 0.100000 loss 2.3119 (2.3195) acc@1 0.1167 (0.0998) acc@5 0.5164 (0.5136)
[2022-06-17 04:23:16] __main__ INFO: Elapsed 1.71
[2022-06-17 04:23:16] __main__ INFO: Val 5
[2022-06-17 04:23:17] __main__ INFO: Epoch 5 loss 2.4240 acc@1 0.1136 acc@5 0.5212
[2022-06-17 04:23:17] __main__ INFO: Elapsed 0.83
[2022-06-17 04:23:17] __main__ INFO: Train 6 195
[2022-06-17 04:23:18] __main__ INFO: Epoch 6 Step 39/39 lr 0.100000 loss 2.3182 (2.3159) acc@1 0.1053 (0.0954) acc@5 0.5129 (0.5048)
[2022-06-17 04:23:18] __main__ INFO: Elapsed 1.79
[2022-06-17 04:23:18] __main__ INFO: Val 6
[2022-06-17 04:23:19] __main__ INFO: Epoch 6 loss 2.3251 acc@1 0.1102 acc@5 0.5114
[2022-06-17 04:23:19] __main__ INFO: Elapsed 0.83
[2022-06-17 04:23:19] __main__ INFO: Train 7 234
[2022-06-17 04:23:21] __main__ INFO: Epoch 7 Step 39/39 lr 0.100000 loss 2.3024 (2.3197) acc@1 0.1042 (0.1020) acc@5 0.5677 (0.5105)
[2022-06-17 04:23:21] __main__ INFO: Elapsed 1.83
[2022-06-17 04:23:21] __main__ INFO: Val 7
[2022-06-17 04:23:22] __main__ INFO: Epoch 7 loss 2.3297 acc@1 0.1093 acc@5 0.5154
[2022-06-17 04:23:22] __main__ INFO: Elapsed 0.80
[2022-06-17 04:23:22] __main__ INFO: Train 8 273
[2022-06-17 04:23:23] __main__ INFO: Epoch 8 Step 39/39 lr 0.100000 loss 2.3239 (2.3116) acc@1 0.1133 (0.1020) acc@5 0.4459 (0.5160)
[2022-06-17 04:23:24] __main__ INFO: Elapsed 1.74
[2022-06-17 04:23:24] __main__ INFO: Val 8
[2022-06-17 04:23:24] __main__ INFO: Epoch 8 loss 2.3113 acc@1 0.1254 acc@5 0.5157
[2022-06-17 04:23:24] __main__ INFO: Elapsed 0.74
[2022-06-17 04:23:24] __main__ INFO: Train 9 312
[2022-06-17 04:23:26] __main__ INFO: Epoch 9 Step 39/39 lr 0.100000 loss 2.3349 (2.3096) acc@1 0.0595 (0.1086) acc@5 0.4914 (0.5211)
[2022-06-17 04:23:26] __main__ INFO: Elapsed 1.84
[2022-06-17 04:23:26] __main__ INFO: Val 9
[2022-06-17 04:23:27] __main__ INFO: Epoch 9 loss 2.3295 acc@1 0.1132 acc@5 0.5229
[2022-06-17 04:23:27] __main__ INFO: Elapsed 0.76
[2022-06-17 04:23:27] __main__ INFO: Train 10 351
[2022-06-17 04:23:29] __main__ INFO: Epoch 10 Step 39/39 lr 0.100000 loss 2.3095 (2.3134) acc@1 0.0636 (0.1127) acc@5 0.5348 (0.5103)
[2022-06-17 04:23:29] __main__ INFO: Elapsed 1.78
[2022-06-17 04:23:29] __main__ INFO: Val 10
[2022-06-17 04:23:29] __main__ INFO: Epoch 10 loss 2.3409 acc@1 0.1096 acc@5 0.5411
[2022-06-17 04:23:29] __main__ INFO: Elapsed 0.79
[2022-06-17 04:23:29] fvcore.common.checkpoint INFO: Saving checkpoint to experiments/cifar10/exp00/checkpoint_00010.pth
[2022-06-17 04:29:25] __main__ INFO: device: cuda
cudnn:
  benchmark: True
  deterministic: False
dataset:
  name: CIFAR10
  dataset_dir: ~/.torch/datasets/CIFAR10
  image_size: 32
  n_channels: 3
  n_classes: 10
model:
  type: cifar
  name: resnet
  init_mode: kaiming_fan_out
  vgg:
    n_channels: [64, 128, 256, 512, 512]
    n_layers: [2, 2, 3, 3, 3]
    use_bn: True
  resnet:
    depth: 50
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
  resnet_preact:
    depth: 110
    n_blocks: [2, 2, 2, 2]
    block_type: basic
    initial_channels: 16
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
  wrn:
    depth: 28
    initial_channels: 16
    widening_factor: 10
    drop_rate: 0.0
  densenet:
    depth: 100
    n_blocks: [6, 12, 24, 16]
    block_type: bottleneck
    growth_rate: 12
    drop_rate: 0.0
    compression_rate: 0.5
  pyramidnet:
    depth: 272
    n_blocks: [3, 24, 36, 3]
    initial_channels: 16
    block_type: bottleneck
    alpha: 200
  resnext:
    depth: 29
    n_blocks: [3, 4, 6, 3]
    initial_channels: 64
    cardinality: 8
    base_channels: 4
  shake_shake:
    depth: 26
    initial_channels: 96
    shake_forward: True
    shake_backward: True
    shake_image: True
  se_resnet_preact:
    depth: 110
    initial_channels: 16
    se_reduction: 16
    block_type: basic
    remove_first_relu: False
    add_last_bn: False
    preact_stage: [True, True, True]
train:
  checkpoint: ''
  resume: False
  use_apex: True
  precision: O0
  batch_size: 128
  small_train: True
  subdivision: 1
  optimizer: sgd
  base_lr: 0.1
  momentum: 0.9
  nesterov: True
  weight_decay: 0.0001
  no_weight_decay_on_bn: False
  gradient_clip: 0.0
  start_epoch: 0
  seed: 0
  val_first: True
  val_period: 1
  val_ratio: 0.0
  use_test_as_val: True
  output_dir: experiments/cifar10/exp00
  log_period: 100
  checkpoint_period: 100
  use_tensorboard: True
  dataloader:
    num_workers: 2
    drop_last: True
    pin_memory: False
    non_blocking: False
  distributed: False
  dist:
    backend: nccl
    init_method: env://
    world_size: -1
    node_rank: -1
    local_rank: 0
    use_sync_bn: False
tensorboard:
  train_images: False
  val_images: False
  model_params: False
optim:
  adam:
    betas: (0.9, 0.999)
  lars:
    eps: 1e-09
    threshold: 0.01
  adabound:
    betas: (0.9, 0.999)
    final_lr: 0.1
    gamma: 0.001
scheduler:
  epochs: 100
  warmup:
    type: none
    epochs: 0
    start_factor: 0.001
    exponent: 4
  type: multistep
  milestones: [80, 120]
  lr_decay: 0.1
  lr_min_factor: 0.001
  T0: 10
  T_mul: 1.0
validation:
  batch_size: 256
  dataloader:
    num_workers: 2
    drop_last: False
    pin_memory: False
    non_blocking: False
augmentation:
  use_random_crop: True
  use_random_horizontal_flip: True
  use_cutout: False
  use_random_erasing: False
  use_dual_cutout: False
  use_mixup: False
  use_ricap: True
  use_cutmix: False
  use_label_smoothing: False
  random_crop:
    padding: 4
    fill: 0
    padding_mode: constant
  random_horizontal_flip:
    prob: 0.5
  cutout:
    prob: 1.0
    mask_size: 16
    cut_inside: False
    mask_color: 0
    dual_cutout_alpha: 0.1
  random_erasing:
    prob: 0.5
    area_ratio_range: [0.02, 0.4]
    min_aspect_ratio: 0.3
    max_attempt: 20
  mixup:
    alpha: 1.0
  ricap:
    beta: 0.3
  cutmix:
    alpha: 1.0
  label_smoothing:
    epsilon: 0.1
tta:
  use_resize: False
  use_center_crop: False
  resize: 256
test:
  checkpoint: ''
  output_dir: ''
  batch_size: 256
  dataloader:
    num_workers: 2
    pin_memory: False
[2022-06-17 04:29:25] __main__ INFO: env_info:
  pytorch_version: 1.9.0a0+c3d40fd
  cuda_version: 11.3
  cudnn_version: 8201
  num_gpus: 2
  gpu_name: TITAN RTX
  gpu_capability: 7.5
[2022-06-17 04:29:29] __main__ INFO: MACs   : 112.57M
[2022-06-17 04:29:29] __main__ INFO: #params: 758.55K
[2022-06-17 04:29:29] __main__ INFO: Val 0
[2022-06-17 04:29:30] __main__ INFO: Epoch 0 loss 27477.4460 acc@1 0.1002 acc@5 0.5020
[2022-06-17 04:29:30] __main__ INFO: Elapsed 0.86
[2022-06-17 04:29:30] __main__ INFO: Train 1 0
[2022-06-17 04:29:32] __main__ INFO: Epoch 1 Step 39/39 lr 0.100000 loss 2.3478 (4.0921) acc@1 0.1238 (0.1062) acc@5 0.5112 (0.5020)
[2022-06-17 04:29:32] __main__ INFO: Elapsed 2.12
[2022-06-17 04:29:32] __main__ INFO: Val 1
[2022-06-17 04:29:33] __main__ INFO: Epoch 1 loss 16.1683 acc@1 0.1008 acc@5 0.5515
[2022-06-17 04:29:33] __main__ INFO: Elapsed 0.73
[2022-06-17 04:29:33] __main__ INFO: Train 2 39
[2022-06-17 04:29:34] __main__ INFO: Epoch 2 Step 39/39 lr 0.100000 loss 2.3146 (2.3406) acc@1 0.1089 (0.1062) acc@5 0.5016 (0.5118)
[2022-06-17 04:29:35] __main__ INFO: Elapsed 1.75
[2022-06-17 04:29:35] __main__ INFO: Val 2
[2022-06-17 04:29:35] __main__ INFO: Epoch 2 loss 2.3304 acc@1 0.1190 acc@5 0.5266
[2022-06-17 04:29:35] __main__ INFO: Elapsed 0.81
[2022-06-17 04:29:35] __main__ INFO: Train 3 78
[2022-06-17 04:29:37] __main__ INFO: Epoch 3 Step 39/39 lr 0.100000 loss 2.3135 (2.3211) acc@1 0.0792 (0.1095) acc@5 0.4970 (0.5283)
[2022-06-17 04:29:37] __main__ INFO: Elapsed 1.80
[2022-06-17 04:29:37] __main__ INFO: Val 3
[2022-06-17 04:29:38] __main__ INFO: Epoch 3 loss 2.3154 acc@1 0.1247 acc@5 0.5285
[2022-06-17 04:29:38] __main__ INFO: Elapsed 0.74
[2022-06-17 04:29:38] __main__ INFO: Train 4 117
[2022-06-17 04:29:40] __main__ INFO: Epoch 4 Step 39/39 lr 0.100000 loss 2.3182 (2.3130) acc@1 0.0866 (0.1087) acc@5 0.5017 (0.5281)
[2022-06-17 04:29:40] __main__ INFO: Elapsed 1.79
[2022-06-17 04:29:40] __main__ INFO: Val 4
[2022-06-17 04:29:40] __main__ INFO: Epoch 4 loss 2.3173 acc@1 0.1328 acc@5 0.5551
[2022-06-17 04:29:40] __main__ INFO: Elapsed 0.77
[2022-06-17 04:29:40] __main__ INFO: Train 5 156
[2022-06-17 04:29:42] __main__ INFO: Epoch 5 Step 39/39 lr 0.100000 loss 2.2783 (2.3028) acc@1 0.0989 (0.1137) acc@5 0.5037 (0.5369)
[2022-06-17 04:29:42] __main__ INFO: Elapsed 1.86
[2022-06-17 04:29:42] __main__ INFO: Val 5
[2022-06-17 04:29:43] __main__ INFO: Epoch 5 loss 2.3040 acc@1 0.1004 acc@5 0.5004
[2022-06-17 04:29:43] __main__ INFO: Elapsed 0.75
[2022-06-17 04:29:43] __main__ INFO: Train 6 195
[2022-06-17 04:29:45] __main__ INFO: Epoch 6 Step 39/39 lr 0.100000 loss 2.2941 (2.2996) acc@1 0.1393 (0.1204) acc@5 0.5226 (0.5423)
[2022-06-17 04:29:45] __main__ INFO: Elapsed 1.75
[2022-06-17 04:29:45] __main__ INFO: Val 6
[2022-06-17 04:29:46] __main__ INFO: Epoch 6 loss 2.3047 acc@1 0.1004 acc@5 0.5005
[2022-06-17 04:29:46] __main__ INFO: Elapsed 0.80
[2022-06-17 04:29:46] __main__ INFO: Train 7 234
[2022-06-17 04:29:47] __main__ INFO: Epoch 7 Step 39/39 lr 0.100000 loss 2.2612 (2.2863) acc@1 0.1439 (0.1268) acc@5 0.5300 (0.5505)
[2022-06-17 04:29:47] __main__ INFO: Elapsed 1.84
[2022-06-17 04:29:47] __main__ INFO: Val 7
[2022-06-17 04:29:48] __main__ INFO: Epoch 7 loss 2.2396 acc@1 0.1540 acc@5 0.6024
[2022-06-17 04:29:48] __main__ INFO: Elapsed 0.72
[2022-06-17 04:29:48] __main__ INFO: Train 8 273
[2022-06-17 04:29:50] __main__ INFO: Epoch 8 Step 39/39 lr 0.100000 loss 2.2870 (2.2806) acc@1 0.1136 (0.1163) acc@5 0.5468 (0.5575)
[2022-06-17 04:29:50] __main__ INFO: Elapsed 1.72
[2022-06-17 04:29:50] __main__ INFO: Val 8
[2022-06-17 04:29:51] __main__ INFO: Epoch 8 loss 2.2835 acc@1 0.1150 acc@5 0.5266
[2022-06-17 04:29:51] __main__ INFO: Elapsed 0.78
[2022-06-17 04:29:51] __main__ INFO: Train 9 312
[2022-06-17 04:29:52] __main__ INFO: Epoch 9 Step 39/39 lr 0.100000 loss 2.2678 (2.2717) acc@1 0.1327 (0.1246) acc@5 0.5459 (0.5615)
[2022-06-17 04:29:52] __main__ INFO: Elapsed 1.76
[2022-06-17 04:29:52] __main__ INFO: Val 9
[2022-06-17 04:29:53] __main__ INFO: Epoch 9 loss 2.2195 acc@1 0.1395 acc@5 0.6161
[2022-06-17 04:29:53] __main__ INFO: Elapsed 0.75
[2022-06-17 04:29:53] __main__ INFO: Train 10 351
[2022-06-17 04:29:55] __main__ INFO: Epoch 10 Step 39/39 lr 0.100000 loss 2.2781 (2.2564) acc@1 0.1226 (0.1349) acc@5 0.5202 (0.5736)
[2022-06-17 04:29:55] __main__ INFO: Elapsed 1.79
[2022-06-17 04:29:55] __main__ INFO: Val 10
[2022-06-17 04:29:56] __main__ INFO: Epoch 10 loss 2.2013 acc@1 0.1741 acc@5 0.6463
[2022-06-17 04:29:56] __main__ INFO: Elapsed 0.76
[2022-06-17 04:29:56] __main__ INFO: Train 11 390
[2022-06-17 04:29:57] __main__ INFO: Epoch 11 Step 39/39 lr 0.100000 loss 2.3090 (2.2419) acc@1 0.1060 (0.1409) acc@5 0.4935 (0.5941)
[2022-06-17 04:29:57] __main__ INFO: Elapsed 1.74
[2022-06-17 04:29:57] __main__ INFO: Val 11
[2022-06-17 04:29:58] __main__ INFO: Epoch 11 loss 2.2083 acc@1 0.1486 acc@5 0.6170
[2022-06-17 04:29:58] __main__ INFO: Elapsed 0.77
[2022-06-17 04:29:58] __main__ INFO: Train 12 429
[2022-06-17 04:30:00] __main__ INFO: Epoch 12 Step 39/39 lr 0.100000 loss 2.2644 (2.2273) acc@1 0.1338 (0.1534) acc@5 0.5553 (0.6122)
[2022-06-17 04:30:00] __main__ INFO: Elapsed 1.74
[2022-06-17 04:30:00] __main__ INFO: Val 12
[2022-06-17 04:30:01] __main__ INFO: Epoch 12 loss 2.2432 acc@1 0.1459 acc@5 0.5824
[2022-06-17 04:30:01] __main__ INFO: Elapsed 0.83
[2022-06-17 04:30:01] __main__ INFO: Train 13 468
[2022-06-17 04:30:03] __main__ INFO: Epoch 13 Step 39/39 lr 0.100000 loss 2.2184 (2.2118) acc@1 0.1211 (0.1636) acc@5 0.6640 (0.6459)
[2022-06-17 04:30:03] __main__ INFO: Elapsed 1.77
[2022-06-17 04:30:03] __main__ INFO: Val 13
[2022-06-17 04:30:03] __main__ INFO: Epoch 13 loss 2.2118 acc@1 0.1725 acc@5 0.6462
[2022-06-17 04:30:03] __main__ INFO: Elapsed 0.81
[2022-06-17 04:30:03] __main__ INFO: Train 14 507
[2022-06-17 04:30:05] __main__ INFO: Epoch 14 Step 39/39 lr 0.100000 loss 2.1915 (2.1868) acc@1 0.1383 (0.1722) acc@5 0.6877 (0.6700)
[2022-06-17 04:30:05] __main__ INFO: Elapsed 1.81
[2022-06-17 04:30:05] __main__ INFO: Val 14
[2022-06-17 04:30:06] __main__ INFO: Epoch 14 loss 2.1080 acc@1 0.2173 acc@5 0.7328
[2022-06-17 04:30:06] __main__ INFO: Elapsed 0.77
[2022-06-17 04:30:06] __main__ INFO: Train 15 546
[2022-06-17 04:30:08] __main__ INFO: Epoch 15 Step 39/39 lr 0.100000 loss 2.2420 (2.1722) acc@1 0.1385 (0.1785) acc@5 0.6099 (0.6783)
[2022-06-17 04:30:08] __main__ INFO: Elapsed 1.80
[2022-06-17 04:30:08] __main__ INFO: Val 15
[2022-06-17 04:30:09] __main__ INFO: Epoch 15 loss 2.0501 acc@1 0.2221 acc@5 0.7531
[2022-06-17 04:30:09] __main__ INFO: Elapsed 0.75
[2022-06-17 04:30:09] __main__ INFO: Train 16 585
[2022-06-17 04:30:10] __main__ INFO: Epoch 16 Step 39/39 lr 0.100000 loss 2.1778 (2.1652) acc@1 0.2032 (0.1782) acc@5 0.6619 (0.6858)
[2022-06-17 04:30:10] __main__ INFO: Elapsed 1.72
[2022-06-17 04:30:10] __main__ INFO: Val 16
[2022-06-17 04:30:11] __main__ INFO: Epoch 16 loss 2.0547 acc@1 0.2207 acc@5 0.7682
[2022-06-17 04:30:11] __main__ INFO: Elapsed 0.81
[2022-06-17 04:30:11] __main__ INFO: Train 17 624
[2022-06-17 04:30:13] __main__ INFO: Epoch 17 Step 39/39 lr 0.100000 loss 2.1981 (2.1406) acc@1 0.1843 (0.1833) acc@5 0.6661 (0.6985)
[2022-06-17 04:30:13] __main__ INFO: Elapsed 1.86
[2022-06-17 04:30:13] __main__ INFO: Val 17
[2022-06-17 04:30:14] __main__ INFO: Epoch 17 loss 2.0393 acc@1 0.2215 acc@5 0.7769
[2022-06-17 04:30:14] __main__ INFO: Elapsed 0.77
[2022-06-17 04:30:14] __main__ INFO: Train 18 663
[2022-06-17 04:30:15] __main__ INFO: Epoch 18 Step 39/39 lr 0.100000 loss 2.0879 (2.1314) acc@1 0.2149 (0.1974) acc@5 0.7492 (0.7080)
[2022-06-17 04:30:16] __main__ INFO: Elapsed 1.81
[2022-06-17 04:30:16] __main__ INFO: Val 18
